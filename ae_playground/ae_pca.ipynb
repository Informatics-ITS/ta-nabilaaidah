{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd16c8c",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575d5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1]\n",
      "Value counts:\n",
      " label\n",
      "0    53802\n",
      "1    17215\n",
      "Name: count, dtype: int64\n",
      "Unique labels: [0 1]\n",
      "Value counts:\n",
      " label\n",
      "0    602450\n",
      "1    162061\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../NLME.csv', low_memory=False)\n",
    "unique_labels = df['label'].unique()\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"Value counts:\\n\", label_counts)\n",
    "\n",
    "df2 = pd.read_csv('../parsed_events (20).csv', low_memory=False)\n",
    "\n",
    "unique_labels = df2['label'].unique()\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "label_counts = df2['label'].value_counts()\n",
    "print(\"Value counts:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af19aa8",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35296af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={\n",
    "    'target_file_name': 'TargetFilename',\n",
    "    'event_id': 'EventID',\n",
    "    'target_process_guid': 'TargetProcessGuid',\n",
    "    'event_type': 'EventType',\n",
    "    'target_image': 'TargetImage',\n",
    "    'previous_creation_utc_time': 'PreviousCreationUtcTime',\n",
    "    'destination_host_name': 'DestinationHostname',\n",
    "    'company': 'Company',\n",
    "    'description': 'Description',\n",
    "    'product': 'Product',\n",
    "    'integrity_level': 'IntegrityLevel',\n",
    "    'creation_utc_time': 'CreationUtcTime',\n",
    "    'start_function': 'StartFunction',\n",
    "    'parent_process_id': 'ParentProcessGuid',\n",
    "    'user': 'User',\n",
    "    'logon_id': 'LogonId',\n",
    "    'parent_process_id': 'ParentProcessId',\n",
    "    'terminal_session_id': 'TerminalSessionId',\n",
    "    'rule_name': 'RuleName',\n",
    "    'target_object': 'TargetObject',\n",
    "    'parent_process_guid': 'ParentProcessGuid'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bc76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = [\n",
    "    'TargetFilename', \n",
    "    'EventID', \n",
    "    'TargetProcessGuid', \n",
    "    # 'EventType',\n",
    "    'TargetImage', \n",
    "    # 'PreviousCreationUtcTime', \n",
    "    # 'DestinationHostname', \n",
    "    'Company', \n",
    "    'Description',\n",
    "    'Product', \n",
    "    'IntegrityLevel', \n",
    "    # 'CreationUtcTime', \n",
    "    # 'StartFunction', \n",
    "    'ParentProcessGuid', \n",
    "    'User',\n",
    "    'LogonId', \n",
    "    'ParentProcessId', \n",
    "    # 'TerminalSessionId', \n",
    "    # 'RuleName', \n",
    "    # 'TargetObject', \n",
    "    'label'\n",
    "]\n",
    "\n",
    "selected_df = df[important_columns]\n",
    "selected_df2 = df2[important_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f3acd",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b6c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_and_encode(df):\n",
    "    df = df.replace('-', np.nan)\n",
    "    df = df.dropna(axis=1, how='all').drop_duplicates()\n",
    "    le = LabelEncoder()\n",
    "    filtered = []\n",
    "    for col in df.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        if df[col].nunique() <= 25 and col != 'label':\n",
    "            filtered.append(col)\n",
    "            mask = df[col].isnull()\n",
    "            df.loc[mask, col] = -1\n",
    "            df.loc[~mask, col] = le.fit_transform(df[col][~mask])\n",
    "        else:\n",
    "            if df[col].dtype == \"object\":\n",
    "                df[col] = df[col].str.len()\n",
    "            df[col] = df[col].fillna(-1)\n",
    "    return df, filtered\n",
    "\n",
    "def preprocess_data_3(df, df2):\n",
    "    df['label'] = df['label'].replace({1: -1, 0: 1})\n",
    "    df2['label'] = df2['label'].replace({1: -1, 0: 1})\n",
    "    df, filteredColumn = clean_and_encode(df)\n",
    "    df2, _ = clean_and_encode(df2)\n",
    "    \n",
    "    benign_df = df[df['label'] == 1]\n",
    "    malware_df = df[df['label'] == -1]\n",
    "    train_df = benign_df.iloc[malware_df.shape[0]:]\n",
    "    test_df = pd.concat([benign_df.iloc[:malware_df.shape[0]], malware_df])\n",
    "\n",
    "    benign_df2 = df2[df2['label'] == 1]\n",
    "    malware_df2 = df2[df2['label'] == -1]\n",
    "    \n",
    "    print(filteredColumn)\n",
    "\n",
    "    train_means = benign_df[filteredColumn].mean()\n",
    "    test_means = benign_df2[filteredColumn].mean()\n",
    "\n",
    "    abs_diff = (train_means - test_means).abs()\n",
    "    df_diff = pd.DataFrame({\n",
    "        \"Train means\": train_means,\n",
    "        \"Test means\": test_means,\n",
    "        \"Diff\": abs_diff\n",
    "    }).sort_values(by=\"Diff\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x=range(len(filteredColumn)), y=train_means.values, label=\"Train Benign\")\n",
    "    sns.lineplot(x=range(len(filteredColumn)), y=test_means.values, label=\"Test Benign\")\n",
    "    plt.xticks(range(len(filteredColumn)), filteredColumn, rotation=90)\n",
    "    plt.title(\"Feature Distribution Shift (Benign)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    benign_shuffled = benign_df2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    malware_shuffled = malware_df2.sample(frac=1, random_state=123).reset_index(drop=True)  # Different seed\n",
    "    \n",
    "    min_size = min(len(benign_shuffled), len(malware_shuffled))\n",
    "    benign_for_test = benign_shuffled.iloc[:min_size]\n",
    "    malware_for_test = malware_shuffled.iloc[:min_size]\n",
    "    \n",
    "    benign_test_chunks = np.array_split(benign_for_test, 10)\n",
    "    malware_test_chunks = np.array_split(malware_for_test, 10)\n",
    "    \n",
    "    test_df2 = pd.concat([benign_for_test, malware_for_test])\n",
    "    test_chunks = []\n",
    "    for i in range(10):\n",
    "        combined_chunk = pd.concat([benign_test_chunks[i], malware_test_chunks[i]], ignore_index=True)\n",
    "        combined_chunk = combined_chunk.sample(frac=1, random_state=i*10).reset_index(drop=True)\n",
    "        test_chunks.append(combined_chunk)\n",
    "        print(f\"Test chunk {i+1}: {len(combined_chunk)} samples, \"\n",
    "              f\"benign: {len(combined_chunk[combined_chunk['label']==1])}, \"\n",
    "              f\"malware: {len(combined_chunk[combined_chunk['label']==-1])}\")\n",
    "    \n",
    "    remaining_benign = benign_shuffled.iloc[min_size:] if len(benign_shuffled) > min_size else benign_shuffled\n",
    "    \n",
    "    train_chunks = []\n",
    "    for i in range(10):\n",
    "        if len(remaining_benign) > 1000:  # If we have enough data\n",
    "            train_sample = remaining_benign.sample(n=min(len(remaining_benign), 5000), \n",
    "                                                 random_state=i*100, \n",
    "                                                 replace=False).reset_index(drop=True)\n",
    "        else:\n",
    "            train_sample = remaining_benign.sample(n=5000, \n",
    "                                                 random_state=i*100, \n",
    "                                                 replace=True).reset_index(drop=True)\n",
    "        \n",
    "        train_chunks.append(train_sample)\n",
    "        print(f\"Train chunk {i+1}: {len(train_sample)} benign samples\")\n",
    "    \n",
    "    print(\"\\nVerifying chunk diversity:\")\n",
    "    for i in range(min(5, len(train_chunks)-1)):\n",
    "        similarity = len(pd.merge(train_chunks[i], train_chunks[i+1], how='inner')) / len(train_chunks[i])\n",
    "        print(f\"Train chunks {i+1} and {i+2} similarity: {similarity:.2%}\")\n",
    "    \n",
    "    for i in range(min(5, len(test_chunks)-1)):\n",
    "        similarity = len(pd.merge(test_chunks[i], test_chunks[i+1], how='inner')) / len(test_chunks[i])\n",
    "        print(f\"Test chunks {i+1} and {i+2} similarity: {similarity:.2%}\")\n",
    "\n",
    "    return df, train_df, test_df, df2, train_chunks, test_chunks, remaining_benign, pd.concat([benign_df2.iloc[:min_size], malware_df2.iloc[:min_size]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8b68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_50595/3903420739.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['label'].replace({1: -1, 0: 1})\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_50595/3903420739.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['label'] = df2['label'].replace({1: -1, 0: 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EventID', 'TargetImage', 'IntegrityLevel', 'User']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/alibidetect/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk 1: 13934 samples, benign: 6967, malware: 6967\n",
      "Test chunk 2: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 3: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 4: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 5: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 6: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 7: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 8: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 9: 13932 samples, benign: 6966, malware: 6966\n",
      "Test chunk 10: 13932 samples, benign: 6966, malware: 6966\n",
      "Train chunk 1: 5000 benign samples\n",
      "Train chunk 2: 5000 benign samples\n",
      "Train chunk 3: 5000 benign samples\n",
      "Train chunk 4: 5000 benign samples\n",
      "Train chunk 5: 5000 benign samples\n",
      "Train chunk 6: 5000 benign samples\n",
      "Train chunk 7: 5000 benign samples\n",
      "Train chunk 8: 5000 benign samples\n",
      "Train chunk 9: 5000 benign samples\n",
      "Train chunk 10: 5000 benign samples\n",
      "\n",
      "Verifying chunk diversity:\n",
      "Train chunks 1 and 2 similarity: 267.12%\n",
      "Train chunks 2 and 3 similarity: 304.30%\n",
      "Train chunks 3 and 4 similarity: 279.16%\n",
      "Train chunks 4 and 5 similarity: 270.08%\n",
      "Train chunks 5 and 6 similarity: 282.02%\n",
      "Test chunks 1 and 2 similarity: 18318.32%\n",
      "Test chunks 2 and 3 similarity: 18376.18%\n",
      "Test chunks 3 and 4 similarity: 18777.82%\n",
      "Test chunks 4 and 5 similarity: 18781.07%\n",
      "Test chunks 5 and 6 similarity: 18148.15%\n"
     ]
    }
   ],
   "source": [
    "ndf, train_df, test_df, ndf2, train_chunks, test_chunks, train_df2, test_df2 = preprocess_data_3(selected_df, selected_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0ef458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## import packages\n",
    "import sys\n",
    "sys.path.append('../admodels/')\n",
    "sys.path.append('../moudles/')\n",
    "sys.path.append('../baselines/')\n",
    "sys.path.append('../')\n",
    "\n",
    "from admodels.AE import AE\n",
    "from admodels.AE import train\n",
    "from admodels.AE import test\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e69d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def count_stats(df, outlier=1, label=1):\n",
    "    tmp = df.apply(lambda x : True if x['outlier'] == outlier and x['label'] == label else False, axis = 1)\n",
    "    return len(tmp[tmp == True].index)\n",
    "\n",
    "def test_model(clf, df, thres):\n",
    "    X_test = df.drop('label', axis=1)\n",
    "    y_true = df['label'].to_numpy()\n",
    "    outliers_predicted, rmse_vec = test(clf, thres, X_test) \n",
    "    new_df = df.copy()\n",
    "    new_df['outlier'] = outliers_predicted\n",
    "    # df.to_csv('y_pred_ae.csv', index=False)\n",
    "    tp = count_stats(new_df, outlier=-1, label=-1)\n",
    "    fn = count_stats(new_df, outlier=1, label=-1)\n",
    "    fp = count_stats(new_df, outlier=-1, label=1)\n",
    "    tn = count_stats(new_df, outlier=1, label=1)\n",
    "    # print({f'tp: {tp}'})\n",
    "    # print({f'fp: {fp}'})\n",
    "    # print({f'fn: {fn}'})\n",
    "    # print({f'tn: {tn}'})\n",
    "\n",
    "    rmse_outliers = rmse_vec[y_true == -1]\n",
    "    # print(f\" Total true outliers in this test set: {len(rmse_outliers)}\")\n",
    "    # print(f\" Number of those with RMSE > threshold: {np.sum(rmse_outliers > thres)}\")\n",
    "    print(f\"Max RMSE among outliers: {np.max(rmse_outliers):.2f}\")\n",
    "    # print(f\" RMSE of first 5 outliers: {rmse_outliers[:5]}\")\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "    return new_df, recall, precision, f1, tp, fn, fp, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103334e3",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5815053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 ----------------\n",
      "Step 0: Loss = 76377.1171875\n",
      "Step 10: Loss = 70782.5546875\n",
      "Step 20: Loss = 66628.578125\n",
      "epoch:0/100 |Loss: 79225.3125\n",
      "\n",
      "Epoch 2/100 ----------------\n",
      "Step 0: Loss = 75723.8984375\n",
      "Step 10: Loss = 74036.6328125\n",
      "Step 20: Loss = 71990.5390625\n",
      "epoch:1/100 |Loss: 72866.1328125\n",
      "\n",
      "Epoch 3/100 ----------------\n",
      "Step 0: Loss = 76234.9453125\n",
      "Step 10: Loss = 68619.84375\n",
      "Step 20: Loss = 71715.734375\n",
      "epoch:2/100 |Loss: 67318.53125\n",
      "\n",
      "Epoch 4/100 ----------------\n",
      "Step 0: Loss = 63801.78125\n",
      "Step 10: Loss = 65578.5234375\n",
      "Step 20: Loss = 75635.9375\n",
      "epoch:3/100 |Loss: 88165.4609375\n",
      "\n",
      "Epoch 5/100 ----------------\n",
      "Step 0: Loss = 66564.6015625\n",
      "Step 10: Loss = 77291.125\n",
      "Step 20: Loss = 67797.046875\n",
      "epoch:4/100 |Loss: 47379.765625\n",
      "\n",
      "Epoch 6/100 ----------------\n",
      "Step 0: Loss = 65146.25\n",
      "Step 10: Loss = 66407.390625\n",
      "Step 20: Loss = 73314.5703125\n",
      "epoch:5/100 |Loss: 65816.4140625\n",
      "\n",
      "Epoch 7/100 ----------------\n",
      "Step 0: Loss = 64777.79296875\n",
      "Step 10: Loss = 69866.4453125\n",
      "Step 20: Loss = 66201.0234375\n",
      "epoch:6/100 |Loss: 86192.078125\n",
      "\n",
      "Epoch 8/100 ----------------\n",
      "Step 0: Loss = 74206.4140625\n",
      "Step 10: Loss = 65518.73828125\n",
      "Step 20: Loss = 61619.50390625\n",
      "epoch:7/100 |Loss: 71199.734375\n",
      "\n",
      "Epoch 9/100 ----------------\n",
      "Step 0: Loss = 69579.8671875\n",
      "Step 10: Loss = 68093.2265625\n",
      "Step 20: Loss = 62849.69921875\n",
      "epoch:8/100 |Loss: 68969.6171875\n",
      "\n",
      "Epoch 10/100 ----------------\n",
      "Step 0: Loss = 69476.9375\n",
      "Step 10: Loss = 68303.6953125\n",
      "Step 20: Loss = 68668.4296875\n",
      "epoch:9/100 |Loss: 80488.3828125\n",
      "\n",
      "Epoch 11/100 ----------------\n",
      "Step 0: Loss = 69525.796875\n",
      "Step 10: Loss = 78672.15625\n",
      "Step 20: Loss = 59907.26953125\n",
      "epoch:10/100 |Loss: 55119.203125\n",
      "\n",
      "Epoch 12/100 ----------------\n",
      "Step 0: Loss = 63006.86328125\n",
      "Step 10: Loss = 73501.875\n",
      "Step 20: Loss = 73402.59375\n",
      "epoch:11/100 |Loss: 72735.515625\n",
      "\n",
      "Epoch 13/100 ----------------\n",
      "Step 0: Loss = 61608.30078125\n",
      "Step 10: Loss = 75648.078125\n",
      "Step 20: Loss = 69654.3125\n",
      "epoch:12/100 |Loss: 74132.5546875\n",
      "\n",
      "Epoch 14/100 ----------------\n",
      "Step 0: Loss = 59542.34765625\n",
      "Step 10: Loss = 67807.59375\n",
      "Step 20: Loss = 67391.1796875\n",
      "epoch:13/100 |Loss: 41910.15625\n",
      "\n",
      "Epoch 15/100 ----------------\n",
      "Step 0: Loss = 56753.44140625\n",
      "Step 10: Loss = 59842.35546875\n",
      "Step 20: Loss = 65637.1640625\n",
      "epoch:14/100 |Loss: 67473.3984375\n",
      "\n",
      "Epoch 16/100 ----------------\n",
      "Step 0: Loss = 61996.64453125\n",
      "Step 10: Loss = 58517.0625\n",
      "Step 20: Loss = 62168.62109375\n",
      "epoch:15/100 |Loss: 59592.1796875\n",
      "\n",
      "Epoch 17/100 ----------------\n",
      "Step 0: Loss = 63228.99609375\n",
      "Step 10: Loss = 54541.140625\n",
      "Step 20: Loss = 58180.84765625\n",
      "epoch:16/100 |Loss: 88528.5546875\n",
      "\n",
      "Epoch 18/100 ----------------\n",
      "Step 0: Loss = 62580.609375\n",
      "Step 10: Loss = 56236.51171875\n",
      "Step 20: Loss = 59930.48046875\n",
      "epoch:17/100 |Loss: 33778.2734375\n",
      "\n",
      "Epoch 19/100 ----------------\n",
      "Step 0: Loss = 66332.7890625\n",
      "Step 10: Loss = 59338.96484375\n",
      "Step 20: Loss = 53797.890625\n",
      "epoch:18/100 |Loss: 51118.40234375\n",
      "\n",
      "Epoch 20/100 ----------------\n",
      "Step 0: Loss = 63229.27734375\n",
      "Step 10: Loss = 56808.89453125\n",
      "Step 20: Loss = 58755.359375\n",
      "epoch:19/100 |Loss: 37634.23828125\n",
      "\n",
      "Epoch 21/100 ----------------\n",
      "Step 0: Loss = 50547.609375\n",
      "Step 10: Loss = 52723.9375\n",
      "Step 20: Loss = 57490.76171875\n",
      "epoch:20/100 |Loss: 70917.875\n",
      "\n",
      "Epoch 22/100 ----------------\n",
      "Step 0: Loss = 56662.41796875\n",
      "Step 10: Loss = 47426.84765625\n",
      "Step 20: Loss = 55982.875\n",
      "epoch:21/100 |Loss: 49757.50390625\n",
      "\n",
      "Epoch 23/100 ----------------\n",
      "Step 0: Loss = 50874.26171875\n",
      "Step 10: Loss = 55613.94921875\n",
      "Step 20: Loss = 48422.88671875\n",
      "epoch:22/100 |Loss: 45780.66015625\n",
      "\n",
      "Epoch 24/100 ----------------\n",
      "Step 0: Loss = 49893.25\n",
      "Step 10: Loss = 60554.53125\n",
      "Step 20: Loss = 41577.44921875\n",
      "epoch:23/100 |Loss: 39306.2890625\n",
      "\n",
      "Epoch 25/100 ----------------\n",
      "Step 0: Loss = 48542.671875\n",
      "Step 10: Loss = 47462.703125\n",
      "Step 20: Loss = 54147.03515625\n",
      "epoch:24/100 |Loss: 52908.91015625\n",
      "\n",
      "Epoch 26/100 ----------------\n",
      "Step 0: Loss = 44076.46484375\n",
      "Step 10: Loss = 48889.546875\n",
      "Step 20: Loss = 43240.09375\n",
      "epoch:25/100 |Loss: 47556.77734375\n",
      "\n",
      "Epoch 27/100 ----------------\n",
      "Step 0: Loss = 40437.89453125\n",
      "Step 10: Loss = 51724.45703125\n",
      "Step 20: Loss = 41317.75\n",
      "epoch:26/100 |Loss: 25174.04296875\n",
      "\n",
      "Epoch 28/100 ----------------\n",
      "Step 0: Loss = 38911.234375\n",
      "Step 10: Loss = 52530.35546875\n",
      "Step 20: Loss = 48955.30078125\n",
      "epoch:27/100 |Loss: 33583.62890625\n",
      "\n",
      "Epoch 29/100 ----------------\n",
      "Step 0: Loss = 42088.86328125\n",
      "Step 10: Loss = 38374.55859375\n",
      "Step 20: Loss = 44435.93359375\n",
      "epoch:28/100 |Loss: 46128.25390625\n",
      "\n",
      "Epoch 30/100 ----------------\n",
      "Step 0: Loss = 44174.26953125\n",
      "Step 10: Loss = 40925.703125\n",
      "Step 20: Loss = 44506.51953125\n",
      "epoch:29/100 |Loss: 40958.640625\n",
      "\n",
      "Epoch 31/100 ----------------\n",
      "Step 0: Loss = 44130.65625\n",
      "Step 10: Loss = 40438.05859375\n",
      "Step 20: Loss = 44139.05859375\n",
      "epoch:30/100 |Loss: 41530.609375\n",
      "\n",
      "Epoch 32/100 ----------------\n",
      "Step 0: Loss = 42519.078125\n",
      "Step 10: Loss = 35341.19921875\n",
      "Step 20: Loss = 33945.203125\n",
      "epoch:31/100 |Loss: 42241.1328125\n",
      "\n",
      "Epoch 33/100 ----------------\n",
      "Step 0: Loss = 34028.43359375\n",
      "Step 10: Loss = 39154.47265625\n",
      "Step 20: Loss = 34160.9140625\n",
      "epoch:32/100 |Loss: 26827.966796875\n",
      "\n",
      "Epoch 34/100 ----------------\n",
      "Step 0: Loss = 38367.69140625\n",
      "Step 10: Loss = 39690.20703125\n",
      "Step 20: Loss = 33863.0234375\n",
      "epoch:33/100 |Loss: 24362.076171875\n",
      "\n",
      "Epoch 35/100 ----------------\n",
      "Step 0: Loss = 33691.99609375\n",
      "Step 10: Loss = 34084.3515625\n",
      "Step 20: Loss = 33623.30078125\n",
      "epoch:34/100 |Loss: 30954.509765625\n",
      "\n",
      "Epoch 36/100 ----------------\n",
      "Step 0: Loss = 32695.197265625\n",
      "Step 10: Loss = 35657.19140625\n",
      "Step 20: Loss = 31608.693359375\n",
      "epoch:35/100 |Loss: 29459.7421875\n",
      "\n",
      "Epoch 37/100 ----------------\n",
      "Step 0: Loss = 31373.494140625\n",
      "Step 10: Loss = 37230.25390625\n",
      "Step 20: Loss = 26843.822265625\n",
      "epoch:36/100 |Loss: 34796.09765625\n",
      "\n",
      "Epoch 38/100 ----------------\n",
      "Step 0: Loss = 27696.380859375\n",
      "Step 10: Loss = 35058.37109375\n",
      "Step 20: Loss = 27354.771484375\n",
      "epoch:37/100 |Loss: 34689.90234375\n",
      "\n",
      "Epoch 39/100 ----------------\n",
      "Step 0: Loss = 35238.7578125\n",
      "Step 10: Loss = 25675.25\n",
      "Step 20: Loss = 29103.158203125\n",
      "epoch:38/100 |Loss: 24030.634765625\n",
      "\n",
      "Epoch 40/100 ----------------\n",
      "Step 0: Loss = 25591.900390625\n",
      "Step 10: Loss = 28141.09375\n",
      "Step 20: Loss = 27542.8515625\n",
      "epoch:39/100 |Loss: 30833.3046875\n",
      "\n",
      "Epoch 41/100 ----------------\n",
      "Step 0: Loss = 24904.7578125\n",
      "Step 10: Loss = 24709.5546875\n",
      "Step 20: Loss = 25527.095703125\n",
      "epoch:40/100 |Loss: 32382.537109375\n",
      "\n",
      "Epoch 42/100 ----------------\n",
      "Step 0: Loss = 32825.32421875\n",
      "Step 10: Loss = 25128.96875\n",
      "Step 20: Loss = 25553.005859375\n",
      "epoch:41/100 |Loss: 26890.03125\n",
      "\n",
      "Epoch 43/100 ----------------\n",
      "Step 0: Loss = 23181.001953125\n",
      "Step 10: Loss = 23644.109375\n",
      "Step 20: Loss = 23719.306640625\n",
      "epoch:42/100 |Loss: 22358.498046875\n",
      "\n",
      "Epoch 44/100 ----------------\n",
      "Step 0: Loss = 21096.001953125\n",
      "Step 10: Loss = 23713.267578125\n",
      "Step 20: Loss = 23072.177734375\n",
      "epoch:43/100 |Loss: 19479.4453125\n",
      "\n",
      "Epoch 45/100 ----------------\n",
      "Step 0: Loss = 21957.025390625\n",
      "Step 10: Loss = 19668.2421875\n",
      "Step 20: Loss = 22391.484375\n",
      "epoch:44/100 |Loss: 10568.419921875\n",
      "\n",
      "Epoch 46/100 ----------------\n",
      "Step 0: Loss = 16789.029296875\n",
      "Step 10: Loss = 22536.982421875\n",
      "Step 20: Loss = 18384.466796875\n",
      "epoch:45/100 |Loss: 10307.103515625\n",
      "\n",
      "Epoch 47/100 ----------------\n",
      "Step 0: Loss = 18158.478515625\n",
      "Step 10: Loss = 20647.353515625\n",
      "Step 20: Loss = 17964.35546875\n",
      "epoch:46/100 |Loss: 19974.9765625\n",
      "\n",
      "Epoch 48/100 ----------------\n",
      "Step 0: Loss = 14673.3798828125\n",
      "Step 10: Loss = 15309.7314453125\n",
      "Step 20: Loss = 19695.74609375\n",
      "epoch:47/100 |Loss: 22784.263671875\n",
      "\n",
      "Epoch 49/100 ----------------\n",
      "Step 0: Loss = 15998.69921875\n",
      "Step 10: Loss = 15204.0869140625\n",
      "Step 20: Loss = 15822.23046875\n",
      "epoch:48/100 |Loss: 6188.57177734375\n",
      "\n",
      "Epoch 50/100 ----------------\n",
      "Step 0: Loss = 15333.5859375\n",
      "Step 10: Loss = 16130.2470703125\n",
      "Step 20: Loss = 18397.771484375\n",
      "epoch:49/100 |Loss: 17306.916015625\n",
      "\n",
      "Epoch 51/100 ----------------\n",
      "Step 0: Loss = 17407.513671875\n",
      "Step 10: Loss = 15623.7939453125\n",
      "Step 20: Loss = 10430.2890625\n",
      "epoch:50/100 |Loss: 18569.609375\n",
      "\n",
      "Epoch 52/100 ----------------\n",
      "Step 0: Loss = 14306.0478515625\n",
      "Step 10: Loss = 13046.0537109375\n",
      "Step 20: Loss = 10011.564453125\n",
      "epoch:51/100 |Loss: 8895.9833984375\n",
      "\n",
      "Epoch 53/100 ----------------\n",
      "Step 0: Loss = 15369.3974609375\n",
      "Step 10: Loss = 10539.8388671875\n",
      "Step 20: Loss = 9764.9775390625\n",
      "epoch:52/100 |Loss: 8115.099609375\n",
      "\n",
      "Epoch 54/100 ----------------\n",
      "Step 0: Loss = 12048.4169921875\n",
      "Step 10: Loss = 10858.0810546875\n",
      "Step 20: Loss = 12819.46875\n",
      "epoch:53/100 |Loss: 6095.884765625\n",
      "\n",
      "Epoch 55/100 ----------------\n",
      "Step 0: Loss = 8969.6845703125\n",
      "Step 10: Loss = 14643.0244140625\n",
      "Step 20: Loss = 10515.1943359375\n",
      "epoch:54/100 |Loss: 16151.087890625\n",
      "\n",
      "Epoch 56/100 ----------------\n",
      "Step 0: Loss = 8168.64501953125\n",
      "Step 10: Loss = 10886.3056640625\n",
      "Step 20: Loss = 8997.6826171875\n",
      "epoch:55/100 |Loss: 5731.17919921875\n",
      "\n",
      "Epoch 57/100 ----------------\n",
      "Step 0: Loss = 7758.75244140625\n",
      "Step 10: Loss = 9213.37109375\n",
      "Step 20: Loss = 11310.1181640625\n",
      "epoch:56/100 |Loss: 23289.677734375\n",
      "\n",
      "Epoch 58/100 ----------------\n",
      "Step 0: Loss = 7070.51220703125\n",
      "Step 10: Loss = 10057.8056640625\n",
      "Step 20: Loss = 9333.0966796875\n",
      "epoch:57/100 |Loss: 7093.53759765625\n",
      "\n",
      "Epoch 59/100 ----------------\n",
      "Step 0: Loss = 10090.8857421875\n",
      "Step 10: Loss = 8508.2841796875\n",
      "Step 20: Loss = 7829.037109375\n",
      "epoch:58/100 |Loss: 12299.7255859375\n",
      "\n",
      "Epoch 60/100 ----------------\n",
      "Step 0: Loss = 8057.68798828125\n",
      "Step 10: Loss = 7320.52978515625\n",
      "Step 20: Loss = 6790.76953125\n",
      "epoch:59/100 |Loss: 5600.5625\n",
      "\n",
      "Epoch 61/100 ----------------\n",
      "Step 0: Loss = 9170.7822265625\n",
      "Step 10: Loss = 6981.623046875\n",
      "Step 20: Loss = 5369.8486328125\n",
      "epoch:60/100 |Loss: 4772.1513671875\n",
      "\n",
      "Epoch 62/100 ----------------\n",
      "Step 0: Loss = 7670.75\n",
      "Step 10: Loss = 6405.23095703125\n",
      "Step 20: Loss = 8612.4130859375\n",
      "epoch:61/100 |Loss: 10896.4287109375\n",
      "\n",
      "Epoch 63/100 ----------------\n",
      "Step 0: Loss = 6555.79248046875\n",
      "Step 10: Loss = 5484.48486328125\n",
      "Step 20: Loss = 5130.12890625\n",
      "epoch:62/100 |Loss: 3275.55078125\n",
      "\n",
      "Epoch 64/100 ----------------\n",
      "Step 0: Loss = 5958.95263671875\n",
      "Step 10: Loss = 6165.244140625\n",
      "Step 20: Loss = 6267.91748046875\n",
      "epoch:63/100 |Loss: 8091.46630859375\n",
      "\n",
      "Epoch 65/100 ----------------\n",
      "Step 0: Loss = 4176.7900390625\n",
      "Step 10: Loss = 5099.87255859375\n",
      "Step 20: Loss = 7470.28173828125\n",
      "epoch:64/100 |Loss: 2168.533447265625\n",
      "\n",
      "Epoch 66/100 ----------------\n",
      "Step 0: Loss = 9394.3056640625\n",
      "Step 10: Loss = 5018.20068359375\n",
      "Step 20: Loss = 5643.86962890625\n",
      "epoch:65/100 |Loss: 5762.99072265625\n",
      "\n",
      "Epoch 67/100 ----------------\n",
      "Step 0: Loss = 5503.6015625\n",
      "Step 10: Loss = 3447.392578125\n",
      "Step 20: Loss = 3669.049560546875\n",
      "epoch:66/100 |Loss: 1431.4166259765625\n",
      "\n",
      "Epoch 68/100 ----------------\n",
      "Step 0: Loss = 4166.81396484375\n",
      "Step 10: Loss = 3218.9580078125\n",
      "Step 20: Loss = 3908.715576171875\n",
      "epoch:67/100 |Loss: 1641.5565185546875\n",
      "\n",
      "Epoch 69/100 ----------------\n",
      "Step 0: Loss = 3406.2568359375\n",
      "Step 10: Loss = 3384.444580078125\n",
      "Step 20: Loss = 4200.62451171875\n",
      "epoch:68/100 |Loss: 2031.484619140625\n",
      "\n",
      "Epoch 70/100 ----------------\n",
      "Step 0: Loss = 4820.7841796875\n",
      "Step 10: Loss = 3276.330078125\n",
      "Step 20: Loss = 2044.4495849609375\n",
      "epoch:69/100 |Loss: 1620.3375244140625\n",
      "\n",
      "Epoch 71/100 ----------------\n",
      "Step 0: Loss = 2184.605712890625\n",
      "Step 10: Loss = 2396.709228515625\n",
      "Step 20: Loss = 2753.173828125\n",
      "epoch:70/100 |Loss: 7327.42626953125\n",
      "\n",
      "Epoch 72/100 ----------------\n",
      "Step 0: Loss = 3915.263427734375\n",
      "Step 10: Loss = 2036.4112548828125\n",
      "Step 20: Loss = 3459.357177734375\n",
      "epoch:71/100 |Loss: 1582.23828125\n",
      "\n",
      "Epoch 73/100 ----------------\n",
      "Step 0: Loss = 2709.867431640625\n",
      "Step 10: Loss = 2208.934814453125\n",
      "Step 20: Loss = 2750.553955078125\n",
      "epoch:72/100 |Loss: 2381.963623046875\n",
      "\n",
      "Epoch 74/100 ----------------\n",
      "Step 0: Loss = 2230.784912109375\n",
      "Step 10: Loss = 3360.874755859375\n",
      "Step 20: Loss = 3386.374755859375\n",
      "epoch:73/100 |Loss: 904.6098022460938\n",
      "\n",
      "Epoch 75/100 ----------------\n",
      "Step 0: Loss = 2601.573974609375\n",
      "Step 10: Loss = 3118.780029296875\n",
      "Step 20: Loss = 1661.1531982421875\n",
      "epoch:74/100 |Loss: 2740.68408203125\n",
      "\n",
      "Epoch 76/100 ----------------\n",
      "Step 0: Loss = 1441.5198974609375\n",
      "Step 10: Loss = 2169.41748046875\n",
      "Step 20: Loss = 2255.30810546875\n",
      "epoch:75/100 |Loss: 1505.5009765625\n",
      "\n",
      "Epoch 77/100 ----------------\n",
      "Step 0: Loss = 2424.847412109375\n",
      "Step 10: Loss = 3025.744873046875\n",
      "Step 20: Loss = 1774.6177978515625\n",
      "epoch:76/100 |Loss: 3393.912841796875\n",
      "\n",
      "Epoch 78/100 ----------------\n",
      "Step 0: Loss = 1367.4013671875\n",
      "Step 10: Loss = 1119.3284912109375\n",
      "Step 20: Loss = 1735.2642822265625\n",
      "epoch:77/100 |Loss: 1685.884765625\n",
      "\n",
      "Epoch 79/100 ----------------\n",
      "Step 0: Loss = 1781.8671875\n",
      "Step 10: Loss = 1780.9447021484375\n",
      "Step 20: Loss = 2601.582275390625\n",
      "epoch:78/100 |Loss: 5356.67431640625\n",
      "\n",
      "Epoch 80/100 ----------------\n",
      "Step 0: Loss = 1047.036376953125\n",
      "Step 10: Loss = 2343.927001953125\n",
      "Step 20: Loss = 933.3480834960938\n",
      "epoch:79/100 |Loss: 780.4988403320312\n",
      "\n",
      "Epoch 81/100 ----------------\n",
      "Step 0: Loss = 2064.00732421875\n",
      "Step 10: Loss = 705.908447265625\n",
      "Step 20: Loss = 1258.72314453125\n",
      "epoch:80/100 |Loss: 930.0200805664062\n",
      "\n",
      "Epoch 82/100 ----------------\n",
      "Step 0: Loss = 1792.4005126953125\n",
      "Step 10: Loss = 817.5250854492188\n",
      "Step 20: Loss = 1233.225830078125\n",
      "epoch:81/100 |Loss: 1688.2017822265625\n",
      "\n",
      "Epoch 83/100 ----------------\n",
      "Step 0: Loss = 433.9266052246094\n",
      "Step 10: Loss = 1969.5521240234375\n",
      "Step 20: Loss = 901.9470825195312\n",
      "epoch:82/100 |Loss: 3095.66748046875\n",
      "\n",
      "Epoch 84/100 ----------------\n",
      "Step 0: Loss = 970.888671875\n",
      "Step 10: Loss = 1253.5191650390625\n",
      "Step 20: Loss = 709.6317749023438\n",
      "epoch:83/100 |Loss: 1425.728515625\n",
      "\n",
      "Epoch 85/100 ----------------\n",
      "Step 0: Loss = 815.65234375\n",
      "Step 10: Loss = 715.0825805664062\n",
      "Step 20: Loss = 497.365966796875\n",
      "epoch:84/100 |Loss: 1326.9691162109375\n",
      "\n",
      "Epoch 86/100 ----------------\n",
      "Step 0: Loss = 379.13525390625\n",
      "Step 10: Loss = 694.6353149414062\n",
      "Step 20: Loss = 551.5101318359375\n",
      "epoch:85/100 |Loss: 285.0826416015625\n",
      "\n",
      "Epoch 87/100 ----------------\n",
      "Step 0: Loss = 728.0878295898438\n",
      "Step 10: Loss = 342.9027404785156\n",
      "Step 20: Loss = 520.6939086914062\n",
      "epoch:86/100 |Loss: 934.7124633789062\n",
      "\n",
      "Epoch 88/100 ----------------\n",
      "Step 0: Loss = 322.315185546875\n",
      "Step 10: Loss = 221.9697723388672\n",
      "Step 20: Loss = 653.4479370117188\n",
      "epoch:87/100 |Loss: 3085.49560546875\n",
      "\n",
      "Epoch 89/100 ----------------\n",
      "Step 0: Loss = 269.679443359375\n",
      "Step 10: Loss = 2230.403076171875\n",
      "Step 20: Loss = 816.3541870117188\n",
      "epoch:88/100 |Loss: 2609.618408203125\n",
      "\n",
      "Epoch 90/100 ----------------\n",
      "Step 0: Loss = 1052.1417236328125\n",
      "Step 10: Loss = 775.3513793945312\n",
      "Step 20: Loss = 460.2330627441406\n",
      "epoch:89/100 |Loss: 3517.177978515625\n",
      "\n",
      "Epoch 91/100 ----------------\n",
      "Step 0: Loss = 279.4687805175781\n",
      "Step 10: Loss = 804.0542602539062\n",
      "Step 20: Loss = 901.91357421875\n",
      "epoch:90/100 |Loss: 539.157958984375\n",
      "\n",
      "Epoch 92/100 ----------------\n",
      "Step 0: Loss = 190.41184997558594\n",
      "Step 10: Loss = 268.9477844238281\n",
      "Step 20: Loss = 260.6229553222656\n",
      "epoch:91/100 |Loss: 951.8922119140625\n",
      "\n",
      "Epoch 93/100 ----------------\n",
      "Step 0: Loss = 199.853759765625\n",
      "Step 10: Loss = 140.7371063232422\n",
      "Step 20: Loss = 139.5699005126953\n",
      "epoch:92/100 |Loss: 2387.248779296875\n",
      "\n",
      "Epoch 94/100 ----------------\n",
      "Step 0: Loss = 133.66477966308594\n",
      "Step 10: Loss = 376.0596618652344\n",
      "Step 20: Loss = 62.8851318359375\n",
      "epoch:93/100 |Loss: 488.5621643066406\n",
      "\n",
      "Epoch 95/100 ----------------\n",
      "Step 0: Loss = 497.0192565917969\n",
      "Step 10: Loss = 57.955780029296875\n",
      "Step 20: Loss = 832.3721313476562\n",
      "epoch:94/100 |Loss: 1189.6715087890625\n",
      "\n",
      "Epoch 96/100 ----------------\n",
      "Step 0: Loss = 126.07511138916016\n",
      "Step 10: Loss = 163.14450073242188\n",
      "Step 20: Loss = 420.1513366699219\n",
      "epoch:95/100 |Loss: 154.20413208007812\n",
      "\n",
      "Epoch 97/100 ----------------\n",
      "Step 0: Loss = 173.9705810546875\n",
      "Step 10: Loss = 98.86917877197266\n",
      "Step 20: Loss = 186.57411193847656\n",
      "epoch:96/100 |Loss: 1578.3109130859375\n",
      "\n",
      "Epoch 98/100 ----------------\n",
      "Step 0: Loss = 47.11789321899414\n",
      "Step 10: Loss = 346.8652038574219\n",
      "Step 20: Loss = 184.7093505859375\n",
      "epoch:97/100 |Loss: 875.6107788085938\n",
      "\n",
      "Epoch 99/100 ----------------\n",
      "Step 0: Loss = 99.8375473022461\n",
      "Step 10: Loss = 90.3509750366211\n",
      "Step 20: Loss = 407.8509521484375\n",
      "epoch:98/100 |Loss: 784.8497924804688\n",
      "\n",
      "Epoch 100/100 ----------------\n",
      "Step 0: Loss = 89.35765838623047\n",
      "Step 10: Loss = 72.80410766601562\n",
      "Step 20: Loss = 66.25889587402344\n",
      "epoch:99/100 |Loss: 722.9588012695312\n",
      "max AD score 42.567055\n",
      "thres: 20.724709\n",
      "Max RMSE among outliers: 59.01\n",
      "Data training:\n",
      "Precision: 0.903, Recall: 0.394, F1-score: 0.548\n",
      "Elapsed: 0.1493\n",
      "Max RMSE among outliers: 947.68\n",
      "Data testing:\n",
      "Precision: 0.444, Recall: 0.757, F1-score: 0.559\n",
      "Elapsed: 2.2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_95926/2941342948.py:48: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop('label', axis=1)\n",
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "start = time.time()\n",
    "_, recall, precision, f1, tp, fn, fp, tn = test_model(model, test_df, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data training:')\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp_2, fn_2, fp_2, tn_2 = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp_2, fn_2],\n",
    "    [fp_2, tn_2]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='grey_r', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix Control\")\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='grey_r', ax=axs[1])\n",
    "axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps_pca.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e6d97",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd0094",
   "metadata": {},
   "source": [
    "### a. Concat old + new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d151e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "reference_vec = train_df.mean().values.reshape(1, -1)\n",
    "similarities = cosine_similarity(train_df2.values, reference_vec).flatten()\n",
    "\n",
    "k = len(train_df)\n",
    "top_k_idx = np.argsort(similarities)[-k:]\n",
    "X2_similar = train_df2.iloc[top_k_idx]\n",
    "\n",
    "retrain_df = pd.concat([train_df, X2_similar]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X_train = retrain_df.drop('label',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01b2072",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m()\n\u001b[1;32m      4\u001b[0m feature_size \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AE(feature_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "model = AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "start = time.time()\n",
    "_, recall, precision, f1, tp, fn, fp, tn = test_model(model, test_df, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data training:')\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp_2, fn_2, fp_2, tn_2 = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp_2, fn_2],\n",
    "    [fp_2, tn_2]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='grey_r', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix Control\")\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='grey_r', ax=axs[1])\n",
    "axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps_b_pca.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f2e2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9400 entries, 0 to 70519\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   TargetFilename     9400 non-null   float64\n",
      " 1   EventID            9400 non-null   int64  \n",
      " 2   TargetProcessGuid  9400 non-null   float64\n",
      " 3   TargetImage        9400 non-null   object \n",
      " 4   Company            9400 non-null   float64\n",
      " 5   Description        9400 non-null   float64\n",
      " 6   Product            9400 non-null   float64\n",
      " 7   IntegrityLevel     9400 non-null   object \n",
      " 8   ParentProcessGuid  9400 non-null   float64\n",
      " 9   User               9400 non-null   object \n",
      " 10  LogonId            9400 non-null   float64\n",
      " 11  ParentProcessId    9400 non-null   float64\n",
      " 12  label              9400 non-null   int64  \n",
      "dtypes: float64(8), int64(2), object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ca52f",
   "metadata": {},
   "source": [
    "### b. Full model replacement with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7225be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 ----------------\n",
      "Step 0: Loss = 37903144.0\n",
      "Step 10: Loss = 34007148.0\n",
      "Step 20: Loss = 36128248.0\n",
      "Step 30: Loss = 36298672.0\n",
      "Step 40: Loss = 34568628.0\n",
      "Step 50: Loss = 34982020.0\n",
      "epoch:0/100 |Loss: 37613576.0\n",
      "\n",
      "Epoch 2/100 ----------------\n",
      "Step 0: Loss = 33241602.0\n",
      "Step 10: Loss = 35987788.0\n",
      "Step 20: Loss = 35698332.0\n",
      "Step 30: Loss = 35766876.0\n",
      "Step 40: Loss = 36006476.0\n",
      "Step 50: Loss = 36314120.0\n",
      "epoch:1/100 |Loss: 35996680.0\n",
      "\n",
      "Epoch 3/100 ----------------\n",
      "Step 0: Loss = 34670108.0\n",
      "Step 10: Loss = 34434792.0\n",
      "Step 20: Loss = 34302732.0\n",
      "Step 30: Loss = 35784708.0\n",
      "Step 40: Loss = 35112672.0\n",
      "Step 50: Loss = 32871694.0\n",
      "epoch:2/100 |Loss: 33179642.0\n",
      "\n",
      "Epoch 4/100 ----------------\n",
      "Step 0: Loss = 35407024.0\n",
      "Step 10: Loss = 33636056.0\n",
      "Step 20: Loss = 36303076.0\n",
      "Step 30: Loss = 34396332.0\n",
      "Step 40: Loss = 32216810.0\n",
      "Step 50: Loss = 33552146.0\n",
      "epoch:3/100 |Loss: 36785752.0\n",
      "\n",
      "Epoch 5/100 ----------------\n",
      "Step 0: Loss = 35435644.0\n",
      "Step 10: Loss = 35189720.0\n",
      "Step 20: Loss = 36870820.0\n",
      "Step 30: Loss = 34211592.0\n",
      "Step 40: Loss = 36017404.0\n",
      "Step 50: Loss = 33518106.0\n",
      "epoch:4/100 |Loss: 36755688.0\n",
      "\n",
      "Epoch 6/100 ----------------\n",
      "Step 0: Loss = 35578588.0\n",
      "Step 10: Loss = 34259236.0\n",
      "Step 20: Loss = 33096952.0\n",
      "Step 30: Loss = 34022284.0\n",
      "Step 40: Loss = 36014908.0\n",
      "Step 50: Loss = 35678040.0\n",
      "epoch:5/100 |Loss: 34196380.0\n",
      "\n",
      "Epoch 7/100 ----------------\n",
      "Step 0: Loss = 34146280.0\n",
      "Step 10: Loss = 32513376.0\n",
      "Step 20: Loss = 36655768.0\n",
      "Step 30: Loss = 34540284.0\n",
      "Step 40: Loss = 32593320.0\n",
      "Step 50: Loss = 32493526.0\n",
      "epoch:6/100 |Loss: 33883468.0\n",
      "\n",
      "Epoch 8/100 ----------------\n",
      "Step 0: Loss = 35501244.0\n",
      "Step 10: Loss = 35591648.0\n",
      "Step 20: Loss = 33951672.0\n",
      "Step 30: Loss = 34945204.0\n",
      "Step 40: Loss = 36298276.0\n",
      "Step 50: Loss = 34272876.0\n",
      "epoch:7/100 |Loss: 35399876.0\n",
      "\n",
      "Epoch 9/100 ----------------\n",
      "Step 0: Loss = 35828892.0\n",
      "Step 10: Loss = 36880468.0\n",
      "Step 20: Loss = 35123596.0\n",
      "Step 30: Loss = 35566288.0\n",
      "Step 40: Loss = 33339158.0\n",
      "Step 50: Loss = 32944298.0\n",
      "epoch:8/100 |Loss: 33582672.0\n",
      "\n",
      "Epoch 10/100 ----------------\n",
      "Step 0: Loss = 33814804.0\n",
      "Step 10: Loss = 33186490.0\n",
      "Step 20: Loss = 35745824.0\n",
      "Step 30: Loss = 34034620.0\n",
      "Step 40: Loss = 37598928.0\n",
      "Step 50: Loss = 33904548.0\n",
      "epoch:9/100 |Loss: 35320356.0\n",
      "\n",
      "Epoch 11/100 ----------------\n",
      "Step 0: Loss = 33935432.0\n",
      "Step 10: Loss = 32916298.0\n",
      "Step 20: Loss = 32865872.0\n",
      "Step 30: Loss = 35187936.0\n",
      "Step 40: Loss = 34545936.0\n",
      "Step 50: Loss = 34310040.0\n",
      "epoch:10/100 |Loss: 34466972.0\n",
      "\n",
      "Epoch 12/100 ----------------\n",
      "Step 0: Loss = 34662504.0\n",
      "Step 10: Loss = 34230012.0\n",
      "Step 20: Loss = 36243148.0\n",
      "Step 30: Loss = 32449088.0\n",
      "Step 40: Loss = 36510988.0\n",
      "Step 50: Loss = 33306242.0\n",
      "epoch:11/100 |Loss: 35681292.0\n",
      "\n",
      "Epoch 13/100 ----------------\n",
      "Step 0: Loss = 33343518.0\n",
      "Step 10: Loss = 34296668.0\n",
      "Step 20: Loss = 34688468.0\n",
      "Step 30: Loss = 33665436.0\n",
      "Step 40: Loss = 36705928.0\n",
      "Step 50: Loss = 35272308.0\n",
      "epoch:12/100 |Loss: 32434946.0\n",
      "\n",
      "Epoch 14/100 ----------------\n",
      "Step 0: Loss = 34886724.0\n",
      "Step 10: Loss = 36201416.0\n",
      "Step 20: Loss = 33053382.0\n",
      "Step 30: Loss = 36757344.0\n",
      "Step 40: Loss = 34341236.0\n",
      "Step 50: Loss = 35671516.0\n",
      "epoch:13/100 |Loss: 32177582.0\n",
      "\n",
      "Epoch 15/100 ----------------\n",
      "Step 0: Loss = 32784808.0\n",
      "Step 10: Loss = 34605204.0\n",
      "Step 20: Loss = 34837928.0\n",
      "Step 30: Loss = 35758644.0\n",
      "Step 40: Loss = 33883252.0\n",
      "Step 50: Loss = 35469636.0\n",
      "epoch:14/100 |Loss: 32799822.0\n",
      "\n",
      "Epoch 16/100 ----------------\n",
      "Step 0: Loss = 33390542.0\n",
      "Step 10: Loss = 33730468.0\n",
      "Step 20: Loss = 34011192.0\n",
      "Step 30: Loss = 33217926.0\n",
      "Step 40: Loss = 36078952.0\n",
      "Step 50: Loss = 32490090.0\n",
      "epoch:15/100 |Loss: 31958694.0\n",
      "\n",
      "Epoch 17/100 ----------------\n",
      "Step 0: Loss = 32543318.0\n",
      "Step 10: Loss = 33889624.0\n",
      "Step 20: Loss = 32910078.0\n",
      "Step 30: Loss = 31506342.0\n",
      "Step 40: Loss = 35569072.0\n",
      "Step 50: Loss = 34291076.0\n",
      "epoch:16/100 |Loss: 34814596.0\n",
      "\n",
      "Epoch 18/100 ----------------\n",
      "Step 0: Loss = 33757816.0\n",
      "Step 10: Loss = 34772724.0\n",
      "Step 20: Loss = 34602148.0\n",
      "Step 30: Loss = 34657936.0\n",
      "Step 40: Loss = 31721798.0\n",
      "Step 50: Loss = 31698210.0\n",
      "epoch:17/100 |Loss: 33098148.0\n",
      "\n",
      "Epoch 19/100 ----------------\n",
      "Step 0: Loss = 33824260.0\n",
      "Step 10: Loss = 34584156.0\n",
      "Step 20: Loss = 33342064.0\n",
      "Step 30: Loss = 35244236.0\n",
      "Step 40: Loss = 35965980.0\n",
      "Step 50: Loss = 33046376.0\n",
      "epoch:18/100 |Loss: 34623048.0\n",
      "\n",
      "Epoch 20/100 ----------------\n",
      "Step 0: Loss = 34239116.0\n",
      "Step 10: Loss = 32495562.0\n",
      "Step 20: Loss = 33162200.0\n",
      "Step 30: Loss = 34025188.0\n",
      "Step 40: Loss = 33621424.0\n",
      "Step 50: Loss = 32503898.0\n",
      "epoch:19/100 |Loss: 33168034.0\n",
      "\n",
      "Epoch 21/100 ----------------\n",
      "Step 0: Loss = 33054936.0\n",
      "Step 10: Loss = 35276592.0\n",
      "Step 20: Loss = 32862376.0\n",
      "Step 30: Loss = 32903090.0\n",
      "Step 40: Loss = 32834346.0\n",
      "Step 50: Loss = 30941162.0\n",
      "epoch:20/100 |Loss: 33789244.0\n",
      "\n",
      "Epoch 22/100 ----------------\n",
      "Step 0: Loss = 35722356.0\n",
      "Step 10: Loss = 33824200.0\n",
      "Step 20: Loss = 33905276.0\n",
      "Step 30: Loss = 31956422.0\n",
      "Step 40: Loss = 31479550.0\n",
      "Step 50: Loss = 32718512.0\n",
      "epoch:21/100 |Loss: 33813008.0\n",
      "\n",
      "Epoch 23/100 ----------------\n",
      "Step 0: Loss = 34016740.0\n",
      "Step 10: Loss = 34788616.0\n",
      "Step 20: Loss = 32625096.0\n",
      "Step 30: Loss = 34257132.0\n",
      "Step 40: Loss = 36117504.0\n",
      "Step 50: Loss = 32672336.0\n",
      "epoch:22/100 |Loss: 32022372.0\n",
      "\n",
      "Epoch 24/100 ----------------\n",
      "Step 0: Loss = 31964496.0\n",
      "Step 10: Loss = 34069756.0\n",
      "Step 20: Loss = 35108600.0\n",
      "Step 30: Loss = 32546536.0\n",
      "Step 40: Loss = 31940506.0\n",
      "Step 50: Loss = 32861192.0\n",
      "epoch:23/100 |Loss: 33809656.0\n",
      "\n",
      "Epoch 25/100 ----------------\n",
      "Step 0: Loss = 32085830.0\n",
      "Step 10: Loss = 32198910.0\n",
      "Step 20: Loss = 30326266.0\n",
      "Step 30: Loss = 35614256.0\n",
      "Step 40: Loss = 32187696.0\n",
      "Step 50: Loss = 30572822.0\n",
      "epoch:24/100 |Loss: 35576124.0\n",
      "\n",
      "Epoch 26/100 ----------------\n",
      "Step 0: Loss = 32274782.0\n",
      "Step 10: Loss = 32495502.0\n",
      "Step 20: Loss = 34141524.0\n",
      "Step 30: Loss = 33739116.0\n",
      "Step 40: Loss = 31787334.0\n",
      "Step 50: Loss = 32537576.0\n",
      "epoch:25/100 |Loss: 34320716.0\n",
      "\n",
      "Epoch 27/100 ----------------\n",
      "Step 0: Loss = 34023024.0\n",
      "Step 10: Loss = 31930398.0\n",
      "Step 20: Loss = 32576686.0\n",
      "Step 30: Loss = 33349846.0\n",
      "Step 40: Loss = 32268032.0\n",
      "Step 50: Loss = 32200766.0\n",
      "epoch:26/100 |Loss: 31105678.0\n",
      "\n",
      "Epoch 28/100 ----------------\n",
      "Step 0: Loss = 31799798.0\n",
      "Step 10: Loss = 31277670.0\n",
      "Step 20: Loss = 31164402.0\n",
      "Step 30: Loss = 32591882.0\n",
      "Step 40: Loss = 33593248.0\n",
      "Step 50: Loss = 29489506.0\n",
      "epoch:27/100 |Loss: 32004600.0\n",
      "\n",
      "Epoch 29/100 ----------------\n",
      "Step 0: Loss = 31764930.0\n",
      "Step 10: Loss = 29544608.0\n",
      "Step 20: Loss = 30752496.0\n",
      "Step 30: Loss = 30897160.0\n",
      "Step 40: Loss = 31355194.0\n",
      "Step 50: Loss = 30792894.0\n",
      "epoch:28/100 |Loss: 30450176.0\n",
      "\n",
      "Epoch 30/100 ----------------\n",
      "Step 0: Loss = 32699454.0\n",
      "Step 10: Loss = 31623066.0\n",
      "Step 20: Loss = 32128634.0\n",
      "Step 30: Loss = 31204784.0\n",
      "Step 40: Loss = 30235008.0\n",
      "Step 50: Loss = 30764672.0\n",
      "epoch:29/100 |Loss: 32439802.0\n",
      "\n",
      "Epoch 31/100 ----------------\n",
      "Step 0: Loss = 32635062.0\n",
      "Step 10: Loss = 33427970.0\n",
      "Step 20: Loss = 31206694.0\n",
      "Step 30: Loss = 32437312.0\n",
      "Step 40: Loss = 29885672.0\n",
      "Step 50: Loss = 31266270.0\n",
      "epoch:30/100 |Loss: 30478480.0\n",
      "\n",
      "Epoch 32/100 ----------------\n",
      "Step 0: Loss = 33410952.0\n",
      "Step 10: Loss = 31377976.0\n",
      "Step 20: Loss = 30396342.0\n",
      "Step 30: Loss = 30380826.0\n",
      "Step 40: Loss = 29361710.0\n",
      "Step 50: Loss = 30539758.0\n",
      "epoch:31/100 |Loss: 30954536.0\n",
      "\n",
      "Epoch 33/100 ----------------\n",
      "Step 0: Loss = 30720638.0\n",
      "Step 10: Loss = 29579994.0\n",
      "Step 20: Loss = 31017178.0\n",
      "Step 30: Loss = 29972438.0\n",
      "Step 40: Loss = 31245922.0\n",
      "Step 50: Loss = 29781058.0\n",
      "epoch:32/100 |Loss: 31888564.0\n",
      "\n",
      "Epoch 34/100 ----------------\n",
      "Step 0: Loss = 29389070.0\n",
      "Step 10: Loss = 29028114.0\n",
      "Step 20: Loss = 31723650.0\n",
      "Step 30: Loss = 33086110.0\n",
      "Step 40: Loss = 30214782.0\n",
      "Step 50: Loss = 28756642.0\n",
      "epoch:33/100 |Loss: 31594190.0\n",
      "\n",
      "Epoch 35/100 ----------------\n",
      "Step 0: Loss = 31898528.0\n",
      "Step 10: Loss = 30118456.0\n",
      "Step 20: Loss = 34789820.0\n",
      "Step 30: Loss = 31219736.0\n",
      "Step 40: Loss = 28963834.0\n",
      "Step 50: Loss = 30916208.0\n",
      "epoch:34/100 |Loss: 29580656.0\n",
      "\n",
      "Epoch 36/100 ----------------\n",
      "Step 0: Loss = 32021928.0\n",
      "Step 10: Loss = 29821750.0\n",
      "Step 20: Loss = 30815194.0\n",
      "Step 30: Loss = 29517432.0\n",
      "Step 40: Loss = 29683954.0\n",
      "Step 50: Loss = 30223856.0\n",
      "epoch:35/100 |Loss: 29952188.0\n",
      "\n",
      "Epoch 37/100 ----------------\n",
      "Step 0: Loss = 29492722.0\n",
      "Step 10: Loss = 30281714.0\n",
      "Step 20: Loss = 28572602.0\n",
      "Step 30: Loss = 29340222.0\n",
      "Step 40: Loss = 30144402.0\n",
      "Step 50: Loss = 30116598.0\n",
      "epoch:36/100 |Loss: 30255742.0\n",
      "\n",
      "Epoch 38/100 ----------------\n",
      "Step 0: Loss = 30914576.0\n",
      "Step 10: Loss = 29768810.0\n",
      "Step 20: Loss = 30047566.0\n",
      "Step 30: Loss = 31490560.0\n",
      "Step 40: Loss = 28241154.0\n",
      "Step 50: Loss = 27951512.0\n",
      "epoch:37/100 |Loss: 29243456.0\n",
      "\n",
      "Epoch 39/100 ----------------\n",
      "Step 0: Loss = 30987374.0\n",
      "Step 10: Loss = 32431790.0\n",
      "Step 20: Loss = 30540258.0\n",
      "Step 30: Loss = 29678842.0\n",
      "Step 40: Loss = 28551056.0\n",
      "Step 50: Loss = 31375234.0\n",
      "epoch:38/100 |Loss: 28534144.0\n",
      "\n",
      "Epoch 40/100 ----------------\n",
      "Step 0: Loss = 29598638.0\n",
      "Step 10: Loss = 30423914.0\n",
      "Step 20: Loss = 29693818.0\n",
      "Step 30: Loss = 30026938.0\n",
      "Step 40: Loss = 30503574.0\n",
      "Step 50: Loss = 28853120.0\n",
      "epoch:39/100 |Loss: 28463206.0\n",
      "\n",
      "Epoch 41/100 ----------------\n",
      "Step 0: Loss = 30643178.0\n",
      "Step 10: Loss = 28128432.0\n",
      "Step 20: Loss = 29726824.0\n",
      "Step 30: Loss = 28089830.0\n",
      "Step 40: Loss = 29346488.0\n",
      "Step 50: Loss = 26662544.0\n",
      "epoch:40/100 |Loss: 29707074.0\n",
      "\n",
      "Epoch 42/100 ----------------\n",
      "Step 0: Loss = 29013320.0\n",
      "Step 10: Loss = 29025410.0\n",
      "Step 20: Loss = 28218542.0\n",
      "Step 30: Loss = 30331906.0\n",
      "Step 40: Loss = 31092350.0\n",
      "Step 50: Loss = 28536846.0\n",
      "epoch:41/100 |Loss: 28942568.0\n",
      "\n",
      "Epoch 43/100 ----------------\n",
      "Step 0: Loss = 29631464.0\n",
      "Step 10: Loss = 27774002.0\n",
      "Step 20: Loss = 28047746.0\n",
      "Step 30: Loss = 28641320.0\n",
      "Step 40: Loss = 30046158.0\n",
      "Step 50: Loss = 28641974.0\n",
      "epoch:42/100 |Loss: 26812884.0\n",
      "\n",
      "Epoch 44/100 ----------------\n",
      "Step 0: Loss = 27448776.0\n",
      "Step 10: Loss = 29283600.0\n",
      "Step 20: Loss = 26438446.0\n",
      "Step 30: Loss = 27440470.0\n",
      "Step 40: Loss = 28835784.0\n",
      "Step 50: Loss = 28015774.0\n",
      "epoch:43/100 |Loss: 29212644.0\n",
      "\n",
      "Epoch 45/100 ----------------\n",
      "Step 0: Loss = 30501486.0\n",
      "Step 10: Loss = 28370938.0\n",
      "Step 20: Loss = 29314238.0\n",
      "Step 30: Loss = 28538304.0\n",
      "Step 40: Loss = 28303024.0\n",
      "Step 50: Loss = 29193126.0\n",
      "epoch:44/100 |Loss: 28959028.0\n",
      "\n",
      "Epoch 46/100 ----------------\n",
      "Step 0: Loss = 28211854.0\n",
      "Step 10: Loss = 27604954.0\n",
      "Step 20: Loss = 28978638.0\n",
      "Step 30: Loss = 26594962.0\n",
      "Step 40: Loss = 26569114.0\n",
      "Step 50: Loss = 27880790.0\n",
      "epoch:45/100 |Loss: 27132978.0\n",
      "\n",
      "Epoch 47/100 ----------------\n",
      "Step 0: Loss = 26744790.0\n",
      "Step 10: Loss = 27405218.0\n",
      "Step 20: Loss = 28799936.0\n",
      "Step 30: Loss = 27393864.0\n",
      "Step 40: Loss = 27503930.0\n",
      "Step 50: Loss = 28014166.0\n",
      "epoch:46/100 |Loss: 27576860.0\n",
      "\n",
      "Epoch 48/100 ----------------\n",
      "Step 0: Loss = 28282592.0\n",
      "Step 10: Loss = 27551784.0\n",
      "Step 20: Loss = 26939502.0\n",
      "Step 30: Loss = 27270238.0\n",
      "Step 40: Loss = 26072248.0\n",
      "Step 50: Loss = 28103384.0\n",
      "epoch:47/100 |Loss: 27387800.0\n",
      "\n",
      "Epoch 49/100 ----------------\n",
      "Step 0: Loss = 27329258.0\n",
      "Step 10: Loss = 27576128.0\n",
      "Step 20: Loss = 27153618.0\n",
      "Step 30: Loss = 27366746.0\n",
      "Step 40: Loss = 27119496.0\n",
      "Step 50: Loss = 25825792.0\n",
      "epoch:48/100 |Loss: 25925720.0\n",
      "\n",
      "Epoch 50/100 ----------------\n",
      "Step 0: Loss = 26985938.0\n",
      "Step 10: Loss = 26762214.0\n",
      "Step 20: Loss = 27520770.0\n",
      "Step 30: Loss = 27061624.0\n",
      "Step 40: Loss = 27247928.0\n",
      "Step 50: Loss = 26920784.0\n",
      "epoch:49/100 |Loss: 31222400.0\n",
      "\n",
      "Epoch 51/100 ----------------\n",
      "Step 0: Loss = 25629418.0\n",
      "Step 10: Loss = 26517762.0\n",
      "Step 20: Loss = 26668288.0\n",
      "Step 30: Loss = 26440870.0\n",
      "Step 40: Loss = 26726672.0\n",
      "Step 50: Loss = 27574728.0\n",
      "epoch:50/100 |Loss: 27760998.0\n",
      "\n",
      "Epoch 52/100 ----------------\n",
      "Step 0: Loss = 25765206.0\n",
      "Step 10: Loss = 25302104.0\n",
      "Step 20: Loss = 27341912.0\n",
      "Step 30: Loss = 26725592.0\n",
      "Step 40: Loss = 26461888.0\n",
      "Step 50: Loss = 26391226.0\n",
      "epoch:51/100 |Loss: 25835030.0\n",
      "\n",
      "Epoch 53/100 ----------------\n",
      "Step 0: Loss = 24972296.0\n",
      "Step 10: Loss = 27432522.0\n",
      "Step 20: Loss = 25449190.0\n",
      "Step 30: Loss = 23020456.0\n",
      "Step 40: Loss = 26426726.0\n",
      "Step 50: Loss = 26747934.0\n",
      "epoch:52/100 |Loss: 26361958.0\n",
      "\n",
      "Epoch 54/100 ----------------\n",
      "Step 0: Loss = 25930024.0\n",
      "Step 10: Loss = 25086666.0\n",
      "Step 20: Loss = 24641534.0\n",
      "Step 30: Loss = 26983248.0\n",
      "Step 40: Loss = 24739674.0\n",
      "Step 50: Loss = 23775632.0\n",
      "epoch:53/100 |Loss: 25843258.0\n",
      "\n",
      "Epoch 55/100 ----------------\n",
      "Step 0: Loss = 26940190.0\n",
      "Step 10: Loss = 26106734.0\n",
      "Step 20: Loss = 25095538.0\n",
      "Step 30: Loss = 27575110.0\n",
      "Step 40: Loss = 26265986.0\n",
      "Step 50: Loss = 28196304.0\n",
      "epoch:54/100 |Loss: 25098660.0\n",
      "\n",
      "Epoch 56/100 ----------------\n",
      "Step 0: Loss = 26173846.0\n",
      "Step 10: Loss = 24310256.0\n",
      "Step 20: Loss = 25409224.0\n",
      "Step 30: Loss = 25154694.0\n",
      "Step 40: Loss = 24483872.0\n",
      "Step 50: Loss = 25295222.0\n",
      "epoch:55/100 |Loss: 26402368.0\n",
      "\n",
      "Epoch 57/100 ----------------\n",
      "Step 0: Loss = 23405232.0\n",
      "Step 10: Loss = 24981202.0\n",
      "Step 20: Loss = 26047122.0\n",
      "Step 30: Loss = 24018118.0\n",
      "Step 40: Loss = 24325830.0\n",
      "Step 50: Loss = 26339758.0\n",
      "epoch:56/100 |Loss: 26922340.0\n",
      "\n",
      "Epoch 58/100 ----------------\n",
      "Step 0: Loss = 26947912.0\n",
      "Step 10: Loss = 24674448.0\n",
      "Step 20: Loss = 23628280.0\n",
      "Step 30: Loss = 25253336.0\n",
      "Step 40: Loss = 25959418.0\n",
      "Step 50: Loss = 24983698.0\n",
      "epoch:57/100 |Loss: 24323888.0\n",
      "\n",
      "Epoch 59/100 ----------------\n",
      "Step 0: Loss = 26713576.0\n",
      "Step 10: Loss = 25528202.0\n",
      "Step 20: Loss = 22054306.0\n",
      "Step 30: Loss = 24322974.0\n",
      "Step 40: Loss = 23975936.0\n",
      "Step 50: Loss = 22381342.0\n",
      "epoch:58/100 |Loss: 24369604.0\n",
      "\n",
      "Epoch 60/100 ----------------\n",
      "Step 0: Loss = 23130362.0\n",
      "Step 10: Loss = 23662430.0\n",
      "Step 20: Loss = 24009864.0\n",
      "Step 30: Loss = 23912896.0\n",
      "Step 40: Loss = 23382510.0\n",
      "Step 50: Loss = 24920326.0\n",
      "epoch:59/100 |Loss: 23359082.0\n",
      "\n",
      "Epoch 61/100 ----------------\n",
      "Step 0: Loss = 24797798.0\n",
      "Step 10: Loss = 22180848.0\n",
      "Step 20: Loss = 24465336.0\n",
      "Step 30: Loss = 23903618.0\n",
      "Step 40: Loss = 24369554.0\n",
      "Step 50: Loss = 23319998.0\n",
      "epoch:60/100 |Loss: 23882846.0\n",
      "\n",
      "Epoch 62/100 ----------------\n",
      "Step 0: Loss = 24913186.0\n",
      "Step 10: Loss = 25127338.0\n",
      "Step 20: Loss = 24705922.0\n",
      "Step 30: Loss = 23664984.0\n",
      "Step 40: Loss = 22712184.0\n",
      "Step 50: Loss = 21691386.0\n",
      "epoch:61/100 |Loss: 23898250.0\n",
      "\n",
      "Epoch 63/100 ----------------\n",
      "Step 0: Loss = 23365248.0\n",
      "Step 10: Loss = 22288904.0\n",
      "Step 20: Loss = 21790398.0\n",
      "Step 30: Loss = 23889814.0\n",
      "Step 40: Loss = 23569478.0\n",
      "Step 50: Loss = 24776610.0\n",
      "epoch:62/100 |Loss: 23315540.0\n",
      "\n",
      "Epoch 64/100 ----------------\n",
      "Step 0: Loss = 26153536.0\n",
      "Step 10: Loss = 22716386.0\n",
      "Step 20: Loss = 21707582.0\n",
      "Step 30: Loss = 23547530.0\n",
      "Step 40: Loss = 22731610.0\n",
      "Step 50: Loss = 23607032.0\n",
      "epoch:63/100 |Loss: 21965498.0\n",
      "\n",
      "Epoch 65/100 ----------------\n",
      "Step 0: Loss = 23238094.0\n",
      "Step 10: Loss = 21630718.0\n",
      "Step 20: Loss = 21771570.0\n",
      "Step 30: Loss = 24327750.0\n",
      "Step 40: Loss = 22584864.0\n",
      "Step 50: Loss = 21398814.0\n",
      "epoch:64/100 |Loss: 21239500.0\n",
      "\n",
      "Epoch 66/100 ----------------\n",
      "Step 0: Loss = 21588326.0\n",
      "Step 10: Loss = 22354978.0\n",
      "Step 20: Loss = 22112330.0\n",
      "Step 30: Loss = 20348720.0\n",
      "Step 40: Loss = 22492518.0\n",
      "Step 50: Loss = 20727652.0\n",
      "epoch:65/100 |Loss: 23737722.0\n",
      "\n",
      "Epoch 67/100 ----------------\n",
      "Step 0: Loss = 22652334.0\n",
      "Step 10: Loss = 22000718.0\n",
      "Step 20: Loss = 24145122.0\n",
      "Step 30: Loss = 23112486.0\n",
      "Step 40: Loss = 21308996.0\n",
      "Step 50: Loss = 22313776.0\n",
      "epoch:66/100 |Loss: 22769484.0\n",
      "\n",
      "Epoch 68/100 ----------------\n",
      "Step 0: Loss = 20557236.0\n",
      "Step 10: Loss = 22739778.0\n",
      "Step 20: Loss = 21327486.0\n",
      "Step 30: Loss = 22151280.0\n",
      "Step 40: Loss = 21984992.0\n",
      "Step 50: Loss = 22424330.0\n",
      "epoch:67/100 |Loss: 20513056.0\n",
      "\n",
      "Epoch 69/100 ----------------\n",
      "Step 0: Loss = 21839702.0\n",
      "Step 10: Loss = 23797202.0\n",
      "Step 20: Loss = 21430042.0\n",
      "Step 30: Loss = 22287276.0\n",
      "Step 40: Loss = 22743454.0\n",
      "Step 50: Loss = 21467938.0\n",
      "epoch:68/100 |Loss: 21104138.0\n",
      "\n",
      "Epoch 70/100 ----------------\n",
      "Step 0: Loss = 21809840.0\n",
      "Step 10: Loss = 22133790.0\n",
      "Step 20: Loss = 20330508.0\n",
      "Step 30: Loss = 21546266.0\n",
      "Step 40: Loss = 23099398.0\n",
      "Step 50: Loss = 23526648.0\n",
      "epoch:69/100 |Loss: 19812224.0\n",
      "\n",
      "Epoch 71/100 ----------------\n",
      "Step 0: Loss = 21541498.0\n",
      "Step 10: Loss = 21689110.0\n",
      "Step 20: Loss = 20526344.0\n",
      "Step 30: Loss = 20666060.0\n",
      "Step 40: Loss = 20207662.0\n",
      "Step 50: Loss = 21518374.0\n",
      "epoch:70/100 |Loss: 21144820.0\n",
      "\n",
      "Epoch 72/100 ----------------\n",
      "Step 0: Loss = 19580064.0\n",
      "Step 10: Loss = 19362688.0\n",
      "Step 20: Loss = 22867896.0\n",
      "Step 30: Loss = 20320522.0\n",
      "Step 40: Loss = 21126038.0\n",
      "Step 50: Loss = 21594934.0\n",
      "epoch:71/100 |Loss: 23005996.0\n",
      "\n",
      "Epoch 73/100 ----------------\n",
      "Step 0: Loss = 20404886.0\n",
      "Step 10: Loss = 20722874.0\n",
      "Step 20: Loss = 19473254.0\n",
      "Step 30: Loss = 18886390.0\n",
      "Step 40: Loss = 20685906.0\n",
      "Step 50: Loss = 19052674.0\n",
      "epoch:72/100 |Loss: 22100820.0\n",
      "\n",
      "Epoch 74/100 ----------------\n",
      "Step 0: Loss = 19717930.0\n",
      "Step 10: Loss = 19456068.0\n",
      "Step 20: Loss = 18544534.0\n",
      "Step 30: Loss = 21746488.0\n",
      "Step 40: Loss = 19563382.0\n",
      "Step 50: Loss = 18525582.0\n",
      "epoch:73/100 |Loss: 19481156.0\n",
      "\n",
      "Epoch 75/100 ----------------\n",
      "Step 0: Loss = 21078394.0\n",
      "Step 10: Loss = 21577618.0\n",
      "Step 20: Loss = 20927998.0\n",
      "Step 30: Loss = 19104472.0\n",
      "Step 40: Loss = 21050872.0\n",
      "Step 50: Loss = 20446558.0\n",
      "epoch:74/100 |Loss: 20028434.0\n",
      "\n",
      "Epoch 76/100 ----------------\n",
      "Step 0: Loss = 19552424.0\n",
      "Step 10: Loss = 18667964.0\n",
      "Step 20: Loss = 19316032.0\n",
      "Step 30: Loss = 19521024.0\n",
      "Step 40: Loss = 19888806.0\n",
      "Step 50: Loss = 19754922.0\n",
      "epoch:75/100 |Loss: 20781466.0\n",
      "\n",
      "Epoch 77/100 ----------------\n",
      "Step 0: Loss = 20473706.0\n",
      "Step 10: Loss = 20019514.0\n",
      "Step 20: Loss = 19558238.0\n",
      "Step 30: Loss = 20073996.0\n",
      "Step 40: Loss = 20053518.0\n",
      "Step 50: Loss = 20040804.0\n",
      "epoch:76/100 |Loss: 20627654.0\n",
      "\n",
      "Epoch 78/100 ----------------\n",
      "Step 0: Loss = 19128322.0\n",
      "Step 10: Loss = 19046258.0\n",
      "Step 20: Loss = 19293842.0\n",
      "Step 30: Loss = 18761270.0\n",
      "Step 40: Loss = 18229454.0\n",
      "Step 50: Loss = 18318310.0\n",
      "epoch:77/100 |Loss: 18115666.0\n",
      "\n",
      "Epoch 79/100 ----------------\n",
      "Step 0: Loss = 18913376.0\n",
      "Step 10: Loss = 19350810.0\n",
      "Step 20: Loss = 17009190.0\n",
      "Step 30: Loss = 19838568.0\n",
      "Step 40: Loss = 20198034.0\n",
      "Step 50: Loss = 17775322.0\n",
      "epoch:78/100 |Loss: 19583924.0\n",
      "\n",
      "Epoch 80/100 ----------------\n",
      "Step 0: Loss = 19992750.0\n",
      "Step 10: Loss = 18593220.0\n",
      "Step 20: Loss = 18415206.0\n",
      "Step 30: Loss = 20377090.0\n",
      "Step 40: Loss = 19140262.0\n",
      "Step 50: Loss = 17821292.0\n",
      "epoch:79/100 |Loss: 18515600.0\n",
      "\n",
      "Epoch 81/100 ----------------\n",
      "Step 0: Loss = 20307876.0\n",
      "Step 10: Loss = 17510658.0\n",
      "Step 20: Loss = 17396690.0\n",
      "Step 30: Loss = 18597988.0\n",
      "Step 40: Loss = 17978040.0\n",
      "Step 50: Loss = 18967270.0\n",
      "epoch:80/100 |Loss: 16701746.0\n",
      "\n",
      "Epoch 82/100 ----------------\n",
      "Step 0: Loss = 18206818.0\n",
      "Step 10: Loss = 18539742.0\n",
      "Step 20: Loss = 16807226.0\n",
      "Step 30: Loss = 16892668.0\n",
      "Step 40: Loss = 17627956.0\n",
      "Step 50: Loss = 17339766.0\n",
      "epoch:81/100 |Loss: 17331992.0\n",
      "\n",
      "Epoch 83/100 ----------------\n",
      "Step 0: Loss = 17947334.0\n",
      "Step 10: Loss = 18079046.0\n",
      "Step 20: Loss = 18265358.0\n",
      "Step 30: Loss = 19425046.0\n",
      "Step 40: Loss = 16248193.0\n",
      "Step 50: Loss = 17281478.0\n",
      "epoch:82/100 |Loss: 15613341.0\n",
      "\n",
      "Epoch 84/100 ----------------\n",
      "Step 0: Loss = 16735843.0\n",
      "Step 10: Loss = 18510122.0\n",
      "Step 20: Loss = 15600367.0\n",
      "Step 30: Loss = 18671066.0\n",
      "Step 40: Loss = 17267170.0\n",
      "Step 50: Loss = 17607934.0\n",
      "epoch:83/100 |Loss: 17096096.0\n",
      "\n",
      "Epoch 85/100 ----------------\n",
      "Step 0: Loss = 15558076.0\n",
      "Step 10: Loss = 18037218.0\n",
      "Step 20: Loss = 17260956.0\n",
      "Step 30: Loss = 16645051.0\n",
      "Step 40: Loss = 18262006.0\n",
      "Step 50: Loss = 15847209.0\n",
      "epoch:84/100 |Loss: 16604306.0\n",
      "\n",
      "Epoch 86/100 ----------------\n",
      "Step 0: Loss = 17506174.0\n",
      "Step 10: Loss = 16997710.0\n",
      "Step 20: Loss = 15923793.0\n",
      "Step 30: Loss = 15361108.0\n",
      "Step 40: Loss = 15756245.0\n",
      "Step 50: Loss = 15443633.0\n",
      "epoch:85/100 |Loss: 17163132.0\n",
      "\n",
      "Epoch 87/100 ----------------\n",
      "Step 0: Loss = 16939006.0\n",
      "Step 10: Loss = 15420089.0\n",
      "Step 20: Loss = 14334721.0\n",
      "Step 30: Loss = 16163648.0\n",
      "Step 40: Loss = 18070984.0\n",
      "Step 50: Loss = 16309448.0\n",
      "epoch:86/100 |Loss: 17527740.0\n",
      "\n",
      "Epoch 88/100 ----------------\n",
      "Step 0: Loss = 16120519.0\n",
      "Step 10: Loss = 16739316.0\n",
      "Step 20: Loss = 16676992.0\n",
      "Step 30: Loss = 17977218.0\n",
      "Step 40: Loss = 15415207.0\n",
      "Step 50: Loss = 16057139.0\n",
      "epoch:87/100 |Loss: 16340097.0\n",
      "\n",
      "Epoch 89/100 ----------------\n",
      "Step 0: Loss = 17167324.0\n",
      "Step 10: Loss = 16876634.0\n",
      "Step 20: Loss = 16913904.0\n",
      "Step 30: Loss = 15141711.0\n",
      "Step 40: Loss = 16403305.0\n",
      "Step 50: Loss = 14155953.0\n",
      "epoch:88/100 |Loss: 15325440.0\n",
      "\n",
      "Epoch 90/100 ----------------\n",
      "Step 0: Loss = 15860677.0\n",
      "Step 10: Loss = 14668911.0\n",
      "Step 20: Loss = 15535115.0\n",
      "Step 30: Loss = 15603412.0\n",
      "Step 40: Loss = 16106919.0\n",
      "Step 50: Loss = 15316105.0\n",
      "epoch:89/100 |Loss: 18018910.0\n",
      "\n",
      "Epoch 91/100 ----------------\n",
      "Step 0: Loss = 16831430.0\n",
      "Step 10: Loss = 15805096.0\n",
      "Step 20: Loss = 15362413.0\n",
      "Step 30: Loss = 14985920.0\n",
      "Step 40: Loss = 16454977.0\n",
      "Step 50: Loss = 14665141.0\n",
      "epoch:90/100 |Loss: 14510413.0\n",
      "\n",
      "Epoch 92/100 ----------------\n",
      "Step 0: Loss = 15598753.0\n",
      "Step 10: Loss = 15138944.0\n",
      "Step 20: Loss = 15585779.0\n",
      "Step 30: Loss = 13396325.0\n",
      "Step 40: Loss = 16196541.0\n",
      "Step 50: Loss = 14433540.0\n",
      "epoch:91/100 |Loss: 15263659.0\n",
      "\n",
      "Epoch 93/100 ----------------\n",
      "Step 0: Loss = 14797048.0\n",
      "Step 10: Loss = 14943797.0\n",
      "Step 20: Loss = 14082532.0\n",
      "Step 30: Loss = 14652869.0\n",
      "Step 40: Loss = 15624364.0\n",
      "Step 50: Loss = 15118601.0\n",
      "epoch:92/100 |Loss: 13016536.0\n",
      "\n",
      "Epoch 94/100 ----------------\n",
      "Step 0: Loss = 15626031.0\n",
      "Step 10: Loss = 14870068.0\n",
      "Step 20: Loss = 14458961.0\n",
      "Step 30: Loss = 15398609.0\n",
      "Step 40: Loss = 15025935.0\n",
      "Step 50: Loss = 14673441.0\n",
      "epoch:93/100 |Loss: 14557614.0\n",
      "\n",
      "Epoch 95/100 ----------------\n",
      "Step 0: Loss = 13399912.0\n",
      "Step 10: Loss = 15093884.0\n",
      "Step 20: Loss = 14210888.0\n",
      "Step 30: Loss = 14644147.0\n",
      "Step 40: Loss = 14932608.0\n",
      "Step 50: Loss = 13885801.0\n",
      "epoch:94/100 |Loss: 14527609.0\n",
      "\n",
      "Epoch 96/100 ----------------\n",
      "Step 0: Loss = 14484563.0\n",
      "Step 10: Loss = 13532093.0\n",
      "Step 20: Loss = 14626835.0\n",
      "Step 30: Loss = 15225987.0\n",
      "Step 40: Loss = 13192021.0\n",
      "Step 50: Loss = 13901519.0\n",
      "epoch:95/100 |Loss: 12545208.0\n",
      "\n",
      "Epoch 97/100 ----------------\n",
      "Step 0: Loss = 14877479.0\n",
      "Step 10: Loss = 14215875.0\n",
      "Step 20: Loss = 12512536.0\n",
      "Step 30: Loss = 14374585.0\n",
      "Step 40: Loss = 14451352.0\n",
      "Step 50: Loss = 15785131.0\n",
      "epoch:96/100 |Loss: 15066389.0\n",
      "\n",
      "Epoch 98/100 ----------------\n",
      "Step 0: Loss = 14562561.0\n",
      "Step 10: Loss = 14121503.0\n",
      "Step 20: Loss = 11957796.0\n",
      "Step 30: Loss = 14388392.0\n",
      "Step 40: Loss = 15152599.0\n",
      "Step 50: Loss = 13667693.0\n",
      "epoch:97/100 |Loss: 14056192.0\n",
      "\n",
      "Epoch 99/100 ----------------\n",
      "Step 0: Loss = 13465627.0\n",
      "Step 10: Loss = 12996629.0\n",
      "Step 20: Loss = 11985361.0\n",
      "Step 30: Loss = 13222688.0\n",
      "Step 40: Loss = 13636153.0\n",
      "Step 50: Loss = 12473740.0\n",
      "epoch:98/100 |Loss: 14175744.0\n",
      "\n",
      "Epoch 100/100 ----------------\n",
      "Step 0: Loss = 12667751.0\n",
      "Step 10: Loss = 13332903.0\n",
      "Step 20: Loss = 13640803.0\n",
      "Step 30: Loss = 13113496.0\n",
      "Step 40: Loss = 12052744.0\n",
      "Step 50: Loss = 13652335.0\n",
      "epoch:99/100 |Loss: 13964240.0\n",
      "max AD score 15385.472\n",
      "thres: 6554.3003\n",
      " Total true outliers in this test set: 69661\n",
      " Number of those with RMSE > threshold: 41507\n",
      " Max RMSE among outliers: 15795.66\n",
      " RMSE of first 5 outliers: [2361.3123  977.2772  273.2362  275.9141  276.6608]\n",
      "Data testing:\n",
      "Precision: 0.936, Recall: 0.596, F1-score: 0.728\n",
      "Elapsed: 2.1178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_86955/1450427169.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df2.drop('label', axis=1)\n",
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp, fn, fp, tn = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs= plt.subplots()\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='grey_r', ax=axs)\n",
    "axs.set_title(\"Confusion Matrix Treatment\")\n",
    "# sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='grey_r', ax=axs[1])\n",
    "# axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps_c_pca.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32999c",
   "metadata": {},
   "source": [
    "### c. Continuous Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8077993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data testing 1:\n",
      "Training on 5000 samples\n",
      "max AD score 26896.848\n",
      "thres: 11033.173\n",
      "Testing on 13934 samples\n",
      "Precision: 0.924, Recall: 0.602, F1-score: 0.729\n",
      "Elapsed: 0.1612\n",
      "First iteration - no previous model to compare\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 2:\n",
      "Training on 5000 samples\n",
      "max AD score 18309.387\n",
      "thres: 10951.912\n",
      "Testing on 13932 samples\n",
      "Precision: 0.914, Recall: 0.602, F1-score: 0.726\n",
      "Elapsed: 0.1606\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13934, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13933\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9341\n",
      "-1    4591\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9393\n",
      "-1    4541\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1526\n",
      "Agreement - Both normal: 6327\n",
      "Disagreement - Current anomaly, Previous normal: 3065\n",
      "Disagreement - Current normal, Previous anomaly: 3014\n",
      "Total disagreements: 6079\n",
      "Agreement rate: 0.564\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1526    3065\n",
      "        Normal    3014    6327\n",
      "McNemar Statistic: 0.4113\n",
      "McNemar p-value: 0.521334798370962\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 3:\n",
      "Training on 5000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_19756/2069904131.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  both_anomaly = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_19756/2069904131.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  curr_anom_prev_norm = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_19756/2069904131.py:57: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  curr_norm_prev_anom = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_19756/2069904131.py:62: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  both_normal = len(current_results[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max AD score 22493.455\n",
      "thres: 10733.039\n",
      "Testing on 13932 samples\n",
      "Precision: 0.916, Recall: 0.636, F1-score: 0.751\n",
      "Elapsed: 0.1619\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9093\n",
      "-1    4839\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9341\n",
      "-1    4591\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1575\n",
      "Agreement - Both normal: 6077\n",
      "Disagreement - Current anomaly, Previous normal: 3264\n",
      "Disagreement - Current normal, Previous anomaly: 3016\n",
      "Total disagreements: 6280\n",
      "Agreement rate: 0.549\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1575    3264\n",
      "        Normal    3016    6077\n",
      "McNemar Statistic: 9.7148\n",
      "McNemar p-value: 0.0018278908405980911\n",
      "** Significant difference between models (p < 0.01)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 4:\n",
      "Training on 5000 samples\n",
      "max AD score 13794.17\n",
      "thres: 10935.2\n",
      "Testing on 13932 samples\n",
      "Precision: 0.924, Recall: 0.607, F1-score: 0.732\n",
      "Elapsed: 0.1618\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9357\n",
      "-1    4575\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9093\n",
      "-1    4839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1577\n",
      "Agreement - Both normal: 6095\n",
      "Disagreement - Current anomaly, Previous normal: 2998\n",
      "Disagreement - Current normal, Previous anomaly: 3262\n",
      "Total disagreements: 6260\n",
      "Agreement rate: 0.551\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1577    2998\n",
      "        Normal    3262    6095\n",
      "McNemar Statistic: 11.0494\n",
      "McNemar p-value: 0.0008871776975637782\n",
      "*** Highly significant difference between models (p < 0.001)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 5:\n",
      "Training on 5000 samples\n",
      "max AD score 24608.047\n",
      "thres: 11062.419\n",
      "Testing on 13932 samples\n",
      "Precision: 0.919, Recall: 0.587, F1-score: 0.717\n",
      "Elapsed: 0.1599\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9485\n",
      "-1    4447\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9357\n",
      "-1    4575\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1460\n",
      "Agreement - Both normal: 6370\n",
      "Disagreement - Current anomaly, Previous normal: 2987\n",
      "Disagreement - Current normal, Previous anomaly: 3115\n",
      "Total disagreements: 6102\n",
      "Agreement rate: 0.562\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1460    2987\n",
      "        Normal    3115    6370\n",
      "McNemar Statistic: 2.6432\n",
      "McNemar p-value: 0.1039918099911769\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 6:\n",
      "Training on 5000 samples\n",
      "max AD score 24246.846\n",
      "thres: 11030.895\n",
      "Testing on 13932 samples\n",
      "Precision: 0.915, Recall: 0.591, F1-score: 0.718\n",
      "Elapsed: 0.1634\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9434\n",
      "-1    4498\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9485\n",
      "-1    4447\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1433\n",
      "Agreement - Both normal: 6420\n",
      "Disagreement - Current anomaly, Previous normal: 3065\n",
      "Disagreement - Current normal, Previous anomaly: 3014\n",
      "Total disagreements: 6079\n",
      "Agreement rate: 0.564\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1433    3065\n",
      "        Normal    3014    6420\n",
      "McNemar Statistic: 0.4113\n",
      "McNemar p-value: 0.521334798370962\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 7:\n",
      "Training on 5000 samples\n",
      "max AD score 23720.656\n",
      "thres: 11052.54\n",
      "Testing on 13932 samples\n",
      "Precision: 0.925, Recall: 0.596, F1-score: 0.725\n",
      "Elapsed: 0.1615\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9445\n",
      "-1    4487\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9434\n",
      "-1    4498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1461\n",
      "Agreement - Both normal: 6408\n",
      "Disagreement - Current anomaly, Previous normal: 3026\n",
      "Disagreement - Current normal, Previous anomaly: 3037\n",
      "Total disagreements: 6063\n",
      "Agreement rate: 0.565\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1461    3026\n",
      "        Normal    3037    6408\n",
      "McNemar Statistic: 0.0165\n",
      "McNemar p-value: 0.8978110936805492\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 8:\n",
      "Training on 5000 samples\n",
      "max AD score 13801.519\n",
      "thres: 11070.698\n",
      "Testing on 13932 samples\n",
      "Precision: 0.918, Recall: 0.585, F1-score: 0.714\n",
      "Elapsed: 0.1604\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9494\n",
      "-1    4438\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9445\n",
      "-1    4487\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1423\n",
      "Agreement - Both normal: 6430\n",
      "Disagreement - Current anomaly, Previous normal: 3015\n",
      "Disagreement - Current normal, Previous anomaly: 3064\n",
      "Total disagreements: 6079\n",
      "Agreement rate: 0.564\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1423    3015\n",
      "        Normal    3064    6430\n",
      "McNemar Statistic: 0.3790\n",
      "McNemar p-value: 0.5381337022049915\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 9:\n",
      "Training on 5000 samples\n",
      "max AD score 26919.148\n",
      "thres: 10863.589\n",
      "Testing on 13932 samples\n",
      "Precision: 0.924, Recall: 0.623, F1-score: 0.744\n",
      "Elapsed: 0.1610\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9239\n",
      "-1    4693\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9494\n",
      "-1    4438\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1489\n",
      "Agreement - Both normal: 6290\n",
      "Disagreement - Current anomaly, Previous normal: 3204\n",
      "Disagreement - Current normal, Previous anomaly: 2949\n",
      "Total disagreements: 6153\n",
      "Agreement rate: 0.558\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1489    3204\n",
      "        Normal    2949    6290\n",
      "McNemar Statistic: 10.4853\n",
      "McNemar p-value: 0.001203286168478354\n",
      "** Significant difference between models (p < 0.01)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 10:\n",
      "Training on 5000 samples\n",
      "max AD score 24592.31\n",
      "thres: 11133.003\n",
      "Testing on 13932 samples\n",
      "Precision: 0.928, Recall: 0.559, F1-score: 0.698\n",
      "Elapsed: 0.1593\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (13932, 14)\n",
      "Previous results shape: (13932, 14)\n",
      "Current results index range: 0 to 13931\n",
      "Previous results index range: 0 to 13931\n",
      "Index overlap: 13932\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9738\n",
      "-1    4194\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9239\n",
      "-1    4693\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1380\n",
      "Agreement - Both normal: 6425\n",
      "Disagreement - Current anomaly, Previous normal: 2814\n",
      "Disagreement - Current normal, Previous anomaly: 3313\n",
      "Total disagreements: 6127\n",
      "Agreement rate: 0.560\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1380    2814\n",
      "        Normal    3313    6425\n",
      "McNemar Statistic: 40.4772\n",
      "McNemar p-value: 1.9892056768300163e-10\n",
      "*** Highly significant difference between models (p < 0.001)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Model retraining and comparison completed!\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "import joblib\n",
    "\n",
    "previous_results = None\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Data testing {i+1}:')\n",
    "    \n",
    "    training_data = train_chunks[i][train_chunks[i]['label'] == 1].copy()\n",
    "    print(f\"Training on {len(training_data)} samples\")\n",
    "    \n",
    "    X_train = training_data.drop('label', axis=1)\n",
    "    X_train = X_train.astype(float)\n",
    "    X_train = X_train.to_numpy()\n",
    "\n",
    "    feature_size = X_train.shape[-1]\n",
    "    AE(feature_size)\n",
    "    model, thres = train(X_train, feature_size)\n",
    "    \n",
    "    test_data = test_chunks[i].copy()\n",
    "    print(f\"Testing on {len(test_data)} samples\")\n",
    "    \n",
    "    start = time.time()\n",
    "    current_results, recall_2, precision_2, f1_2 = test_model(model, test_data, thres)\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "    print(f\"Elapsed: {elapsed:.4f}\")\n",
    "    \n",
    "    if previous_results is not None:\n",
    "        print(f\"\\n=== DEBUG INFO ===\")\n",
    "        print(f\"Current results shape: {current_results.shape}\")\n",
    "        print(f\"Previous results shape: {previous_results.shape}\")\n",
    "        print(f\"Current results index range: {current_results.index.min()} to {current_results.index.max()}\")\n",
    "        print(f\"Previous results index range: {previous_results.index.min()} to {previous_results.index.max()}\")\n",
    "        print(f\"Index overlap: {len(set(current_results.index) & set(previous_results.index))}\")\n",
    "        print(f\"Current 'outlier' value counts:\\n{current_results['outlier'].value_counts()}\")\n",
    "        print(f\"Previous 'outlier' value counts:\\n{previous_results['outlier'].value_counts()}\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- McNemar Test Analysis ---\")\n",
    "        \n",
    "        if 'outlier' in current_results.columns and 'outlier' in previous_results.columns:\n",
    "            both_anomaly = len(current_results[\n",
    "                (current_results['outlier'] == -1) & \n",
    "                (previous_results['outlier'] == -1)\n",
    "            ])\n",
    "            \n",
    "            curr_anom_prev_norm = len(current_results[\n",
    "                (current_results['outlier'] == -1) & \n",
    "                (previous_results['outlier'] == 1)\n",
    "            ])\n",
    "            \n",
    "            curr_norm_prev_anom = len(current_results[\n",
    "                (current_results['outlier'] == 1) & \n",
    "                (previous_results['outlier'] == -1)\n",
    "            ])\n",
    "            \n",
    "            both_normal = len(current_results[\n",
    "                (current_results['outlier'] == 1) & \n",
    "                (previous_results['outlier'] == 1)\n",
    "            ])\n",
    "            \n",
    "            print(f\"Agreement - Both anomaly: {both_anomaly}\")\n",
    "            print(f\"Agreement - Both normal: {both_normal}\")\n",
    "            print(f\"Disagreement - Current anomaly, Previous normal: {curr_anom_prev_norm}\")\n",
    "            print(f\"Disagreement - Current normal, Previous anomaly: {curr_norm_prev_anom}\")\n",
    "            \n",
    "            total_disagreements = curr_anom_prev_norm + curr_norm_prev_anom\n",
    "            total_samples = both_anomaly + both_normal + total_disagreements\n",
    "            agreement_rate = (both_anomaly + both_normal) / total_samples\n",
    "            \n",
    "            print(f\"Total disagreements: {total_disagreements}\")\n",
    "            print(f\"Agreement rate: {agreement_rate:.3f}\")\n",
    "            \n",
    "            if total_disagreements > 0:\n",
    "                mcnemar_table = np.array([\n",
    "                    [both_anomaly, curr_anom_prev_norm], \n",
    "                    [curr_norm_prev_anom, both_normal]\n",
    "                ])\n",
    "                \n",
    "                print(f\"McNemar Table:\")\n",
    "                print(f\"                Previous Model\")\n",
    "                print(f\"               Anomaly  Normal\")\n",
    "                print(f\"Current Anomaly   {both_anomaly:4d}    {curr_anom_prev_norm:4d}\")\n",
    "                print(f\"        Normal    {curr_norm_prev_anom:4d}    {both_normal:4d}\")\n",
    "                \n",
    "                try:\n",
    "                    result = mcnemar(mcnemar_table, exact=False, correction=True)\n",
    "                    print(f\"McNemar Statistic: {result.statistic:.4f}\")\n",
    "                    print(f\"McNemar p-value: {result.pvalue}\")\n",
    "                    \n",
    "                    if result.pvalue < 0.001:\n",
    "                        print(\"*** Highly significant difference between models (p < 0.001)\")\n",
    "                    elif result.pvalue < 0.01:\n",
    "                        print(\"** Significant difference between models (p < 0.01)\")\n",
    "                    elif result.pvalue < 0.05:\n",
    "                        print(\"* Marginally significant difference between models (p < 0.05)\")\n",
    "                    else:\n",
    "                        print(\"No significant difference between models (p >= 0.05)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in McNemar test: {e}\")\n",
    "            else:\n",
    "                print(\"No disagreements between models - McNemar test not applicable\")\n",
    "        else:\n",
    "            print(\"Missing 'outlier' column in results - cannot perform McNemar test\")\n",
    "    elif previous_results is not None:\n",
    "        print(f\"Warning: Result sizes don't match - Current: {len(current_results)}, Previous: {len(previous_results)}\")\n",
    "    else:\n",
    "        print(\"First iteration - no previous model to compare\")\n",
    "    \n",
    "    previous_results = current_results.copy()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"Model retraining and comparison completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibidetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
