{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd16c8c",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575d5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1]\n",
      "Value counts:\n",
      " label\n",
      "0    53802\n",
      "1    17215\n",
      "Name: count, dtype: int64\n",
      "Unique labels: [0 1]\n",
      "Value counts:\n",
      " label\n",
      "0    602450\n",
      "1    162061\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../NLME.csv', low_memory=False)\n",
    "unique_labels = df['label'].unique()\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"Value counts:\\n\", label_counts)\n",
    "\n",
    "df2 = pd.read_csv('../parsed_events (20).csv', low_memory=False)\n",
    "\n",
    "unique_labels = df2['label'].unique()\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "label_counts = df2['label'].value_counts()\n",
    "print(\"Value counts:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af19aa8",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35296af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={\n",
    "    'target_file_name': 'TargetFilename',\n",
    "    'event_id': 'EventID',\n",
    "    'target_process_guid': 'TargetProcessGuid',\n",
    "    'event_type': 'EventType',\n",
    "    'target_image': 'TargetImage',\n",
    "    'previous_creation_utc_time': 'PreviousCreationUtcTime',\n",
    "    'destination_host_name': 'DestinationHostname',\n",
    "    'company': 'Company',\n",
    "    'description': 'Description',\n",
    "    'product': 'Product',\n",
    "    'integrity_level': 'IntegrityLevel',\n",
    "    'creation_utc_time': 'CreationUtcTime',\n",
    "    'start_function': 'StartFunction',\n",
    "    'parent_process_id': 'ParentProcessGuid',\n",
    "    'user': 'User',\n",
    "    'logon_id': 'LogonId',\n",
    "    'parent_process_id': 'ParentProcessId',\n",
    "    'terminal_session_id': 'TerminalSessionId',\n",
    "    'rule_name': 'RuleName',\n",
    "    'target_object': 'TargetObject',\n",
    "    'parent_process_guid': 'ParentProcessGuid'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bc76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = [\n",
    "    'TargetFilename', \n",
    "    'EventID', \n",
    "    'TargetProcessGuid', \n",
    "    'EventType',\n",
    "    'TargetImage', \n",
    "    'PreviousCreationUtcTime', \n",
    "    'DestinationHostname', \n",
    "    'Company', \n",
    "    'Description',\n",
    "    'Product', \n",
    "    'IntegrityLevel', \n",
    "    'CreationUtcTime', \n",
    "    # 'StartFunction', \n",
    "    'ParentProcessGuid', \n",
    "    'User',\n",
    "    'LogonId', \n",
    "    'ParentProcessId', \n",
    "    'TerminalSessionId', \n",
    "    'RuleName', \n",
    "    'TargetObject', \n",
    "    'label'\n",
    "]\n",
    "\n",
    "selected_df = df[important_columns]\n",
    "selected_df2 = df2[important_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f3acd",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b6c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_and_encode(df):\n",
    "    df = df.replace('-', np.nan)\n",
    "    df = df.dropna(axis=1, how='all').drop_duplicates()\n",
    "    le = LabelEncoder()\n",
    "    filtered = []\n",
    "    for col in df.columns:\n",
    "        if col == 'label':\n",
    "            continue\n",
    "        if df[col].nunique() <= 25 and col != 'label':\n",
    "            filtered.append(col)\n",
    "            mask = df[col].isnull()\n",
    "            df.loc[mask, col] = -1\n",
    "            df.loc[~mask, col] = le.fit_transform(df[col][~mask])\n",
    "        else:\n",
    "            if df[col].dtype == \"object\":\n",
    "                df[col] = df[col].str.len()\n",
    "            df[col] = df[col].fillna(-1)\n",
    "    return df, filtered\n",
    "\n",
    "def preprocess_data_3(df, df2):\n",
    "    df['label'] = df['label'].replace({1: -1, 0: 1})\n",
    "    df2['label'] = df2['label'].replace({1: -1, 0: 1})\n",
    "    df, filteredColumn = clean_and_encode(df)\n",
    "    df2, _ = clean_and_encode(df2)\n",
    "    \n",
    "    benign_df = df[df['label'] == 1]\n",
    "    malware_df = df[df['label'] == -1]\n",
    "    train_df = benign_df.iloc[malware_df.shape[0]:]\n",
    "    test_df = pd.concat([benign_df.iloc[:malware_df.shape[0]], malware_df])\n",
    "\n",
    "    benign_df2 = df2[df2['label'] == 1]\n",
    "    malware_df2 = df2[df2['label'] == -1]\n",
    "    \n",
    "    print(filteredColumn)\n",
    "\n",
    "    train_means = benign_df[filteredColumn].mean()\n",
    "    test_means = benign_df2[filteredColumn].mean()\n",
    "\n",
    "    abs_diff = (train_means - test_means).abs()\n",
    "    df_diff = pd.DataFrame({\n",
    "        \"Train means\": train_means,\n",
    "        \"Test means\": test_means,\n",
    "        \"Diff\": abs_diff\n",
    "    }).sort_values(by=\"Diff\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x=range(len(filteredColumn)), y=train_means.values, label=\"Train Benign\")\n",
    "    sns.lineplot(x=range(len(filteredColumn)), y=test_means.values, label=\"Test Benign\")\n",
    "    plt.xticks(range(len(filteredColumn)), filteredColumn, rotation=90)\n",
    "    plt.title(\"Feature Distribution Shift (Benign)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    benign_shuffled = benign_df2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    malware_shuffled = malware_df2.sample(frac=1, random_state=123).reset_index(drop=True)  # Different seed\n",
    "    \n",
    "    min_size = min(len(benign_shuffled), len(malware_shuffled))\n",
    "    benign_for_test = benign_shuffled.iloc[:min_size]\n",
    "    malware_for_test = malware_shuffled.iloc[:min_size]\n",
    "    \n",
    "    benign_test_chunks = np.array_split(benign_for_test, 10)\n",
    "    malware_test_chunks = np.array_split(malware_for_test, 10)\n",
    "    \n",
    "    test_df2 = pd.concat([benign_for_test, malware_for_test])\n",
    "    test_chunks = []\n",
    "    for i in range(10):\n",
    "        combined_chunk = pd.concat([benign_test_chunks[i], malware_test_chunks[i]], ignore_index=True)\n",
    "        combined_chunk = combined_chunk.sample(frac=1, random_state=i*10).reset_index(drop=True)\n",
    "        test_chunks.append(combined_chunk)\n",
    "        print(f\"Test chunk {i+1}: {len(combined_chunk)} samples, \"\n",
    "              f\"benign: {len(combined_chunk[combined_chunk['label']==1])}, \"\n",
    "              f\"malware: {len(combined_chunk[combined_chunk['label']==-1])}\")\n",
    "    \n",
    "    remaining_benign = benign_shuffled.iloc[min_size:] if len(benign_shuffled) > min_size else benign_shuffled\n",
    "    \n",
    "    train_chunks = []\n",
    "    for i in range(10):\n",
    "        if len(remaining_benign) > 1000:  # If we have enough data\n",
    "            train_sample = remaining_benign.sample(n=min(len(remaining_benign), 5000), \n",
    "                                                 random_state=i*100, \n",
    "                                                 replace=False).reset_index(drop=True)\n",
    "        else:\n",
    "            train_sample = remaining_benign.sample(n=5000, \n",
    "                                                 random_state=i*100, \n",
    "                                                 replace=True).reset_index(drop=True)\n",
    "        \n",
    "        train_chunks.append(train_sample)\n",
    "        print(f\"Train chunk {i+1}: {len(train_sample)} benign samples\")\n",
    "    \n",
    "    print(\"\\nVerifying chunk diversity:\")\n",
    "    for i in range(min(5, len(train_chunks)-1)):\n",
    "        similarity = len(pd.merge(train_chunks[i], train_chunks[i+1], how='inner')) / len(train_chunks[i])\n",
    "        print(f\"Train chunks {i+1} and {i+2} similarity: {similarity:.2%}\")\n",
    "    \n",
    "    for i in range(min(5, len(test_chunks)-1)):\n",
    "        similarity = len(pd.merge(test_chunks[i], test_chunks[i+1], how='inner')) / len(test_chunks[i])\n",
    "        print(f\"Test chunks {i+1} and {i+2} similarity: {similarity:.2%}\")\n",
    "\n",
    "    return df, train_df, test_df, df2, train_chunks, test_chunks, remaining_benign, pd.concat([benign_df2.iloc[:min_size], malware_df2.iloc[:min_size]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8b68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_15692/3903420739.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['label'].replace({1: -1, 0: 1})\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_15692/3903420739.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['label'] = df2['label'].replace({1: -1, 0: 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EventID', 'EventType', 'TargetImage', 'DestinationHostname', 'IntegrityLevel', 'User', 'TerminalSessionId', 'RuleName']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/alibidetect/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chunk 1: 14044 samples, benign: 7022, malware: 7022\n",
      "Test chunk 2: 14044 samples, benign: 7022, malware: 7022\n",
      "Test chunk 3: 14044 samples, benign: 7022, malware: 7022\n",
      "Test chunk 4: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 5: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 6: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 7: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 8: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 9: 14042 samples, benign: 7021, malware: 7021\n",
      "Test chunk 10: 14042 samples, benign: 7021, malware: 7021\n",
      "Train chunk 1: 5000 benign samples\n",
      "Train chunk 2: 5000 benign samples\n",
      "Train chunk 3: 5000 benign samples\n",
      "Train chunk 4: 5000 benign samples\n",
      "Train chunk 5: 5000 benign samples\n",
      "Train chunk 6: 5000 benign samples\n",
      "Train chunk 7: 5000 benign samples\n",
      "Train chunk 8: 5000 benign samples\n",
      "Train chunk 9: 5000 benign samples\n",
      "Train chunk 10: 5000 benign samples\n",
      "\n",
      "Verifying chunk diversity:\n",
      "Train chunks 1 and 2 similarity: 439.66%\n",
      "Train chunks 2 and 3 similarity: 393.86%\n",
      "Train chunks 3 and 4 similarity: 398.32%\n",
      "Train chunks 4 and 5 similarity: 405.14%\n",
      "Train chunks 5 and 6 similarity: 403.54%\n",
      "Test chunks 1 and 2 similarity: 17343.05%\n",
      "Test chunks 2 and 3 similarity: 18223.02%\n",
      "Test chunks 3 and 4 similarity: 18925.04%\n",
      "Test chunks 4 and 5 similarity: 19430.56%\n",
      "Test chunks 5 and 6 similarity: 18878.64%\n"
     ]
    }
   ],
   "source": [
    "ndf, train_df, test_df, ndf2, train_chunks, test_chunks, train_df2, test_df2 = preprocess_data_3(selected_df, selected_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0ef458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## import packages\n",
    "import sys\n",
    "sys.path.append('../admodels/')\n",
    "sys.path.append('../moudles/')\n",
    "sys.path.append('../baselines/')\n",
    "sys.path.append('../')\n",
    "\n",
    "from admodels.AE import AE\n",
    "from admodels.AE import train\n",
    "from admodels.AE import test\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def count_stats(df, outlier=1, label=1):\n",
    "    tmp = df.apply(lambda x : True if x['outlier'] == outlier and x['label'] == label else False, axis = 1)\n",
    "    return len(tmp[tmp == True].index)\n",
    "\n",
    "def test_model(clf, df, thres):\n",
    "    X_test = df.drop('label', axis=1)\n",
    "    y_true = df['label'].to_numpy()\n",
    "    outliers_predicted, rmse_vec = test(clf, thres, X_test) \n",
    "    new_df = df.copy()\n",
    "    new_df['outlier'] = outliers_predicted\n",
    "    # df.to_csv('y_pred_ae.csv', index=False)\n",
    "    tp = count_stats(new_df, outlier=-1, label=-1)\n",
    "    fn = count_stats(new_df, outlier=1, label=-1)\n",
    "    fp = count_stats(new_df, outlier=-1, label=1)\n",
    "    tn = count_stats(new_df, outlier=1, label=1)\n",
    "    # print({f'tp: {tp}'})\n",
    "    # print({f'fp: {fp}'})\n",
    "    # print({f'fn: {fn}'})\n",
    "    # print({f'tn: {tn}'})\n",
    "\n",
    "    rmse_outliers = rmse_vec[y_true == -1]\n",
    "    # print(f\"➡️ Total true outliers in this test set: {len(rmse_outliers)}\")\n",
    "    # print(f\"➡️ Number of those with RMSE > threshold: {np.sum(rmse_outliers > thres)}\")\n",
    "    print(f\"Max RMSE among outliers: {np.max(rmse_outliers):.2f}\")\n",
    "    # print(f\"➡️ RMSE of first 5 outliers: {rmse_outliers[:5]}\")\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "    return new_df, recall, precision, f1, tp, fn, fp, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103334e3",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5815053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 ----------------\n",
      "Step 0: Loss = 43482.85546875\n",
      "Step 10: Loss = 43383.546875\n",
      "Step 20: Loss = 45798.37890625\n",
      "epoch:0/100 |Loss: 40983.78125\n",
      "\n",
      "Epoch 2/100 ----------------\n",
      "Step 0: Loss = 47394.89453125\n",
      "Step 10: Loss = 38452.80078125\n",
      "Step 20: Loss = 48371.25390625\n",
      "epoch:1/100 |Loss: 39383.5859375\n",
      "\n",
      "Epoch 3/100 ----------------\n",
      "Step 0: Loss = 50227.69921875\n",
      "Step 10: Loss = 44641.80078125\n",
      "Step 20: Loss = 42050.34375\n",
      "epoch:2/100 |Loss: 47948.82421875\n",
      "\n",
      "Epoch 4/100 ----------------\n",
      "Step 0: Loss = 47073.99609375\n",
      "Step 10: Loss = 43723.16796875\n",
      "Step 20: Loss = 38145.35546875\n",
      "epoch:3/100 |Loss: 35812.59765625\n",
      "\n",
      "Epoch 5/100 ----------------\n",
      "Step 0: Loss = 46250.53125\n",
      "Step 10: Loss = 41120.85546875\n",
      "Step 20: Loss = 44226.4140625\n",
      "epoch:4/100 |Loss: 43702.53515625\n",
      "\n",
      "Epoch 6/100 ----------------\n",
      "Step 0: Loss = 44498.19921875\n",
      "Step 10: Loss = 46576.66796875\n",
      "Step 20: Loss = 46234.26171875\n",
      "epoch:5/100 |Loss: 42784.5546875\n",
      "\n",
      "Epoch 7/100 ----------------\n",
      "Step 0: Loss = 40060.94921875\n",
      "Step 10: Loss = 39241.3046875\n",
      "Step 20: Loss = 35742.4140625\n",
      "epoch:6/100 |Loss: 37628.66796875\n",
      "\n",
      "Epoch 8/100 ----------------\n",
      "Step 0: Loss = 40310.54296875\n",
      "Step 10: Loss = 38374.9453125\n",
      "Step 20: Loss = 46323.83984375\n",
      "epoch:7/100 |Loss: 36906.3203125\n",
      "\n",
      "Epoch 9/100 ----------------\n",
      "Step 0: Loss = 41197.2890625\n",
      "Step 10: Loss = 46440.50390625\n",
      "Step 20: Loss = 41472.40625\n",
      "epoch:8/100 |Loss: 45489.89453125\n",
      "\n",
      "Epoch 10/100 ----------------\n",
      "Step 0: Loss = 42708.13671875\n",
      "Step 10: Loss = 41947.828125\n",
      "Step 20: Loss = 34094.9140625\n",
      "epoch:9/100 |Loss: 48463.30859375\n",
      "\n",
      "Epoch 11/100 ----------------\n",
      "Step 0: Loss = 41224.43359375\n",
      "Step 10: Loss = 41146.453125\n",
      "Step 20: Loss = 45426.98828125\n",
      "epoch:10/100 |Loss: 42441.52734375\n",
      "\n",
      "Epoch 12/100 ----------------\n",
      "Step 0: Loss = 43121.9296875\n",
      "Step 10: Loss = 39766.02734375\n",
      "Step 20: Loss = 35180.73828125\n",
      "epoch:11/100 |Loss: 41685.6796875\n",
      "\n",
      "Epoch 13/100 ----------------\n",
      "Step 0: Loss = 37324.484375\n",
      "Step 10: Loss = 36236.671875\n",
      "Step 20: Loss = 39966.76953125\n",
      "epoch:12/100 |Loss: 40479.60546875\n",
      "\n",
      "Epoch 14/100 ----------------\n",
      "Step 0: Loss = 35763.52734375\n",
      "Step 10: Loss = 29525.068359375\n",
      "Step 20: Loss = 35222.859375\n",
      "epoch:13/100 |Loss: 37185.60546875\n",
      "\n",
      "Epoch 15/100 ----------------\n",
      "Step 0: Loss = 35546.37109375\n",
      "Step 10: Loss = 31142.173828125\n",
      "Step 20: Loss = 37933.41015625\n",
      "epoch:14/100 |Loss: 28141.154296875\n",
      "\n",
      "Epoch 16/100 ----------------\n",
      "Step 0: Loss = 32890.99609375\n",
      "Step 10: Loss = 37050.046875\n",
      "Step 20: Loss = 35987.046875\n",
      "epoch:15/100 |Loss: 26683.427734375\n",
      "\n",
      "Epoch 17/100 ----------------\n",
      "Step 0: Loss = 28918.662109375\n",
      "Step 10: Loss = 34467.68359375\n",
      "Step 20: Loss = 31033.30859375\n",
      "epoch:16/100 |Loss: 39906.78125\n",
      "\n",
      "Epoch 18/100 ----------------\n",
      "Step 0: Loss = 30530.80859375\n",
      "Step 10: Loss = 31790.51953125\n",
      "Step 20: Loss = 30842.296875\n",
      "epoch:17/100 |Loss: 30308.91796875\n",
      "\n",
      "Epoch 19/100 ----------------\n",
      "Step 0: Loss = 25081.228515625\n",
      "Step 10: Loss = 32221.55859375\n",
      "Step 20: Loss = 31980.4609375\n",
      "epoch:18/100 |Loss: 32026.5703125\n",
      "\n",
      "Epoch 20/100 ----------------\n",
      "Step 0: Loss = 27682.275390625\n",
      "Step 10: Loss = 30537.0390625\n",
      "Step 20: Loss = 28468.966796875\n",
      "epoch:19/100 |Loss: 27733.70703125\n",
      "\n",
      "Epoch 21/100 ----------------\n",
      "Step 0: Loss = 22947.943359375\n",
      "Step 10: Loss = 23964.302734375\n",
      "Step 20: Loss = 23465.0546875\n",
      "epoch:20/100 |Loss: 25169.021484375\n",
      "\n",
      "Epoch 22/100 ----------------\n",
      "Step 0: Loss = 25502.416015625\n",
      "Step 10: Loss = 23372.212890625\n",
      "Step 20: Loss = 26823.26171875\n",
      "epoch:21/100 |Loss: 24790.576171875\n",
      "\n",
      "Epoch 23/100 ----------------\n",
      "Step 0: Loss = 24230.62109375\n",
      "Step 10: Loss = 22065.265625\n",
      "Step 20: Loss = 23726.537109375\n",
      "epoch:22/100 |Loss: 27202.8046875\n",
      "\n",
      "Epoch 24/100 ----------------\n",
      "Step 0: Loss = 23110.005859375\n",
      "Step 10: Loss = 21845.517578125\n",
      "Step 20: Loss = 21546.888671875\n",
      "epoch:23/100 |Loss: 22460.87109375\n",
      "\n",
      "Epoch 25/100 ----------------\n",
      "Step 0: Loss = 22083.755859375\n",
      "Step 10: Loss = 18683.623046875\n",
      "Step 20: Loss = 18047.109375\n",
      "epoch:24/100 |Loss: 14390.7900390625\n",
      "\n",
      "Epoch 26/100 ----------------\n",
      "Step 0: Loss = 19917.662109375\n",
      "Step 10: Loss = 20019.08984375\n",
      "Step 20: Loss = 19802.029296875\n",
      "epoch:25/100 |Loss: 18178.94140625\n",
      "\n",
      "Epoch 27/100 ----------------\n",
      "Step 0: Loss = 22196.529296875\n",
      "Step 10: Loss = 20331.87109375\n",
      "Step 20: Loss = 13243.654296875\n",
      "epoch:26/100 |Loss: 22697.70703125\n",
      "\n",
      "Epoch 28/100 ----------------\n",
      "Step 0: Loss = 16647.384765625\n",
      "Step 10: Loss = 18930.32421875\n",
      "Step 20: Loss = 18834.193359375\n",
      "epoch:27/100 |Loss: 12389.462890625\n",
      "\n",
      "Epoch 29/100 ----------------\n",
      "Step 0: Loss = 15983.1318359375\n",
      "Step 10: Loss = 14478.935546875\n",
      "Step 20: Loss = 12969.9228515625\n",
      "epoch:28/100 |Loss: 12557.85546875\n",
      "\n",
      "Epoch 30/100 ----------------\n",
      "Step 0: Loss = 16145.9873046875\n",
      "Step 10: Loss = 12094.7060546875\n",
      "Step 20: Loss = 15001.490234375\n",
      "epoch:29/100 |Loss: 16529.171875\n",
      "\n",
      "Epoch 31/100 ----------------\n",
      "Step 0: Loss = 10603.904296875\n",
      "Step 10: Loss = 14059.439453125\n",
      "Step 20: Loss = 13971.740234375\n",
      "epoch:30/100 |Loss: 10591.9375\n",
      "\n",
      "Epoch 32/100 ----------------\n",
      "Step 0: Loss = 12713.90234375\n",
      "Step 10: Loss = 12770.9140625\n",
      "Step 20: Loss = 12622.9013671875\n",
      "epoch:31/100 |Loss: 16055.7314453125\n",
      "\n",
      "Epoch 33/100 ----------------\n",
      "Step 0: Loss = 12254.2958984375\n",
      "Step 10: Loss = 11970.1259765625\n",
      "Step 20: Loss = 11307.0791015625\n",
      "epoch:32/100 |Loss: 11612.5361328125\n",
      "\n",
      "Epoch 34/100 ----------------\n",
      "Step 0: Loss = 10667.14453125\n",
      "Step 10: Loss = 11209.8408203125\n",
      "Step 20: Loss = 9825.650390625\n",
      "epoch:33/100 |Loss: 9292.3671875\n",
      "\n",
      "Epoch 35/100 ----------------\n",
      "Step 0: Loss = 8596.708984375\n",
      "Step 10: Loss = 11865.2822265625\n",
      "Step 20: Loss = 9512.0810546875\n",
      "epoch:34/100 |Loss: 8077.99853515625\n",
      "\n",
      "Epoch 36/100 ----------------\n",
      "Step 0: Loss = 7239.849609375\n",
      "Step 10: Loss = 9600.521484375\n",
      "Step 20: Loss = 8948.21875\n",
      "epoch:35/100 |Loss: 7322.62109375\n",
      "\n",
      "Epoch 37/100 ----------------\n",
      "Step 0: Loss = 10760.0751953125\n",
      "Step 10: Loss = 9450.1015625\n",
      "Step 20: Loss = 6923.24609375\n",
      "epoch:36/100 |Loss: 7681.4716796875\n",
      "\n",
      "Epoch 38/100 ----------------\n",
      "Step 0: Loss = 8719.580078125\n",
      "Step 10: Loss = 5526.1943359375\n",
      "Step 20: Loss = 8000.208984375\n",
      "epoch:37/100 |Loss: 7097.06884765625\n",
      "\n",
      "Epoch 39/100 ----------------\n",
      "Step 0: Loss = 5482.9208984375\n",
      "Step 10: Loss = 5939.1240234375\n",
      "Step 20: Loss = 4512.5654296875\n",
      "epoch:38/100 |Loss: 5234.7470703125\n",
      "\n",
      "Epoch 40/100 ----------------\n",
      "Step 0: Loss = 4591.94921875\n",
      "Step 10: Loss = 7005.06982421875\n",
      "Step 20: Loss = 4980.6875\n",
      "epoch:39/100 |Loss: 9443.8193359375\n",
      "\n",
      "Epoch 41/100 ----------------\n",
      "Step 0: Loss = 4233.4970703125\n",
      "Step 10: Loss = 4798.13134765625\n",
      "Step 20: Loss = 5009.03955078125\n",
      "epoch:40/100 |Loss: 4944.6123046875\n",
      "\n",
      "Epoch 42/100 ----------------\n",
      "Step 0: Loss = 6776.56103515625\n",
      "Step 10: Loss = 4711.80517578125\n",
      "Step 20: Loss = 4854.37158203125\n",
      "epoch:41/100 |Loss: 3148.314453125\n",
      "\n",
      "Epoch 43/100 ----------------\n",
      "Step 0: Loss = 4656.5146484375\n",
      "Step 10: Loss = 3211.968505859375\n",
      "Step 20: Loss = 4132.46484375\n",
      "epoch:42/100 |Loss: 2639.260986328125\n",
      "\n",
      "Epoch 44/100 ----------------\n",
      "Step 0: Loss = 2664.82666015625\n",
      "Step 10: Loss = 3354.07666015625\n",
      "Step 20: Loss = 3761.834228515625\n",
      "epoch:43/100 |Loss: 4277.99462890625\n",
      "\n",
      "Epoch 45/100 ----------------\n",
      "Step 0: Loss = 3446.87841796875\n",
      "Step 10: Loss = 3337.527587890625\n",
      "Step 20: Loss = 4009.035400390625\n",
      "epoch:44/100 |Loss: 2165.318603515625\n",
      "\n",
      "Epoch 46/100 ----------------\n",
      "Step 0: Loss = 3608.771484375\n",
      "Step 10: Loss = 3097.79248046875\n",
      "Step 20: Loss = 1910.2259521484375\n",
      "epoch:45/100 |Loss: 2370.474609375\n",
      "\n",
      "Epoch 47/100 ----------------\n",
      "Step 0: Loss = 1762.6624755859375\n",
      "Step 10: Loss = 2711.74609375\n",
      "Step 20: Loss = 1770.7738037109375\n",
      "epoch:46/100 |Loss: 1784.103271484375\n",
      "\n",
      "Epoch 48/100 ----------------\n",
      "Step 0: Loss = 2319.478515625\n",
      "Step 10: Loss = 1837.4495849609375\n",
      "Step 20: Loss = 2122.206298828125\n",
      "epoch:47/100 |Loss: 2306.716796875\n",
      "\n",
      "Epoch 49/100 ----------------\n",
      "Step 0: Loss = 2963.87744140625\n",
      "Step 10: Loss = 2206.658935546875\n",
      "Step 20: Loss = 1436.96728515625\n",
      "epoch:48/100 |Loss: 1896.0675048828125\n",
      "\n",
      "Epoch 50/100 ----------------\n",
      "Step 0: Loss = 1728.449462890625\n",
      "Step 10: Loss = 1263.7718505859375\n",
      "Step 20: Loss = 1221.818115234375\n",
      "epoch:49/100 |Loss: 1259.7720947265625\n",
      "\n",
      "Epoch 51/100 ----------------\n",
      "Step 0: Loss = 1120.847412109375\n",
      "Step 10: Loss = 1699.44287109375\n",
      "Step 20: Loss = 1083.5634765625\n",
      "epoch:50/100 |Loss: 1243.4090576171875\n",
      "\n",
      "Epoch 52/100 ----------------\n",
      "Step 0: Loss = 1642.6722412109375\n",
      "Step 10: Loss = 1728.12744140625\n",
      "Step 20: Loss = 1067.3616943359375\n",
      "epoch:51/100 |Loss: 1147.3641357421875\n",
      "\n",
      "Epoch 53/100 ----------------\n",
      "Step 0: Loss = 1544.8194580078125\n",
      "Step 10: Loss = 1250.7794189453125\n",
      "Step 20: Loss = 1426.88671875\n",
      "epoch:52/100 |Loss: 1184.7474365234375\n",
      "\n",
      "Epoch 54/100 ----------------\n",
      "Step 0: Loss = 985.162109375\n",
      "Step 10: Loss = 1525.1982421875\n",
      "Step 20: Loss = 1021.28564453125\n",
      "epoch:53/100 |Loss: 597.146484375\n",
      "\n",
      "Epoch 55/100 ----------------\n",
      "Step 0: Loss = 1580.7841796875\n",
      "Step 10: Loss = 951.6515502929688\n",
      "Step 20: Loss = 518.1044921875\n",
      "epoch:54/100 |Loss: 1454.804443359375\n",
      "\n",
      "Epoch 56/100 ----------------\n",
      "Step 0: Loss = 1015.4262084960938\n",
      "Step 10: Loss = 855.19677734375\n",
      "Step 20: Loss = 415.6165771484375\n",
      "epoch:55/100 |Loss: 417.2181701660156\n",
      "\n",
      "Epoch 57/100 ----------------\n",
      "Step 0: Loss = 423.6922912597656\n",
      "Step 10: Loss = 464.6927795410156\n",
      "Step 20: Loss = 720.0366821289062\n",
      "epoch:56/100 |Loss: 1105.10107421875\n",
      "\n",
      "Epoch 58/100 ----------------\n",
      "Step 0: Loss = 521.84619140625\n",
      "Step 10: Loss = 475.4012145996094\n",
      "Step 20: Loss = 1124.5765380859375\n",
      "epoch:57/100 |Loss: 2001.9012451171875\n",
      "\n",
      "Epoch 59/100 ----------------\n",
      "Step 0: Loss = 603.1317138671875\n",
      "Step 10: Loss = 383.9026184082031\n",
      "Step 20: Loss = 283.9450378417969\n",
      "epoch:58/100 |Loss: 544.2257080078125\n",
      "\n",
      "Epoch 60/100 ----------------\n",
      "Step 0: Loss = 719.83984375\n",
      "Step 10: Loss = 607.3084106445312\n",
      "Step 20: Loss = 414.5869445800781\n",
      "epoch:59/100 |Loss: 694.0330200195312\n",
      "\n",
      "Epoch 61/100 ----------------\n",
      "Step 0: Loss = 300.3804626464844\n",
      "Step 10: Loss = 301.1854553222656\n",
      "Step 20: Loss = 163.72914123535156\n",
      "epoch:60/100 |Loss: 1125.7430419921875\n",
      "\n",
      "Epoch 62/100 ----------------\n",
      "Step 0: Loss = 585.2259521484375\n",
      "Step 10: Loss = 427.5238342285156\n",
      "Step 20: Loss = 148.92578125\n",
      "epoch:61/100 |Loss: 94.51210021972656\n",
      "\n",
      "Epoch 63/100 ----------------\n",
      "Step 0: Loss = 119.74078369140625\n",
      "Step 10: Loss = 336.8594055175781\n",
      "Step 20: Loss = 92.65673065185547\n",
      "epoch:62/100 |Loss: 608.0137329101562\n",
      "\n",
      "Epoch 64/100 ----------------\n",
      "Step 0: Loss = 286.185302734375\n",
      "Step 10: Loss = 173.32876586914062\n",
      "Step 20: Loss = 102.09504699707031\n",
      "epoch:63/100 |Loss: 110.55447387695312\n",
      "\n",
      "Epoch 65/100 ----------------\n",
      "Step 0: Loss = 192.66226196289062\n",
      "Step 10: Loss = 222.60711669921875\n",
      "Step 20: Loss = 149.88406372070312\n",
      "epoch:64/100 |Loss: 93.56710815429688\n",
      "\n",
      "Epoch 66/100 ----------------\n",
      "Step 0: Loss = 416.4580383300781\n",
      "Step 10: Loss = 79.7566146850586\n",
      "Step 20: Loss = 914.4638061523438\n",
      "epoch:65/100 |Loss: 83.65174865722656\n",
      "\n",
      "Epoch 67/100 ----------------\n",
      "Step 0: Loss = 456.5523681640625\n",
      "Step 10: Loss = 41.81632995605469\n",
      "Step 20: Loss = 481.2872619628906\n",
      "epoch:66/100 |Loss: 148.5270233154297\n",
      "\n",
      "Epoch 68/100 ----------------\n",
      "Step 0: Loss = 48.34033966064453\n",
      "Step 10: Loss = 33.847145080566406\n",
      "Step 20: Loss = 520.29150390625\n",
      "epoch:67/100 |Loss: 34.10639190673828\n",
      "\n",
      "Epoch 69/100 ----------------\n",
      "Step 0: Loss = 52.49745559692383\n",
      "Step 10: Loss = 48.847007751464844\n",
      "Step 20: Loss = 40.14596939086914\n",
      "epoch:68/100 |Loss: 806.457763671875\n",
      "\n",
      "Epoch 70/100 ----------------\n",
      "Step 0: Loss = 210.59613037109375\n",
      "Step 10: Loss = 32.6240348815918\n",
      "Step 20: Loss = 269.248291015625\n",
      "epoch:69/100 |Loss: 55.98423385620117\n",
      "\n",
      "Epoch 71/100 ----------------\n",
      "Step 0: Loss = 26.736122131347656\n",
      "Step 10: Loss = 30.931194305419922\n",
      "Step 20: Loss = 133.60595703125\n",
      "epoch:70/100 |Loss: 153.48182678222656\n",
      "\n",
      "Epoch 72/100 ----------------\n",
      "Step 0: Loss = 256.4828186035156\n",
      "Step 10: Loss = 80.06536102294922\n",
      "Step 20: Loss = 75.0775375366211\n",
      "epoch:71/100 |Loss: 85.2070083618164\n",
      "\n",
      "Epoch 73/100 ----------------\n",
      "Step 0: Loss = 126.09251403808594\n",
      "Step 10: Loss = 140.83016967773438\n",
      "Step 20: Loss = 312.1990661621094\n",
      "epoch:72/100 |Loss: 88.8570556640625\n",
      "\n",
      "Epoch 74/100 ----------------\n",
      "Step 0: Loss = 46.956787109375\n",
      "Step 10: Loss = 49.43764114379883\n",
      "Step 20: Loss = 106.80918884277344\n",
      "epoch:73/100 |Loss: 45.07560729980469\n",
      "\n",
      "Epoch 75/100 ----------------\n",
      "Step 0: Loss = 26.426294326782227\n",
      "Step 10: Loss = 79.3834457397461\n",
      "Step 20: Loss = 18.386165618896484\n",
      "epoch:74/100 |Loss: 42.93832778930664\n",
      "\n",
      "Epoch 76/100 ----------------\n",
      "Step 0: Loss = 205.5100555419922\n",
      "Step 10: Loss = 102.32501220703125\n",
      "Step 20: Loss = 51.094032287597656\n",
      "epoch:75/100 |Loss: 126.65310668945312\n",
      "\n",
      "Epoch 77/100 ----------------\n",
      "Step 0: Loss = 174.79640197753906\n",
      "Step 10: Loss = 105.59011840820312\n",
      "Step 20: Loss = 30.966039657592773\n",
      "epoch:76/100 |Loss: 51.681156158447266\n",
      "\n",
      "Epoch 78/100 ----------------\n",
      "Step 0: Loss = 134.4691925048828\n",
      "Step 10: Loss = 31.98948860168457\n",
      "Step 20: Loss = 44.28984069824219\n",
      "epoch:77/100 |Loss: 101.55184173583984\n",
      "\n",
      "Epoch 79/100 ----------------\n",
      "Step 0: Loss = 163.8327178955078\n",
      "Step 10: Loss = 67.29107666015625\n",
      "Step 20: Loss = 63.0284309387207\n",
      "epoch:78/100 |Loss: 149.7015838623047\n",
      "\n",
      "Epoch 80/100 ----------------\n",
      "Step 0: Loss = 234.7290496826172\n",
      "Step 10: Loss = 24.253036499023438\n",
      "Step 20: Loss = 26.454381942749023\n",
      "epoch:79/100 |Loss: 29.874940872192383\n",
      "\n",
      "Epoch 81/100 ----------------\n",
      "Step 0: Loss = 35.44797897338867\n",
      "Step 10: Loss = 14.391996383666992\n",
      "Step 20: Loss = 23.23924446105957\n",
      "epoch:80/100 |Loss: 33.46003341674805\n",
      "\n",
      "Epoch 82/100 ----------------\n",
      "Step 0: Loss = 48.491241455078125\n",
      "Step 10: Loss = 16.458362579345703\n",
      "Step 20: Loss = 24.5606746673584\n",
      "epoch:81/100 |Loss: 90.06721496582031\n",
      "\n",
      "Epoch 83/100 ----------------\n",
      "Step 0: Loss = 145.4086456298828\n",
      "Step 10: Loss = 13.55688190460205\n",
      "Step 20: Loss = 49.78810501098633\n",
      "epoch:82/100 |Loss: 42.384727478027344\n",
      "\n",
      "Epoch 84/100 ----------------\n",
      "Step 0: Loss = 31.233488082885742\n",
      "Step 10: Loss = 75.5355224609375\n",
      "Step 20: Loss = 78.47618865966797\n",
      "epoch:83/100 |Loss: 69.08735656738281\n",
      "\n",
      "Epoch 85/100 ----------------\n",
      "Step 0: Loss = 24.037424087524414\n",
      "Step 10: Loss = 148.9363555908203\n",
      "Step 20: Loss = 33.3635139465332\n",
      "epoch:84/100 |Loss: 100.7886962890625\n",
      "\n",
      "Epoch 86/100 ----------------\n",
      "Step 0: Loss = 9.1174955368042\n",
      "Step 10: Loss = 86.31539154052734\n",
      "Step 20: Loss = 131.2015380859375\n",
      "epoch:85/100 |Loss: 279.294189453125\n",
      "\n",
      "Epoch 87/100 ----------------\n",
      "Step 0: Loss = 453.4688415527344\n",
      "Step 10: Loss = 38.43239974975586\n",
      "Step 20: Loss = 24.62480354309082\n",
      "epoch:86/100 |Loss: 188.15538024902344\n",
      "\n",
      "Epoch 88/100 ----------------\n",
      "Step 0: Loss = 85.04238891601562\n",
      "Step 10: Loss = 80.35977172851562\n",
      "Step 20: Loss = 124.43490600585938\n",
      "epoch:87/100 |Loss: 240.32089233398438\n",
      "\n",
      "Epoch 89/100 ----------------\n",
      "Step 0: Loss = 58.10139465332031\n",
      "Step 10: Loss = 73.00936889648438\n",
      "Step 20: Loss = 24.195453643798828\n",
      "epoch:88/100 |Loss: 88.6583480834961\n",
      "\n",
      "Epoch 90/100 ----------------\n",
      "Step 0: Loss = 161.44894409179688\n",
      "Step 10: Loss = 14.80820369720459\n",
      "Step 20: Loss = 39.812339782714844\n",
      "epoch:89/100 |Loss: 33.188350677490234\n",
      "\n",
      "Epoch 91/100 ----------------\n",
      "Step 0: Loss = 34.14926528930664\n",
      "Step 10: Loss = 74.11046600341797\n",
      "Step 20: Loss = 14.5491304397583\n",
      "epoch:90/100 |Loss: 179.93540954589844\n",
      "\n",
      "Epoch 92/100 ----------------\n",
      "Step 0: Loss = 14.489490509033203\n",
      "Step 10: Loss = 60.3139533996582\n",
      "Step 20: Loss = 27.462007522583008\n",
      "epoch:91/100 |Loss: 119.33256530761719\n",
      "\n",
      "Epoch 93/100 ----------------\n",
      "Step 0: Loss = 89.89102172851562\n",
      "Step 10: Loss = 23.376596450805664\n",
      "Step 20: Loss = 38.4132080078125\n",
      "epoch:92/100 |Loss: 72.77650451660156\n",
      "\n",
      "Epoch 94/100 ----------------\n",
      "Step 0: Loss = 95.6685562133789\n",
      "Step 10: Loss = 153.74969482421875\n",
      "Step 20: Loss = 26.588733673095703\n",
      "epoch:93/100 |Loss: 187.6964111328125\n",
      "\n",
      "Epoch 95/100 ----------------\n",
      "Step 0: Loss = 53.93415832519531\n",
      "Step 10: Loss = 14.457338333129883\n",
      "Step 20: Loss = 8.667957305908203\n",
      "epoch:94/100 |Loss: 68.494384765625\n",
      "\n",
      "Epoch 96/100 ----------------\n",
      "Step 0: Loss = 13.738110542297363\n",
      "Step 10: Loss = 139.3781280517578\n",
      "Step 20: Loss = 29.093257904052734\n",
      "epoch:95/100 |Loss: 16.551950454711914\n",
      "\n",
      "Epoch 97/100 ----------------\n",
      "Step 0: Loss = 24.46833610534668\n",
      "Step 10: Loss = 8.664385795593262\n",
      "Step 20: Loss = 48.489681243896484\n",
      "epoch:96/100 |Loss: 29.739877700805664\n",
      "\n",
      "Epoch 98/100 ----------------\n",
      "Step 0: Loss = 20.470434188842773\n",
      "Step 10: Loss = 14.642356872558594\n",
      "Step 20: Loss = 14.260009765625\n",
      "epoch:97/100 |Loss: 47.94636154174805\n",
      "\n",
      "Epoch 99/100 ----------------\n",
      "Step 0: Loss = 280.9450988769531\n",
      "Step 10: Loss = 66.17533111572266\n",
      "Step 20: Loss = 14.678250312805176\n",
      "epoch:98/100 |Loss: 305.5875244140625\n",
      "\n",
      "Epoch 100/100 ----------------\n",
      "Step 0: Loss = 44.504302978515625\n",
      "Step 10: Loss = 196.0440673828125\n",
      "Step 20: Loss = 12.778301239013672\n",
      "epoch:99/100 |Loss: 133.3350830078125\n",
      "max AD score 45.649746\n",
      "thres: 18.477903\n",
      "➡️ Max RMSE among outliers: 57.57\n",
      "Data training:\n",
      "Precision: 0.882, Recall: 0.382, F1-score: 0.533\n",
      "Elapsed: 0.1631\n",
      "➡️ Max RMSE among outliers: 519.78\n",
      "Data testing:\n",
      "Precision: 0.462, Recall: 0.758, F1-score: 0.574\n",
      "Elapsed: 2.3720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_15692/2206432067.py:48: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop('label', axis=1)\n",
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "start = time.time()\n",
    "_, recall, precision, f1, tp, fn, fp, tn = test_model(model, test_df, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data training:')\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp_2, fn_2, fp_2, tn_2 = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp_2, fn_2],\n",
    "    [fp_2, tn_2]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='gray_r', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix Control\")\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='gray_r', ax=axs[1])\n",
    "axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e6d97",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd0094",
   "metadata": {},
   "source": [
    "### a. Concat old + new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d151e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "reference_vec = train_df.mean().values.reshape(1, -1)\n",
    "similarities = cosine_similarity(train_df2.values, reference_vec).flatten()\n",
    "\n",
    "k = len(train_df)\n",
    "top_k_idx = np.argsort(similarities)[-k:]\n",
    "X2_similar = train_df2.iloc[top_k_idx]\n",
    "\n",
    "retrain_df = pd.concat([train_df, X2_similar]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X_train = retrain_df.drop('label',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01b2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 ----------------\n",
      "Step 0: Loss = 3049464.75\n",
      "Step 10: Loss = 2953041.75\n",
      "Step 20: Loss = 3152404.75\n",
      "Step 30: Loss = 3075275.75\n",
      "Step 40: Loss = 3116042.75\n",
      "epoch:0/100 |Loss: 2396976.25\n",
      "\n",
      "Epoch 2/100 ----------------\n",
      "Step 0: Loss = 3052053.0\n",
      "Step 10: Loss = 3146415.25\n",
      "Step 20: Loss = 3303217.75\n",
      "Step 30: Loss = 3028830.75\n",
      "Step 40: Loss = 3208341.5\n",
      "epoch:1/100 |Loss: 3138755.25\n",
      "\n",
      "Epoch 3/100 ----------------\n",
      "Step 0: Loss = 3039370.0\n",
      "Step 10: Loss = 2959216.75\n",
      "Step 20: Loss = 2893264.25\n",
      "Step 30: Loss = 3194073.0\n",
      "Step 40: Loss = 3210803.25\n",
      "epoch:2/100 |Loss: 3077772.25\n",
      "\n",
      "Epoch 4/100 ----------------\n",
      "Step 0: Loss = 2931472.75\n",
      "Step 10: Loss = 3222739.25\n",
      "Step 20: Loss = 3153168.75\n",
      "Step 30: Loss = 3357296.75\n",
      "Step 40: Loss = 3300766.25\n",
      "epoch:3/100 |Loss: 3429788.75\n",
      "\n",
      "Epoch 5/100 ----------------\n",
      "Step 0: Loss = 3127370.75\n",
      "Step 10: Loss = 3225384.75\n",
      "Step 20: Loss = 3303802.25\n",
      "Step 30: Loss = 2943496.5\n",
      "Step 40: Loss = 3107797.5\n",
      "epoch:4/100 |Loss: 3808413.25\n",
      "\n",
      "Epoch 6/100 ----------------\n",
      "Step 0: Loss = 3038140.75\n",
      "Step 10: Loss = 3203519.25\n",
      "Step 20: Loss = 2911657.0\n",
      "Step 30: Loss = 2958510.75\n",
      "Step 40: Loss = 2762669.25\n",
      "epoch:5/100 |Loss: 4069538.75\n",
      "\n",
      "Epoch 7/100 ----------------\n",
      "Step 0: Loss = 3085850.25\n",
      "Step 10: Loss = 2967429.25\n",
      "Step 20: Loss = 2913234.0\n",
      "Step 30: Loss = 2979565.25\n",
      "Step 40: Loss = 3008814.25\n",
      "epoch:6/100 |Loss: 3928540.5\n",
      "\n",
      "Epoch 8/100 ----------------\n",
      "Step 0: Loss = 3034506.5\n",
      "Step 10: Loss = 3102684.75\n",
      "Step 20: Loss = 2938529.0\n",
      "Step 30: Loss = 3064557.75\n",
      "Step 40: Loss = 3069560.25\n",
      "epoch:7/100 |Loss: 2580610.25\n",
      "\n",
      "Epoch 9/100 ----------------\n",
      "Step 0: Loss = 3063604.75\n",
      "Step 10: Loss = 2990877.5\n",
      "Step 20: Loss = 2937633.75\n",
      "Step 30: Loss = 3014328.75\n",
      "Step 40: Loss = 3114845.25\n",
      "epoch:8/100 |Loss: 3264885.5\n",
      "\n",
      "Epoch 10/100 ----------------\n",
      "Step 0: Loss = 3123434.5\n",
      "Step 10: Loss = 2979156.75\n",
      "Step 20: Loss = 2946483.25\n",
      "Step 30: Loss = 2994951.5\n",
      "Step 40: Loss = 3042352.75\n",
      "epoch:9/100 |Loss: 3004705.5\n",
      "\n",
      "Epoch 11/100 ----------------\n",
      "Step 0: Loss = 2928960.5\n",
      "Step 10: Loss = 2982768.0\n",
      "Step 20: Loss = 2916761.25\n",
      "Step 30: Loss = 2705447.25\n",
      "Step 40: Loss = 2981439.5\n",
      "epoch:10/100 |Loss: 2852445.75\n",
      "\n",
      "Epoch 12/100 ----------------\n",
      "Step 0: Loss = 3149662.75\n",
      "Step 10: Loss = 3074936.0\n",
      "Step 20: Loss = 3100975.0\n",
      "Step 30: Loss = 2733234.0\n",
      "Step 40: Loss = 2961350.75\n",
      "epoch:11/100 |Loss: 2677017.0\n",
      "\n",
      "Epoch 13/100 ----------------\n",
      "Step 0: Loss = 2818771.5\n",
      "Step 10: Loss = 2715224.75\n",
      "Step 20: Loss = 2908939.25\n",
      "Step 30: Loss = 2733581.0\n",
      "Step 40: Loss = 2673409.75\n",
      "epoch:12/100 |Loss: 3641383.25\n",
      "\n",
      "Epoch 14/100 ----------------\n",
      "Step 0: Loss = 2776160.5\n",
      "Step 10: Loss = 2736412.25\n",
      "Step 20: Loss = 2815471.25\n",
      "Step 30: Loss = 2578035.25\n",
      "Step 40: Loss = 2728770.5\n",
      "epoch:13/100 |Loss: 2541074.5\n",
      "\n",
      "Epoch 15/100 ----------------\n",
      "Step 0: Loss = 2746883.5\n",
      "Step 10: Loss = 2886864.5\n",
      "Step 20: Loss = 2565858.0\n",
      "Step 30: Loss = 2819426.0\n",
      "Step 40: Loss = 2699748.75\n",
      "epoch:14/100 |Loss: 3011343.0\n",
      "\n",
      "Epoch 16/100 ----------------\n",
      "Step 0: Loss = 2728294.0\n",
      "Step 10: Loss = 2723660.5\n",
      "Step 20: Loss = 2760410.0\n",
      "Step 30: Loss = 2502198.0\n",
      "Step 40: Loss = 2872470.0\n",
      "epoch:15/100 |Loss: 2077977.875\n",
      "\n",
      "Epoch 17/100 ----------------\n",
      "Step 0: Loss = 2874866.5\n",
      "Step 10: Loss = 2660155.0\n",
      "Step 20: Loss = 2654112.25\n",
      "Step 30: Loss = 2813982.0\n",
      "Step 40: Loss = 2537930.5\n",
      "epoch:16/100 |Loss: 2090948.875\n",
      "\n",
      "Epoch 18/100 ----------------\n",
      "Step 0: Loss = 2493349.5\n",
      "Step 10: Loss = 2476183.25\n",
      "Step 20: Loss = 2353144.75\n",
      "Step 30: Loss = 2589147.75\n",
      "Step 40: Loss = 2538133.0\n",
      "epoch:17/100 |Loss: 2391917.0\n",
      "\n",
      "Epoch 19/100 ----------------\n",
      "Step 0: Loss = 2506248.75\n",
      "Step 10: Loss = 2444175.5\n",
      "Step 20: Loss = 2494008.75\n",
      "Step 30: Loss = 2635581.25\n",
      "Step 40: Loss = 2707669.5\n",
      "epoch:18/100 |Loss: 2543531.5\n",
      "\n",
      "Epoch 20/100 ----------------\n",
      "Step 0: Loss = 2351859.5\n",
      "Step 10: Loss = 2453883.5\n",
      "Step 20: Loss = 2444913.5\n",
      "Step 30: Loss = 2448506.5\n",
      "Step 40: Loss = 2433201.25\n",
      "epoch:19/100 |Loss: 2698788.5\n",
      "\n",
      "Epoch 21/100 ----------------\n",
      "Step 0: Loss = 2502669.75\n",
      "Step 10: Loss = 2466118.0\n",
      "Step 20: Loss = 2502507.25\n",
      "Step 30: Loss = 2431815.25\n",
      "Step 40: Loss = 2369828.75\n",
      "epoch:20/100 |Loss: 2909644.25\n",
      "\n",
      "Epoch 22/100 ----------------\n",
      "Step 0: Loss = 2430775.0\n",
      "Step 10: Loss = 2254884.5\n",
      "Step 20: Loss = 2380941.0\n",
      "Step 30: Loss = 2420872.75\n",
      "Step 40: Loss = 2205470.5\n",
      "epoch:21/100 |Loss: 2361155.5\n",
      "\n",
      "Epoch 23/100 ----------------\n",
      "Step 0: Loss = 2308021.5\n",
      "Step 10: Loss = 2224246.75\n",
      "Step 20: Loss = 2354775.5\n",
      "Step 30: Loss = 2184534.25\n",
      "Step 40: Loss = 2550582.0\n",
      "epoch:22/100 |Loss: 1882457.0\n",
      "\n",
      "Epoch 24/100 ----------------\n",
      "Step 0: Loss = 2278709.25\n",
      "Step 10: Loss = 2373952.0\n",
      "Step 20: Loss = 2308147.75\n",
      "Step 30: Loss = 2459557.5\n",
      "Step 40: Loss = 2318699.25\n",
      "epoch:23/100 |Loss: 2927230.75\n",
      "\n",
      "Epoch 25/100 ----------------\n",
      "Step 0: Loss = 2264510.0\n",
      "Step 10: Loss = 2275131.75\n",
      "Step 20: Loss = 2394965.5\n",
      "Step 30: Loss = 2253575.25\n",
      "Step 40: Loss = 2053775.125\n",
      "epoch:24/100 |Loss: 2642649.75\n",
      "\n",
      "Epoch 26/100 ----------------\n",
      "Step 0: Loss = 2092435.0\n",
      "Step 10: Loss = 2056242.375\n",
      "Step 20: Loss = 2044074.125\n",
      "Step 30: Loss = 2184564.75\n",
      "Step 40: Loss = 2221544.0\n",
      "epoch:25/100 |Loss: 2439001.25\n",
      "\n",
      "Epoch 27/100 ----------------\n",
      "Step 0: Loss = 2077336.25\n",
      "Step 10: Loss = 2001574.75\n",
      "Step 20: Loss = 2285577.0\n",
      "Step 30: Loss = 2186951.25\n",
      "Step 40: Loss = 2035267.125\n",
      "epoch:26/100 |Loss: 2399825.25\n",
      "\n",
      "Epoch 28/100 ----------------\n",
      "Step 0: Loss = 1965867.0\n",
      "Step 10: Loss = 1974096.25\n",
      "Step 20: Loss = 1895084.25\n",
      "Step 30: Loss = 1887445.5\n",
      "Step 40: Loss = 2018644.625\n",
      "epoch:27/100 |Loss: 1961446.0\n",
      "\n",
      "Epoch 29/100 ----------------\n",
      "Step 0: Loss = 1904289.875\n",
      "Step 10: Loss = 2085714.125\n",
      "Step 20: Loss = 2278562.0\n",
      "Step 30: Loss = 1845413.625\n",
      "Step 40: Loss = 1931030.75\n",
      "epoch:28/100 |Loss: 2264347.75\n",
      "\n",
      "Epoch 30/100 ----------------\n",
      "Step 0: Loss = 1885077.625\n",
      "Step 10: Loss = 1748450.875\n",
      "Step 20: Loss = 1948095.125\n",
      "Step 30: Loss = 1747461.25\n",
      "Step 40: Loss = 1861540.875\n",
      "epoch:29/100 |Loss: 1984111.5\n",
      "\n",
      "Epoch 31/100 ----------------\n",
      "Step 0: Loss = 1811830.5\n",
      "Step 10: Loss = 1934073.25\n",
      "Step 20: Loss = 1612178.25\n",
      "Step 30: Loss = 1702970.75\n",
      "Step 40: Loss = 1849257.25\n",
      "epoch:30/100 |Loss: 1508104.875\n",
      "\n",
      "Epoch 32/100 ----------------\n",
      "Step 0: Loss = 1724341.625\n",
      "Step 10: Loss = 1702629.875\n",
      "Step 20: Loss = 1773493.875\n",
      "Step 30: Loss = 1763741.5\n",
      "Step 40: Loss = 1559107.125\n",
      "epoch:31/100 |Loss: 1413040.875\n",
      "\n",
      "Epoch 33/100 ----------------\n",
      "Step 0: Loss = 1534412.875\n",
      "Step 10: Loss = 1478981.0\n",
      "Step 20: Loss = 1599939.125\n",
      "Step 30: Loss = 1630159.625\n",
      "Step 40: Loss = 1801870.125\n",
      "epoch:32/100 |Loss: 1042911.6875\n",
      "\n",
      "Epoch 34/100 ----------------\n",
      "Step 0: Loss = 1599249.625\n",
      "Step 10: Loss = 1717046.25\n",
      "Step 20: Loss = 1692364.375\n",
      "Step 30: Loss = 1557514.125\n",
      "Step 40: Loss = 1688835.25\n",
      "epoch:33/100 |Loss: 1902391.0\n",
      "\n",
      "Epoch 35/100 ----------------\n",
      "Step 0: Loss = 1484580.0\n",
      "Step 10: Loss = 1598853.5\n",
      "Step 20: Loss = 1520930.125\n",
      "Step 30: Loss = 1641122.5\n",
      "Step 40: Loss = 1393295.75\n",
      "epoch:34/100 |Loss: 1802806.5\n",
      "\n",
      "Epoch 36/100 ----------------\n",
      "Step 0: Loss = 1526444.25\n",
      "Step 10: Loss = 1666156.875\n",
      "Step 20: Loss = 1724185.625\n",
      "Step 30: Loss = 1381351.375\n",
      "Step 40: Loss = 1409789.875\n",
      "epoch:35/100 |Loss: 1472558.75\n",
      "\n",
      "Epoch 37/100 ----------------\n",
      "Step 0: Loss = 1418833.0\n",
      "Step 10: Loss = 1326207.875\n",
      "Step 20: Loss = 1387659.625\n",
      "Step 30: Loss = 1413827.375\n",
      "Step 40: Loss = 1304279.125\n",
      "epoch:36/100 |Loss: 1260331.75\n",
      "\n",
      "Epoch 38/100 ----------------\n",
      "Step 0: Loss = 1308465.875\n",
      "Step 10: Loss = 1301793.25\n",
      "Step 20: Loss = 1600481.625\n",
      "Step 30: Loss = 1264157.75\n",
      "Step 40: Loss = 1292309.25\n",
      "epoch:37/100 |Loss: 1235401.625\n",
      "\n",
      "Epoch 39/100 ----------------\n",
      "Step 0: Loss = 1400105.25\n",
      "Step 10: Loss = 1366202.125\n",
      "Step 20: Loss = 1492993.0\n",
      "Step 30: Loss = 1371745.625\n",
      "Step 40: Loss = 1072743.375\n",
      "epoch:38/100 |Loss: 1156259.5\n",
      "\n",
      "Epoch 40/100 ----------------\n",
      "Step 0: Loss = 1281299.75\n",
      "Step 10: Loss = 1127991.875\n",
      "Step 20: Loss = 1256952.375\n",
      "Step 30: Loss = 1403142.625\n",
      "Step 40: Loss = 1199053.5\n",
      "epoch:39/100 |Loss: 1059817.25\n",
      "\n",
      "Epoch 41/100 ----------------\n",
      "Step 0: Loss = 1268360.25\n",
      "Step 10: Loss = 1150129.75\n",
      "Step 20: Loss = 1178344.375\n",
      "Step 30: Loss = 1192039.875\n",
      "Step 40: Loss = 1047280.0\n",
      "epoch:40/100 |Loss: 883581.875\n",
      "\n",
      "Epoch 42/100 ----------------\n",
      "Step 0: Loss = 1244831.0\n",
      "Step 10: Loss = 995400.9375\n",
      "Step 20: Loss = 1138755.875\n",
      "Step 30: Loss = 1085266.25\n",
      "Step 40: Loss = 1170512.625\n",
      "epoch:41/100 |Loss: 1283699.0\n",
      "\n",
      "Epoch 43/100 ----------------\n",
      "Step 0: Loss = 1281840.5\n",
      "Step 10: Loss = 1067478.125\n",
      "Step 20: Loss = 1020336.75\n",
      "Step 30: Loss = 1151103.375\n",
      "Step 40: Loss = 964926.75\n",
      "epoch:42/100 |Loss: 1275269.5\n",
      "\n",
      "Epoch 44/100 ----------------\n",
      "Step 0: Loss = 889401.5625\n",
      "Step 10: Loss = 941536.1875\n",
      "Step 20: Loss = 1053555.0\n",
      "Step 30: Loss = 1085265.375\n",
      "Step 40: Loss = 927235.875\n",
      "epoch:43/100 |Loss: 830192.5\n",
      "\n",
      "Epoch 45/100 ----------------\n",
      "Step 0: Loss = 904958.9375\n",
      "Step 10: Loss = 969308.5\n",
      "Step 20: Loss = 1006550.3125\n",
      "Step 30: Loss = 836506.8125\n",
      "Step 40: Loss = 1073852.0\n",
      "epoch:44/100 |Loss: 826163.125\n",
      "\n",
      "Epoch 46/100 ----------------\n",
      "Step 0: Loss = 992611.1875\n",
      "Step 10: Loss = 766221.875\n",
      "Step 20: Loss = 1093248.125\n",
      "Step 30: Loss = 900577.8125\n",
      "Step 40: Loss = 931880.625\n",
      "epoch:45/100 |Loss: 762809.3125\n",
      "\n",
      "Epoch 47/100 ----------------\n",
      "Step 0: Loss = 911216.8125\n",
      "Step 10: Loss = 824479.6875\n",
      "Step 20: Loss = 735812.6875\n",
      "Step 30: Loss = 798975.8125\n",
      "Step 40: Loss = 837316.1875\n",
      "epoch:46/100 |Loss: 1224112.125\n",
      "\n",
      "Epoch 48/100 ----------------\n",
      "Step 0: Loss = 760139.6875\n",
      "Step 10: Loss = 893520.625\n",
      "Step 20: Loss = 758434.8125\n",
      "Step 30: Loss = 721860.5625\n",
      "Step 40: Loss = 714176.1875\n",
      "epoch:47/100 |Loss: 688162.9375\n",
      "\n",
      "Epoch 49/100 ----------------\n",
      "Step 0: Loss = 694761.0625\n",
      "Step 10: Loss = 723404.5\n",
      "Step 20: Loss = 630752.5\n",
      "Step 30: Loss = 844465.4375\n",
      "Step 40: Loss = 788729.75\n",
      "epoch:48/100 |Loss: 889983.75\n",
      "\n",
      "Epoch 50/100 ----------------\n",
      "Step 0: Loss = 645519.5625\n",
      "Step 10: Loss = 621405.8125\n",
      "Step 20: Loss = 714402.625\n",
      "Step 30: Loss = 701864.3125\n",
      "Step 40: Loss = 623675.5\n",
      "epoch:49/100 |Loss: 622990.1875\n",
      "\n",
      "Epoch 51/100 ----------------\n",
      "Step 0: Loss = 684969.6875\n",
      "Step 10: Loss = 629132.9375\n",
      "Step 20: Loss = 733657.875\n",
      "Step 30: Loss = 643468.75\n",
      "Step 40: Loss = 611293.875\n",
      "epoch:50/100 |Loss: 1031093.0\n",
      "\n",
      "Epoch 52/100 ----------------\n",
      "Step 0: Loss = 703644.8125\n",
      "Step 10: Loss = 587100.5\n",
      "Step 20: Loss = 697001.875\n",
      "Step 30: Loss = 554671.0625\n",
      "Step 40: Loss = 544524.625\n",
      "epoch:51/100 |Loss: 693524.875\n",
      "\n",
      "Epoch 53/100 ----------------\n",
      "Step 0: Loss = 582600.75\n",
      "Step 10: Loss = 568374.4375\n",
      "Step 20: Loss = 489452.59375\n",
      "Step 30: Loss = 536068.5\n",
      "Step 40: Loss = 621261.6875\n",
      "epoch:52/100 |Loss: 505617.3125\n",
      "\n",
      "Epoch 54/100 ----------------\n",
      "Step 0: Loss = 505749.0625\n",
      "Step 10: Loss = 476827.3125\n",
      "Step 20: Loss = 596879.0625\n",
      "Step 30: Loss = 584059.4375\n",
      "Step 40: Loss = 531744.125\n",
      "epoch:53/100 |Loss: 627873.6875\n",
      "\n",
      "Epoch 55/100 ----------------\n",
      "Step 0: Loss = 546291.5\n",
      "Step 10: Loss = 552119.5\n",
      "Step 20: Loss = 494750.40625\n",
      "Step 30: Loss = 462863.0625\n",
      "Step 40: Loss = 414360.75\n",
      "epoch:54/100 |Loss: 433871.28125\n",
      "\n",
      "Epoch 56/100 ----------------\n",
      "Step 0: Loss = 449053.40625\n",
      "Step 10: Loss = 485884.46875\n",
      "Step 20: Loss = 478623.625\n",
      "Step 30: Loss = 411800.03125\n",
      "Step 40: Loss = 596587.4375\n",
      "epoch:55/100 |Loss: 320294.84375\n",
      "\n",
      "Epoch 57/100 ----------------\n",
      "Step 0: Loss = 446666.3125\n",
      "Step 10: Loss = 365027.90625\n",
      "Step 20: Loss = 372286.4375\n",
      "Step 30: Loss = 424105.15625\n",
      "Step 40: Loss = 449346.46875\n",
      "epoch:56/100 |Loss: 167630.828125\n",
      "\n",
      "Epoch 58/100 ----------------\n",
      "Step 0: Loss = 345241.09375\n",
      "Step 10: Loss = 405969.40625\n",
      "Step 20: Loss = 496763.375\n",
      "Step 30: Loss = 486183.59375\n",
      "Step 40: Loss = 372473.21875\n",
      "epoch:57/100 |Loss: 221656.296875\n",
      "\n",
      "Epoch 59/100 ----------------\n",
      "Step 0: Loss = 462399.15625\n",
      "Step 10: Loss = 437757.25\n",
      "Step 20: Loss = 333389.21875\n",
      "Step 30: Loss = 389495.34375\n",
      "Step 40: Loss = 333797.25\n",
      "epoch:58/100 |Loss: 252274.546875\n",
      "\n",
      "Epoch 60/100 ----------------\n",
      "Step 0: Loss = 385943.09375\n",
      "Step 10: Loss = 339542.46875\n",
      "Step 20: Loss = 251213.546875\n",
      "Step 30: Loss = 365241.1875\n",
      "Step 40: Loss = 285635.84375\n",
      "epoch:59/100 |Loss: 310107.9375\n",
      "\n",
      "Epoch 61/100 ----------------\n",
      "Step 0: Loss = 362987.84375\n",
      "Step 10: Loss = 323032.625\n",
      "Step 20: Loss = 336255.53125\n",
      "Step 30: Loss = 250839.46875\n",
      "Step 40: Loss = 279493.40625\n",
      "epoch:60/100 |Loss: 476493.375\n",
      "\n",
      "Epoch 62/100 ----------------\n",
      "Step 0: Loss = 286861.28125\n",
      "Step 10: Loss = 329752.90625\n",
      "Step 20: Loss = 211395.515625\n",
      "Step 30: Loss = 274418.53125\n",
      "Step 40: Loss = 273294.0\n",
      "epoch:61/100 |Loss: 369376.5\n",
      "\n",
      "Epoch 63/100 ----------------\n",
      "Step 0: Loss = 327214.5625\n",
      "Step 10: Loss = 275085.84375\n",
      "Step 20: Loss = 253075.890625\n",
      "Step 30: Loss = 281243.25\n",
      "Step 40: Loss = 274579.0\n",
      "epoch:62/100 |Loss: 180544.421875\n",
      "\n",
      "Epoch 64/100 ----------------\n",
      "Step 0: Loss = 269071.5625\n",
      "Step 10: Loss = 209937.0\n",
      "Step 20: Loss = 259347.46875\n",
      "Step 30: Loss = 194388.375\n",
      "Step 40: Loss = 192301.59375\n",
      "epoch:63/100 |Loss: 185075.84375\n",
      "\n",
      "Epoch 65/100 ----------------\n",
      "Step 0: Loss = 199961.515625\n",
      "Step 10: Loss = 166129.390625\n",
      "Step 20: Loss = 224602.953125\n",
      "Step 30: Loss = 181927.765625\n",
      "Step 40: Loss = 165045.375\n",
      "epoch:64/100 |Loss: 83467.1171875\n",
      "\n",
      "Epoch 66/100 ----------------\n",
      "Step 0: Loss = 181753.5625\n",
      "Step 10: Loss = 230493.96875\n",
      "Step 20: Loss = 162831.609375\n",
      "Step 30: Loss = 168201.3125\n",
      "Step 40: Loss = 192922.78125\n",
      "epoch:65/100 |Loss: 236208.734375\n",
      "\n",
      "Epoch 67/100 ----------------\n",
      "Step 0: Loss = 182242.125\n",
      "Step 10: Loss = 194219.515625\n",
      "Step 20: Loss = 185073.0625\n",
      "Step 30: Loss = 116780.6484375\n",
      "Step 40: Loss = 225056.84375\n",
      "epoch:66/100 |Loss: 116152.109375\n",
      "\n",
      "Epoch 68/100 ----------------\n",
      "Step 0: Loss = 196822.546875\n",
      "Step 10: Loss = 167729.890625\n",
      "Step 20: Loss = 181093.265625\n",
      "Step 30: Loss = 124447.015625\n",
      "Step 40: Loss = 198779.046875\n",
      "epoch:67/100 |Loss: 142435.078125\n",
      "\n",
      "Epoch 69/100 ----------------\n",
      "Step 0: Loss = 143590.09375\n",
      "Step 10: Loss = 119892.8828125\n",
      "Step 20: Loss = 94266.359375\n",
      "Step 30: Loss = 75854.4609375\n",
      "Step 40: Loss = 114412.578125\n",
      "epoch:68/100 |Loss: 100839.4609375\n",
      "\n",
      "Epoch 70/100 ----------------\n",
      "Step 0: Loss = 112844.8828125\n",
      "Step 10: Loss = 127506.265625\n",
      "Step 20: Loss = 171053.5\n",
      "Step 30: Loss = 109444.7265625\n",
      "Step 40: Loss = 73588.1953125\n",
      "epoch:69/100 |Loss: 58909.359375\n",
      "\n",
      "Epoch 71/100 ----------------\n",
      "Step 0: Loss = 123223.90625\n",
      "Step 10: Loss = 99915.84375\n",
      "Step 20: Loss = 129429.765625\n",
      "Step 30: Loss = 106254.3828125\n",
      "Step 40: Loss = 98530.6015625\n",
      "epoch:70/100 |Loss: 32394.87890625\n",
      "\n",
      "Epoch 72/100 ----------------\n",
      "Step 0: Loss = 122638.9375\n",
      "Step 10: Loss = 124734.15625\n",
      "Step 20: Loss = 52470.26171875\n",
      "Step 30: Loss = 91548.609375\n",
      "Step 40: Loss = 94826.921875\n",
      "epoch:71/100 |Loss: 196332.046875\n",
      "\n",
      "Epoch 73/100 ----------------\n",
      "Step 0: Loss = 114618.421875\n",
      "Step 10: Loss = 102007.8046875\n",
      "Step 20: Loss = 84473.3125\n",
      "Step 30: Loss = 84113.4921875\n",
      "Step 40: Loss = 44968.5546875\n",
      "epoch:72/100 |Loss: 128561.2890625\n",
      "\n",
      "Epoch 74/100 ----------------\n",
      "Step 0: Loss = 103504.0\n",
      "Step 10: Loss = 71674.1171875\n",
      "Step 20: Loss = 41703.96875\n",
      "Step 30: Loss = 96763.75\n",
      "Step 40: Loss = 57976.30078125\n",
      "epoch:73/100 |Loss: 67512.1484375\n",
      "\n",
      "Epoch 75/100 ----------------\n",
      "Step 0: Loss = 72173.3125\n",
      "Step 10: Loss = 46314.86328125\n",
      "Step 20: Loss = 46010.6015625\n",
      "Step 30: Loss = 39816.3125\n",
      "Step 40: Loss = 72611.8515625\n",
      "epoch:74/100 |Loss: 75473.234375\n",
      "\n",
      "Epoch 76/100 ----------------\n",
      "Step 0: Loss = 37649.01171875\n",
      "Step 10: Loss = 67572.953125\n",
      "Step 20: Loss = 54619.6015625\n",
      "Step 30: Loss = 70286.1875\n",
      "Step 40: Loss = 52647.34765625\n",
      "epoch:75/100 |Loss: 92193.546875\n",
      "\n",
      "Epoch 77/100 ----------------\n",
      "Step 0: Loss = 67909.9296875\n",
      "Step 10: Loss = 44919.8359375\n",
      "Step 20: Loss = 59371.421875\n",
      "Step 30: Loss = 46826.30078125\n",
      "Step 40: Loss = 59070.625\n",
      "epoch:76/100 |Loss: 82948.3984375\n",
      "\n",
      "Epoch 78/100 ----------------\n",
      "Step 0: Loss = 65961.578125\n",
      "Step 10: Loss = 13625.9833984375\n",
      "Step 20: Loss = 60438.15234375\n",
      "Step 30: Loss = 42300.6875\n",
      "Step 40: Loss = 29127.861328125\n",
      "epoch:77/100 |Loss: 24361.99609375\n",
      "\n",
      "Epoch 79/100 ----------------\n",
      "Step 0: Loss = 64019.27734375\n",
      "Step 10: Loss = 30728.408203125\n",
      "Step 20: Loss = 34236.54296875\n",
      "Step 30: Loss = 57405.6171875\n",
      "Step 40: Loss = 31857.298828125\n",
      "epoch:78/100 |Loss: 28766.8515625\n",
      "\n",
      "Epoch 80/100 ----------------\n",
      "Step 0: Loss = 9893.001953125\n",
      "Step 10: Loss = 51546.24609375\n",
      "Step 20: Loss = 33718.30078125\n",
      "Step 30: Loss = 31200.650390625\n",
      "Step 40: Loss = 12309.994140625\n",
      "epoch:79/100 |Loss: 4794.2265625\n",
      "\n",
      "Epoch 81/100 ----------------\n",
      "Step 0: Loss = 22768.716796875\n",
      "Step 10: Loss = 25888.224609375\n",
      "Step 20: Loss = 2208.78076171875\n",
      "Step 30: Loss = 27507.046875\n",
      "Step 40: Loss = 15617.998046875\n",
      "epoch:80/100 |Loss: 28819.39453125\n",
      "\n",
      "Epoch 82/100 ----------------\n",
      "Step 0: Loss = 19707.986328125\n",
      "Step 10: Loss = 25046.6484375\n",
      "Step 20: Loss = 17447.29296875\n",
      "Step 30: Loss = 9953.8564453125\n",
      "Step 40: Loss = 22636.052734375\n",
      "epoch:81/100 |Loss: 28027.36328125\n",
      "\n",
      "Epoch 83/100 ----------------\n",
      "Step 0: Loss = 19773.875\n",
      "Step 10: Loss = 13524.5244140625\n",
      "Step 20: Loss = 46321.55078125\n",
      "Step 30: Loss = 8131.13720703125\n",
      "Step 40: Loss = 23308.84765625\n",
      "epoch:82/100 |Loss: 7209.80810546875\n",
      "\n",
      "Epoch 84/100 ----------------\n",
      "Step 0: Loss = 13193.0322265625\n",
      "Step 10: Loss = 17279.318359375\n",
      "Step 20: Loss = 51581.48046875\n",
      "Step 30: Loss = 32500.93359375\n",
      "Step 40: Loss = 6891.06884765625\n",
      "epoch:83/100 |Loss: 4079.916015625\n",
      "\n",
      "Epoch 85/100 ----------------\n",
      "Step 0: Loss = 18781.99609375\n",
      "Step 10: Loss = 17333.306640625\n",
      "Step 20: Loss = 14224.802734375\n",
      "Step 30: Loss = 4927.2978515625\n",
      "Step 40: Loss = 25744.296875\n",
      "epoch:84/100 |Loss: 12945.2314453125\n",
      "\n",
      "Epoch 86/100 ----------------\n",
      "Step 0: Loss = 3583.801513671875\n",
      "Step 10: Loss = 10337.1806640625\n",
      "Step 20: Loss = 16383.73828125\n",
      "Step 30: Loss = 3576.644775390625\n",
      "Step 40: Loss = 8371.072265625\n",
      "epoch:85/100 |Loss: 2681.0205078125\n",
      "\n",
      "Epoch 87/100 ----------------\n",
      "Step 0: Loss = 8922.6611328125\n",
      "Step 10: Loss = 3028.630126953125\n",
      "Step 20: Loss = 10125.6875\n",
      "Step 30: Loss = 5800.29931640625\n",
      "Step 40: Loss = 4038.112548828125\n",
      "epoch:86/100 |Loss: 14565.2919921875\n",
      "\n",
      "Epoch 88/100 ----------------\n",
      "Step 0: Loss = 11611.982421875\n",
      "Step 10: Loss = 24320.998046875\n",
      "Step 20: Loss = 430.15557861328125\n",
      "Step 30: Loss = 8811.1484375\n",
      "Step 40: Loss = 4440.4248046875\n",
      "epoch:87/100 |Loss: 46661.80859375\n",
      "\n",
      "Epoch 89/100 ----------------\n",
      "Step 0: Loss = 7864.16552734375\n",
      "Step 10: Loss = 8236.0078125\n",
      "Step 20: Loss = 10168.8046875\n",
      "Step 30: Loss = 1767.097412109375\n",
      "Step 40: Loss = 1788.2884521484375\n",
      "epoch:88/100 |Loss: 9399.6025390625\n",
      "\n",
      "Epoch 90/100 ----------------\n",
      "Step 0: Loss = 1350.882568359375\n",
      "Step 10: Loss = 6153.83203125\n",
      "Step 20: Loss = 1451.1734619140625\n",
      "Step 30: Loss = 6933.3134765625\n",
      "Step 40: Loss = 3936.778076171875\n",
      "epoch:89/100 |Loss: 447.0534362792969\n",
      "\n",
      "Epoch 91/100 ----------------\n",
      "Step 0: Loss = 5074.78759765625\n",
      "Step 10: Loss = 17497.23046875\n",
      "Step 20: Loss = 43214.42578125\n",
      "Step 30: Loss = 1531.397705078125\n",
      "Step 40: Loss = 11966.5556640625\n",
      "epoch:90/100 |Loss: 10381.361328125\n",
      "\n",
      "Epoch 92/100 ----------------\n",
      "Step 0: Loss = 4586.8173828125\n",
      "Step 10: Loss = 6390.5107421875\n",
      "Step 20: Loss = 339.8804626464844\n",
      "Step 30: Loss = 2093.364013671875\n",
      "Step 40: Loss = 1206.1220703125\n",
      "epoch:91/100 |Loss: 650.3733520507812\n",
      "\n",
      "Epoch 93/100 ----------------\n",
      "Step 0: Loss = 9852.2373046875\n",
      "Step 10: Loss = 533.33349609375\n",
      "Step 20: Loss = 8945.0478515625\n",
      "Step 30: Loss = 2161.7734375\n",
      "Step 40: Loss = 807.1574096679688\n",
      "epoch:92/100 |Loss: 7327.45556640625\n",
      "\n",
      "Epoch 94/100 ----------------\n",
      "Step 0: Loss = 5704.7080078125\n",
      "Step 10: Loss = 4046.2138671875\n",
      "Step 20: Loss = 938.8126831054688\n",
      "Step 30: Loss = 3041.52734375\n",
      "Step 40: Loss = 12897.6904296875\n",
      "epoch:93/100 |Loss: 10049.7314453125\n",
      "\n",
      "Epoch 95/100 ----------------\n",
      "Step 0: Loss = 11180.9072265625\n",
      "Step 10: Loss = 403.387451171875\n",
      "Step 20: Loss = 215.41587829589844\n",
      "Step 30: Loss = 1991.6827392578125\n",
      "Step 40: Loss = 921.4000854492188\n",
      "epoch:94/100 |Loss: 9342.029296875\n",
      "\n",
      "Epoch 96/100 ----------------\n",
      "Step 0: Loss = 179.21359252929688\n",
      "Step 10: Loss = 547.4214477539062\n",
      "Step 20: Loss = 11427.42578125\n",
      "Step 30: Loss = 1265.4366455078125\n",
      "Step 40: Loss = 5275.2353515625\n",
      "epoch:95/100 |Loss: 9381.4619140625\n",
      "\n",
      "Epoch 97/100 ----------------\n",
      "Step 0: Loss = 6183.9306640625\n",
      "Step 10: Loss = 751.96240234375\n",
      "Step 20: Loss = 364.3171691894531\n",
      "Step 30: Loss = 781.0636596679688\n",
      "Step 40: Loss = 1722.8558349609375\n",
      "epoch:96/100 |Loss: 2575.3291015625\n",
      "\n",
      "Epoch 98/100 ----------------\n",
      "Step 0: Loss = 1989.0538330078125\n",
      "Step 10: Loss = 299.0401306152344\n",
      "Step 20: Loss = 888.0078125\n",
      "Step 30: Loss = 476.63604736328125\n",
      "Step 40: Loss = 357.82171630859375\n",
      "epoch:97/100 |Loss: 669.6272583007812\n",
      "\n",
      "Epoch 99/100 ----------------\n",
      "Step 0: Loss = 374.6763916015625\n",
      "Step 10: Loss = 597.1636352539062\n",
      "Step 20: Loss = 10176.806640625\n",
      "Step 30: Loss = 1459.78271484375\n",
      "Step 40: Loss = 946.8289184570312\n",
      "epoch:98/100 |Loss: 8953.9609375\n",
      "\n",
      "Epoch 100/100 ----------------\n",
      "Step 0: Loss = 1942.55615234375\n",
      "Step 10: Loss = 1741.74755859375\n",
      "Step 20: Loss = 2396.781005859375\n",
      "Step 30: Loss = 2200.931396484375\n",
      "Step 40: Loss = 13455.8876953125\n",
      "epoch:99/100 |Loss: 9118.40234375\n",
      "max AD score 241.55144\n",
      "thres: 98.323105\n",
      "➡️ Max RMSE among outliers: 111.84\n",
      "Data training:\n",
      "Precision: 0.670, Recall: 0.184, F1-score: 0.288\n",
      "Elapsed: 0.1683\n",
      "➡️ Max RMSE among outliers: 585.71\n",
      "Data testing:\n",
      "Precision: 0.637, Recall: 0.729, F1-score: 0.679\n",
      "Elapsed: 2.3650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_15692/4001870339.py:47: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "model = AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "start = time.time()\n",
    "_, recall, precision, f1, tp, fn, fp, tn = test_model(model, test_df, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data training:')\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp_2, fn_2, fp_2, tn_2 = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp_2, fn_2],\n",
    "    [fp_2, tn_2]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='gray_r', ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix Control\")\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='gray_r', ax=axs[1])\n",
    "axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps_b.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ca52f",
   "metadata": {},
   "source": [
    "### b. Full model replacement with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7225be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 ----------------\n",
      "Step 0: Loss = 20901780.0\n",
      "Step 10: Loss = 19966036.0\n",
      "Step 20: Loss = 21683588.0\n",
      "Step 30: Loss = 19536920.0\n",
      "Step 40: Loss = 20023280.0\n",
      "Step 50: Loss = 20759622.0\n",
      "Step 60: Loss = 19682570.0\n",
      "epoch:0/100 |Loss: 19675042.0\n",
      "\n",
      "Epoch 2/100 ----------------\n",
      "Step 0: Loss = 20522382.0\n",
      "Step 10: Loss = 21371170.0\n",
      "Step 20: Loss = 19597556.0\n",
      "Step 30: Loss = 20820366.0\n",
      "Step 40: Loss = 20477928.0\n",
      "Step 50: Loss = 19889968.0\n",
      "Step 60: Loss = 20652810.0\n",
      "epoch:1/100 |Loss: 20519786.0\n",
      "\n",
      "Epoch 3/100 ----------------\n",
      "Step 0: Loss = 20472766.0\n",
      "Step 10: Loss = 19096408.0\n",
      "Step 20: Loss = 19967124.0\n",
      "Step 30: Loss = 19496582.0\n",
      "Step 40: Loss = 20250134.0\n",
      "Step 50: Loss = 19668608.0\n",
      "Step 60: Loss = 19511128.0\n",
      "epoch:2/100 |Loss: 19539684.0\n",
      "\n",
      "Epoch 4/100 ----------------\n",
      "Step 0: Loss = 20111440.0\n",
      "Step 10: Loss = 19916366.0\n",
      "Step 20: Loss = 20266618.0\n",
      "Step 30: Loss = 20497968.0\n",
      "Step 40: Loss = 18315150.0\n",
      "Step 50: Loss = 19379682.0\n",
      "Step 60: Loss = 18770330.0\n",
      "epoch:3/100 |Loss: 15370110.0\n",
      "\n",
      "Epoch 5/100 ----------------\n",
      "Step 0: Loss = 20331160.0\n",
      "Step 10: Loss = 20365694.0\n",
      "Step 20: Loss = 21220790.0\n",
      "Step 30: Loss = 20305876.0\n",
      "Step 40: Loss = 20306052.0\n",
      "Step 50: Loss = 19426708.0\n",
      "Step 60: Loss = 19729568.0\n",
      "epoch:4/100 |Loss: 18358572.0\n",
      "\n",
      "Epoch 6/100 ----------------\n",
      "Step 0: Loss = 19349812.0\n",
      "Step 10: Loss = 19805198.0\n",
      "Step 20: Loss = 18499730.0\n",
      "Step 30: Loss = 19336388.0\n",
      "Step 40: Loss = 20786034.0\n",
      "Step 50: Loss = 21995846.0\n",
      "Step 60: Loss = 20545818.0\n",
      "epoch:5/100 |Loss: 21470742.0\n",
      "\n",
      "Epoch 7/100 ----------------\n",
      "Step 0: Loss = 19620960.0\n",
      "Step 10: Loss = 18747098.0\n",
      "Step 20: Loss = 21167272.0\n",
      "Step 30: Loss = 18689300.0\n",
      "Step 40: Loss = 19694980.0\n",
      "Step 50: Loss = 19437180.0\n",
      "Step 60: Loss = 20851046.0\n",
      "epoch:6/100 |Loss: 22150646.0\n",
      "\n",
      "Epoch 8/100 ----------------\n",
      "Step 0: Loss = 18838710.0\n",
      "Step 10: Loss = 20683170.0\n",
      "Step 20: Loss = 19935824.0\n",
      "Step 30: Loss = 20016176.0\n",
      "Step 40: Loss = 19082500.0\n",
      "Step 50: Loss = 20386000.0\n",
      "Step 60: Loss = 19815030.0\n",
      "epoch:7/100 |Loss: 23842180.0\n",
      "\n",
      "Epoch 9/100 ----------------\n",
      "Step 0: Loss = 19106900.0\n",
      "Step 10: Loss = 17969082.0\n",
      "Step 20: Loss = 18521768.0\n",
      "Step 30: Loss = 19784842.0\n",
      "Step 40: Loss = 20465392.0\n",
      "Step 50: Loss = 18746266.0\n",
      "Step 60: Loss = 19797982.0\n",
      "epoch:8/100 |Loss: 22371240.0\n",
      "\n",
      "Epoch 10/100 ----------------\n",
      "Step 0: Loss = 20259572.0\n",
      "Step 10: Loss = 19096282.0\n",
      "Step 20: Loss = 20006168.0\n",
      "Step 30: Loss = 18483074.0\n",
      "Step 40: Loss = 19789106.0\n",
      "Step 50: Loss = 20564936.0\n",
      "Step 60: Loss = 20291000.0\n",
      "epoch:9/100 |Loss: 19264370.0\n",
      "\n",
      "Epoch 11/100 ----------------\n",
      "Step 0: Loss = 19852390.0\n",
      "Step 10: Loss = 18075070.0\n",
      "Step 20: Loss = 19736846.0\n",
      "Step 30: Loss = 20376954.0\n",
      "Step 40: Loss = 19027804.0\n",
      "Step 50: Loss = 18000680.0\n",
      "Step 60: Loss = 19017388.0\n",
      "epoch:10/100 |Loss: 16955600.0\n",
      "\n",
      "Epoch 12/100 ----------------\n",
      "Step 0: Loss = 19638828.0\n",
      "Step 10: Loss = 17961036.0\n",
      "Step 20: Loss = 18516612.0\n",
      "Step 30: Loss = 18828578.0\n",
      "Step 40: Loss = 17216038.0\n",
      "Step 50: Loss = 17944328.0\n",
      "Step 60: Loss = 18818352.0\n",
      "epoch:11/100 |Loss: 17755610.0\n",
      "\n",
      "Epoch 13/100 ----------------\n",
      "Step 0: Loss = 20202662.0\n",
      "Step 10: Loss = 18756792.0\n",
      "Step 20: Loss = 19374096.0\n",
      "Step 30: Loss = 18128554.0\n",
      "Step 40: Loss = 18636012.0\n",
      "Step 50: Loss = 19880902.0\n",
      "Step 60: Loss = 18762564.0\n",
      "epoch:12/100 |Loss: 19432458.0\n",
      "\n",
      "Epoch 14/100 ----------------\n",
      "Step 0: Loss = 19005168.0\n",
      "Step 10: Loss = 19839548.0\n",
      "Step 20: Loss = 18893568.0\n",
      "Step 30: Loss = 19084898.0\n",
      "Step 40: Loss = 19468804.0\n",
      "Step 50: Loss = 18848032.0\n",
      "Step 60: Loss = 18098922.0\n",
      "epoch:13/100 |Loss: 14366608.0\n",
      "\n",
      "Epoch 15/100 ----------------\n",
      "Step 0: Loss = 18144128.0\n",
      "Step 10: Loss = 19139274.0\n",
      "Step 20: Loss = 18032550.0\n",
      "Step 30: Loss = 17768006.0\n",
      "Step 40: Loss = 18398074.0\n",
      "Step 50: Loss = 19001242.0\n",
      "Step 60: Loss = 19200102.0\n",
      "epoch:14/100 |Loss: 18079318.0\n",
      "\n",
      "Epoch 16/100 ----------------\n",
      "Step 0: Loss = 18044578.0\n",
      "Step 10: Loss = 18229218.0\n",
      "Step 20: Loss = 18208522.0\n",
      "Step 30: Loss = 18392646.0\n",
      "Step 40: Loss = 19058920.0\n",
      "Step 50: Loss = 18145212.0\n",
      "Step 60: Loss = 19367724.0\n",
      "epoch:15/100 |Loss: 18116786.0\n",
      "\n",
      "Epoch 17/100 ----------------\n",
      "Step 0: Loss = 18139708.0\n",
      "Step 10: Loss = 19331802.0\n",
      "Step 20: Loss = 18951360.0\n",
      "Step 30: Loss = 17857968.0\n",
      "Step 40: Loss = 17855314.0\n",
      "Step 50: Loss = 18298854.0\n",
      "Step 60: Loss = 17040126.0\n",
      "epoch:16/100 |Loss: 17192456.0\n",
      "\n",
      "Epoch 18/100 ----------------\n",
      "Step 0: Loss = 17510770.0\n",
      "Step 10: Loss = 19496650.0\n",
      "Step 20: Loss = 18018996.0\n",
      "Step 30: Loss = 20120178.0\n",
      "Step 40: Loss = 17468318.0\n",
      "Step 50: Loss = 17762928.0\n",
      "Step 60: Loss = 17717728.0\n",
      "epoch:17/100 |Loss: 16774720.0\n",
      "\n",
      "Epoch 19/100 ----------------\n",
      "Step 0: Loss = 18942466.0\n",
      "Step 10: Loss = 16969370.0\n",
      "Step 20: Loss = 18041476.0\n",
      "Step 30: Loss = 17450358.0\n",
      "Step 40: Loss = 17082874.0\n",
      "Step 50: Loss = 17614440.0\n",
      "Step 60: Loss = 16548158.0\n",
      "epoch:18/100 |Loss: 15552641.0\n",
      "\n",
      "Epoch 20/100 ----------------\n",
      "Step 0: Loss = 16950486.0\n",
      "Step 10: Loss = 18715532.0\n",
      "Step 20: Loss = 17577062.0\n",
      "Step 30: Loss = 17922576.0\n",
      "Step 40: Loss = 18125808.0\n",
      "Step 50: Loss = 16986544.0\n",
      "Step 60: Loss = 16965808.0\n",
      "epoch:19/100 |Loss: 19248568.0\n",
      "\n",
      "Epoch 21/100 ----------------\n",
      "Step 0: Loss = 17543060.0\n",
      "Step 10: Loss = 16449728.0\n",
      "Step 20: Loss = 17060420.0\n",
      "Step 30: Loss = 18026452.0\n",
      "Step 40: Loss = 18976068.0\n",
      "Step 50: Loss = 17776626.0\n",
      "Step 60: Loss = 17246554.0\n",
      "epoch:20/100 |Loss: 15010477.0\n",
      "\n",
      "Epoch 22/100 ----------------\n",
      "Step 0: Loss = 16387198.0\n",
      "Step 10: Loss = 16375704.0\n",
      "Step 20: Loss = 17445248.0\n",
      "Step 30: Loss = 17390800.0\n",
      "Step 40: Loss = 17149686.0\n",
      "Step 50: Loss = 17542862.0\n",
      "Step 60: Loss = 17335170.0\n",
      "epoch:21/100 |Loss: 16608617.0\n",
      "\n",
      "Epoch 23/100 ----------------\n",
      "Step 0: Loss = 17118310.0\n",
      "Step 10: Loss = 16918798.0\n",
      "Step 20: Loss = 15246457.0\n",
      "Step 30: Loss = 17528104.0\n",
      "Step 40: Loss = 15739756.0\n",
      "Step 50: Loss = 16766427.0\n",
      "Step 60: Loss = 16385181.0\n",
      "epoch:22/100 |Loss: 14913130.0\n",
      "\n",
      "Epoch 24/100 ----------------\n",
      "Step 0: Loss = 16573898.0\n",
      "Step 10: Loss = 16751923.0\n",
      "Step 20: Loss = 17191302.0\n",
      "Step 30: Loss = 15824379.0\n",
      "Step 40: Loss = 15063488.0\n",
      "Step 50: Loss = 16393534.0\n",
      "Step 60: Loss = 17071324.0\n",
      "epoch:23/100 |Loss: 15728938.0\n",
      "\n",
      "Epoch 25/100 ----------------\n",
      "Step 0: Loss = 15661528.0\n",
      "Step 10: Loss = 15570914.0\n",
      "Step 20: Loss = 15569492.0\n",
      "Step 30: Loss = 15869669.0\n",
      "Step 40: Loss = 15470516.0\n",
      "Step 50: Loss = 15139909.0\n",
      "Step 60: Loss = 15908692.0\n",
      "epoch:24/100 |Loss: 16624014.0\n",
      "\n",
      "Epoch 26/100 ----------------\n",
      "Step 0: Loss = 15686050.0\n",
      "Step 10: Loss = 16458580.0\n",
      "Step 20: Loss = 15629912.0\n",
      "Step 30: Loss = 16288770.0\n",
      "Step 40: Loss = 15855709.0\n",
      "Step 50: Loss = 14136906.0\n",
      "Step 60: Loss = 16251640.0\n",
      "epoch:25/100 |Loss: 17414778.0\n",
      "\n",
      "Epoch 27/100 ----------------\n",
      "Step 0: Loss = 15253211.0\n",
      "Step 10: Loss = 15176152.0\n",
      "Step 20: Loss = 15777964.0\n",
      "Step 30: Loss = 15400352.0\n",
      "Step 40: Loss = 15245036.0\n",
      "Step 50: Loss = 14773105.0\n",
      "Step 60: Loss = 15846867.0\n",
      "epoch:26/100 |Loss: 11983237.0\n",
      "\n",
      "Epoch 28/100 ----------------\n",
      "Step 0: Loss = 15939348.0\n",
      "Step 10: Loss = 14748781.0\n",
      "Step 20: Loss = 14708187.0\n",
      "Step 30: Loss = 16120606.0\n",
      "Step 40: Loss = 14302147.0\n",
      "Step 50: Loss = 15095867.0\n",
      "Step 60: Loss = 16273243.0\n",
      "epoch:27/100 |Loss: 12721284.0\n",
      "\n",
      "Epoch 29/100 ----------------\n",
      "Step 0: Loss = 16425523.0\n",
      "Step 10: Loss = 13917762.0\n",
      "Step 20: Loss = 14178328.0\n",
      "Step 30: Loss = 15012109.0\n",
      "Step 40: Loss = 13575352.0\n",
      "Step 50: Loss = 14557854.0\n",
      "Step 60: Loss = 15501965.0\n",
      "epoch:28/100 |Loss: 12219575.0\n",
      "\n",
      "Epoch 30/100 ----------------\n",
      "Step 0: Loss = 14290053.0\n",
      "Step 10: Loss = 15901224.0\n",
      "Step 20: Loss = 15924106.0\n",
      "Step 30: Loss = 14624569.0\n",
      "Step 40: Loss = 14276813.0\n",
      "Step 50: Loss = 14238312.0\n",
      "Step 60: Loss = 14272258.0\n",
      "epoch:29/100 |Loss: 14006568.0\n",
      "\n",
      "Epoch 31/100 ----------------\n",
      "Step 0: Loss = 14767446.0\n",
      "Step 10: Loss = 13817245.0\n",
      "Step 20: Loss = 14137875.0\n",
      "Step 30: Loss = 15577462.0\n",
      "Step 40: Loss = 14120999.0\n",
      "Step 50: Loss = 14651665.0\n",
      "Step 60: Loss = 15024030.0\n",
      "epoch:30/100 |Loss: 10362028.0\n",
      "\n",
      "Epoch 32/100 ----------------\n",
      "Step 0: Loss = 13173637.0\n",
      "Step 10: Loss = 13612967.0\n",
      "Step 20: Loss = 13608808.0\n",
      "Step 30: Loss = 14286154.0\n",
      "Step 40: Loss = 14518769.0\n",
      "Step 50: Loss = 15112888.0\n",
      "Step 60: Loss = 13218421.0\n",
      "epoch:31/100 |Loss: 15942234.0\n",
      "\n",
      "Epoch 33/100 ----------------\n",
      "Step 0: Loss = 12608660.0\n",
      "Step 10: Loss = 13483760.0\n",
      "Step 20: Loss = 13012486.0\n",
      "Step 30: Loss = 13357924.0\n",
      "Step 40: Loss = 13259683.0\n",
      "Step 50: Loss = 13683813.0\n",
      "Step 60: Loss = 14269437.0\n",
      "epoch:32/100 |Loss: 13429141.0\n",
      "\n",
      "Epoch 34/100 ----------------\n",
      "Step 0: Loss = 13452980.0\n",
      "Step 10: Loss = 12329240.0\n",
      "Step 20: Loss = 12614070.0\n",
      "Step 30: Loss = 12382383.0\n",
      "Step 40: Loss = 14180222.0\n",
      "Step 50: Loss = 12584266.0\n",
      "Step 60: Loss = 13087565.0\n",
      "epoch:33/100 |Loss: 14048156.0\n",
      "\n",
      "Epoch 35/100 ----------------\n",
      "Step 0: Loss = 14156948.0\n",
      "Step 10: Loss = 14418808.0\n",
      "Step 20: Loss = 12630546.0\n",
      "Step 30: Loss = 12833914.0\n",
      "Step 40: Loss = 12721482.0\n",
      "Step 50: Loss = 13397594.0\n",
      "Step 60: Loss = 12231341.0\n",
      "epoch:34/100 |Loss: 11456386.0\n",
      "\n",
      "Epoch 36/100 ----------------\n",
      "Step 0: Loss = 13256616.0\n",
      "Step 10: Loss = 12812273.0\n",
      "Step 20: Loss = 12798370.0\n",
      "Step 30: Loss = 13641382.0\n",
      "Step 40: Loss = 13345273.0\n",
      "Step 50: Loss = 11959625.0\n",
      "Step 60: Loss = 12230385.0\n",
      "epoch:35/100 |Loss: 11110542.0\n",
      "\n",
      "Epoch 37/100 ----------------\n",
      "Step 0: Loss = 12220829.0\n",
      "Step 10: Loss = 12483483.0\n",
      "Step 20: Loss = 13086335.0\n",
      "Step 30: Loss = 11780516.0\n",
      "Step 40: Loss = 12317381.0\n",
      "Step 50: Loss = 12194603.0\n",
      "Step 60: Loss = 12608278.0\n",
      "epoch:36/100 |Loss: 11460045.0\n",
      "\n",
      "Epoch 38/100 ----------------\n",
      "Step 0: Loss = 11852888.0\n",
      "Step 10: Loss = 11690605.0\n",
      "Step 20: Loss = 12159898.0\n",
      "Step 30: Loss = 11648611.0\n",
      "Step 40: Loss = 11289807.0\n",
      "Step 50: Loss = 11287685.0\n",
      "Step 60: Loss = 12995765.0\n",
      "epoch:37/100 |Loss: 13646415.0\n",
      "\n",
      "Epoch 39/100 ----------------\n",
      "Step 0: Loss = 11966387.0\n",
      "Step 10: Loss = 11840211.0\n",
      "Step 20: Loss = 10843537.0\n",
      "Step 30: Loss = 11877380.0\n",
      "Step 40: Loss = 11679597.0\n",
      "Step 50: Loss = 11405980.0\n",
      "Step 60: Loss = 12326890.0\n",
      "epoch:38/100 |Loss: 10996347.0\n",
      "\n",
      "Epoch 40/100 ----------------\n",
      "Step 0: Loss = 11374419.0\n",
      "Step 10: Loss = 12598328.0\n",
      "Step 20: Loss = 11560792.0\n",
      "Step 30: Loss = 11193635.0\n",
      "Step 40: Loss = 11290623.0\n",
      "Step 50: Loss = 12310108.0\n",
      "Step 60: Loss = 10870077.0\n",
      "epoch:39/100 |Loss: 11063857.0\n",
      "\n",
      "Epoch 41/100 ----------------\n",
      "Step 0: Loss = 10690643.0\n",
      "Step 10: Loss = 11470595.0\n",
      "Step 20: Loss = 10855137.0\n",
      "Step 30: Loss = 11020065.0\n",
      "Step 40: Loss = 11092648.0\n",
      "Step 50: Loss = 10835315.0\n",
      "Step 60: Loss = 11059323.0\n",
      "epoch:40/100 |Loss: 10319358.0\n",
      "\n",
      "Epoch 42/100 ----------------\n",
      "Step 0: Loss = 12182319.0\n",
      "Step 10: Loss = 10668293.0\n",
      "Step 20: Loss = 10695070.0\n",
      "Step 30: Loss = 10783087.0\n",
      "Step 40: Loss = 11415540.0\n",
      "Step 50: Loss = 11864611.0\n",
      "Step 60: Loss = 10128711.0\n",
      "epoch:41/100 |Loss: 9317163.0\n",
      "\n",
      "Epoch 43/100 ----------------\n",
      "Step 0: Loss = 10107324.0\n",
      "Step 10: Loss = 9114408.0\n",
      "Step 20: Loss = 11200389.0\n",
      "Step 30: Loss = 11132125.0\n",
      "Step 40: Loss = 10293965.0\n",
      "Step 50: Loss = 9545758.0\n",
      "Step 60: Loss = 9643222.0\n",
      "epoch:42/100 |Loss: 9833145.0\n",
      "\n",
      "Epoch 44/100 ----------------\n",
      "Step 0: Loss = 10028861.0\n",
      "Step 10: Loss = 10137758.0\n",
      "Step 20: Loss = 9882208.0\n",
      "Step 30: Loss = 10809350.0\n",
      "Step 40: Loss = 9127592.0\n",
      "Step 50: Loss = 10113351.0\n",
      "Step 60: Loss = 10655314.0\n",
      "epoch:43/100 |Loss: 10651561.0\n",
      "\n",
      "Epoch 45/100 ----------------\n",
      "Step 0: Loss = 10855715.0\n",
      "Step 10: Loss = 9421772.0\n",
      "Step 20: Loss = 9221128.0\n",
      "Step 30: Loss = 8883909.0\n",
      "Step 40: Loss = 8841775.0\n",
      "Step 50: Loss = 10278301.0\n",
      "Step 60: Loss = 9872214.0\n",
      "epoch:44/100 |Loss: 12316200.0\n",
      "\n",
      "Epoch 46/100 ----------------\n",
      "Step 0: Loss = 10989327.0\n",
      "Step 10: Loss = 8716443.0\n",
      "Step 20: Loss = 9116266.0\n",
      "Step 30: Loss = 9990334.0\n",
      "Step 40: Loss = 9190221.0\n",
      "Step 50: Loss = 9226669.0\n",
      "Step 60: Loss = 9313689.0\n",
      "epoch:45/100 |Loss: 8253321.5\n",
      "\n",
      "Epoch 47/100 ----------------\n",
      "Step 0: Loss = 9232207.0\n",
      "Step 10: Loss = 9405728.0\n",
      "Step 20: Loss = 9160235.0\n",
      "Step 30: Loss = 9847276.0\n",
      "Step 40: Loss = 9554653.0\n",
      "Step 50: Loss = 9047609.0\n",
      "Step 60: Loss = 8607416.0\n",
      "epoch:46/100 |Loss: 10619868.0\n",
      "\n",
      "Epoch 48/100 ----------------\n",
      "Step 0: Loss = 9366254.0\n",
      "Step 10: Loss = 7920244.0\n",
      "Step 20: Loss = 9012185.0\n",
      "Step 30: Loss = 9183761.0\n",
      "Step 40: Loss = 8659405.0\n",
      "Step 50: Loss = 7769403.0\n",
      "Step 60: Loss = 8689704.0\n",
      "epoch:47/100 |Loss: 6401534.0\n",
      "\n",
      "Epoch 49/100 ----------------\n",
      "Step 0: Loss = 8395562.0\n",
      "Step 10: Loss = 8247491.5\n",
      "Step 20: Loss = 9343781.0\n",
      "Step 30: Loss = 8419317.0\n",
      "Step 40: Loss = 8938798.0\n",
      "Step 50: Loss = 8242881.0\n",
      "Step 60: Loss = 7513133.5\n",
      "epoch:48/100 |Loss: 7320954.0\n",
      "\n",
      "Epoch 50/100 ----------------\n",
      "Step 0: Loss = 8769177.0\n",
      "Step 10: Loss = 7662296.5\n",
      "Step 20: Loss = 7429329.0\n",
      "Step 30: Loss = 8140857.5\n",
      "Step 40: Loss = 7696553.5\n",
      "Step 50: Loss = 7960849.0\n",
      "Step 60: Loss = 8733701.0\n",
      "epoch:49/100 |Loss: 7424329.0\n",
      "\n",
      "Epoch 51/100 ----------------\n",
      "Step 0: Loss = 7766493.5\n",
      "Step 10: Loss = 7488859.0\n",
      "Step 20: Loss = 8397694.0\n",
      "Step 30: Loss = 8103422.5\n",
      "Step 40: Loss = 7273289.5\n",
      "Step 50: Loss = 8209194.0\n",
      "Step 60: Loss = 7751589.0\n",
      "epoch:50/100 |Loss: 5100801.0\n",
      "\n",
      "Epoch 52/100 ----------------\n",
      "Step 0: Loss = 7664006.5\n",
      "Step 10: Loss = 7238693.0\n",
      "Step 20: Loss = 7508864.0\n",
      "Step 30: Loss = 7687114.0\n",
      "Step 40: Loss = 8023486.5\n",
      "Step 50: Loss = 7661056.0\n",
      "Step 60: Loss = 8278053.0\n",
      "epoch:51/100 |Loss: 9941376.0\n",
      "\n",
      "Epoch 53/100 ----------------\n",
      "Step 0: Loss = 8173207.5\n",
      "Step 10: Loss = 8006823.5\n",
      "Step 20: Loss = 7491036.0\n",
      "Step 30: Loss = 7829625.5\n",
      "Step 40: Loss = 8752284.0\n",
      "Step 50: Loss = 7130071.5\n",
      "Step 60: Loss = 6661290.5\n",
      "epoch:52/100 |Loss: 7536337.5\n",
      "\n",
      "Epoch 54/100 ----------------\n",
      "Step 0: Loss = 6419296.0\n",
      "Step 10: Loss = 6564796.0\n",
      "Step 20: Loss = 7004045.0\n",
      "Step 30: Loss = 6785708.5\n",
      "Step 40: Loss = 6577502.5\n",
      "Step 50: Loss = 6990905.5\n",
      "Step 60: Loss = 7596158.5\n",
      "epoch:53/100 |Loss: 7701270.5\n",
      "\n",
      "Epoch 55/100 ----------------\n",
      "Step 0: Loss = 7197991.5\n",
      "Step 10: Loss = 7079576.5\n",
      "Step 20: Loss = 6744492.5\n",
      "Step 30: Loss = 6906257.5\n",
      "Step 40: Loss = 6263867.0\n",
      "Step 50: Loss = 6173734.5\n",
      "Step 60: Loss = 6701945.0\n",
      "epoch:54/100 |Loss: 5422144.0\n",
      "\n",
      "Epoch 56/100 ----------------\n",
      "Step 0: Loss = 7161240.5\n",
      "Step 10: Loss = 5858828.0\n",
      "Step 20: Loss = 6957103.5\n",
      "Step 30: Loss = 6631694.5\n",
      "Step 40: Loss = 6120946.5\n",
      "Step 50: Loss = 6915441.5\n",
      "Step 60: Loss = 5793753.5\n",
      "epoch:55/100 |Loss: 6923910.0\n",
      "\n",
      "Epoch 57/100 ----------------\n",
      "Step 0: Loss = 6803350.0\n",
      "Step 10: Loss = 7005851.0\n",
      "Step 20: Loss = 6386547.5\n",
      "Step 30: Loss = 6383815.5\n",
      "Step 40: Loss = 5984257.5\n",
      "Step 50: Loss = 5013125.0\n",
      "Step 60: Loss = 6189830.0\n",
      "epoch:56/100 |Loss: 6558553.0\n",
      "\n",
      "Epoch 58/100 ----------------\n",
      "Step 0: Loss = 5144961.5\n",
      "Step 10: Loss = 5735709.5\n",
      "Step 20: Loss = 6485154.5\n",
      "Step 30: Loss = 6100854.5\n",
      "Step 40: Loss = 6246886.5\n",
      "Step 50: Loss = 6703954.5\n",
      "Step 60: Loss = 5733654.5\n",
      "epoch:57/100 |Loss: 5104384.0\n",
      "\n",
      "Epoch 59/100 ----------------\n",
      "Step 0: Loss = 5844788.0\n",
      "Step 10: Loss = 6037480.5\n",
      "Step 20: Loss = 5072895.0\n",
      "Step 30: Loss = 5569789.5\n",
      "Step 40: Loss = 5810890.5\n",
      "Step 50: Loss = 6110214.5\n",
      "Step 60: Loss = 5300170.0\n",
      "epoch:58/100 |Loss: 5105861.5\n",
      "\n",
      "Epoch 60/100 ----------------\n",
      "Step 0: Loss = 5630446.5\n",
      "Step 10: Loss = 5039238.5\n",
      "Step 20: Loss = 4815791.0\n",
      "Step 30: Loss = 5406450.5\n",
      "Step 40: Loss = 6387394.0\n",
      "Step 50: Loss = 5537324.0\n",
      "Step 60: Loss = 5649259.5\n",
      "epoch:59/100 |Loss: 5036540.5\n",
      "\n",
      "Epoch 61/100 ----------------\n",
      "Step 0: Loss = 5386942.5\n",
      "Step 10: Loss = 5291455.5\n",
      "Step 20: Loss = 5460393.0\n",
      "Step 30: Loss = 4496669.5\n",
      "Step 40: Loss = 5347409.0\n",
      "Step 50: Loss = 5691416.0\n",
      "Step 60: Loss = 4958618.5\n",
      "epoch:60/100 |Loss: 5576619.0\n",
      "\n",
      "Epoch 62/100 ----------------\n",
      "Step 0: Loss = 5315115.5\n",
      "Step 10: Loss = 4537116.0\n",
      "Step 20: Loss = 4635162.0\n",
      "Step 30: Loss = 5300424.0\n",
      "Step 40: Loss = 5282525.5\n",
      "Step 50: Loss = 4755801.5\n",
      "Step 60: Loss = 4880004.0\n",
      "epoch:61/100 |Loss: 6873568.0\n",
      "\n",
      "Epoch 63/100 ----------------\n",
      "Step 0: Loss = 4870739.5\n",
      "Step 10: Loss = 5151321.5\n",
      "Step 20: Loss = 5382722.0\n",
      "Step 30: Loss = 3880465.75\n",
      "Step 40: Loss = 4765842.5\n",
      "Step 50: Loss = 4536015.0\n",
      "Step 60: Loss = 4796368.0\n",
      "epoch:62/100 |Loss: 3384682.5\n",
      "\n",
      "Epoch 64/100 ----------------\n",
      "Step 0: Loss = 4369628.0\n",
      "Step 10: Loss = 5255641.5\n",
      "Step 20: Loss = 4383609.5\n",
      "Step 30: Loss = 4248623.0\n",
      "Step 40: Loss = 4389043.5\n",
      "Step 50: Loss = 4392108.5\n",
      "Step 60: Loss = 4103012.25\n",
      "epoch:63/100 |Loss: 4514852.0\n",
      "\n",
      "Epoch 65/100 ----------------\n",
      "Step 0: Loss = 3876137.25\n",
      "Step 10: Loss = 4617001.5\n",
      "Step 20: Loss = 5012321.0\n",
      "Step 30: Loss = 4076484.25\n",
      "Step 40: Loss = 4029139.75\n",
      "Step 50: Loss = 3957856.0\n",
      "Step 60: Loss = 4623007.0\n",
      "epoch:64/100 |Loss: 5945130.5\n",
      "\n",
      "Epoch 66/100 ----------------\n",
      "Step 0: Loss = 4088984.5\n",
      "Step 10: Loss = 4306887.5\n",
      "Step 20: Loss = 4011394.0\n",
      "Step 30: Loss = 4433598.5\n",
      "Step 40: Loss = 3514355.0\n",
      "Step 50: Loss = 3352130.0\n",
      "Step 60: Loss = 3889246.75\n",
      "epoch:65/100 |Loss: 4485257.5\n",
      "\n",
      "Epoch 67/100 ----------------\n",
      "Step 0: Loss = 4139066.5\n",
      "Step 10: Loss = 3665677.0\n",
      "Step 20: Loss = 3661612.75\n",
      "Step 30: Loss = 3829292.75\n",
      "Step 40: Loss = 3932745.75\n",
      "Step 50: Loss = 4276341.0\n",
      "Step 60: Loss = 3340388.75\n",
      "epoch:66/100 |Loss: 4579467.5\n",
      "\n",
      "Epoch 68/100 ----------------\n",
      "Step 0: Loss = 3481704.75\n",
      "Step 10: Loss = 4213967.0\n",
      "Step 20: Loss = 3813832.5\n",
      "Step 30: Loss = 3230047.25\n",
      "Step 40: Loss = 3744115.25\n",
      "Step 50: Loss = 3329552.75\n",
      "Step 60: Loss = 3055877.25\n",
      "epoch:67/100 |Loss: 3068543.25\n",
      "\n",
      "Epoch 69/100 ----------------\n",
      "Step 0: Loss = 3279680.5\n",
      "Step 10: Loss = 3761371.25\n",
      "Step 20: Loss = 3749196.75\n",
      "Step 30: Loss = 3826891.75\n",
      "Step 40: Loss = 2988870.75\n",
      "Step 50: Loss = 3224593.5\n",
      "Step 60: Loss = 3255252.0\n",
      "epoch:68/100 |Loss: 2128593.5\n",
      "\n",
      "Epoch 70/100 ----------------\n",
      "Step 0: Loss = 3155792.75\n",
      "Step 10: Loss = 3537558.75\n",
      "Step 20: Loss = 3217241.25\n",
      "Step 30: Loss = 3150982.0\n",
      "Step 40: Loss = 3189811.25\n",
      "Step 50: Loss = 3341322.0\n",
      "Step 60: Loss = 2793295.5\n",
      "epoch:69/100 |Loss: 2688640.5\n",
      "\n",
      "Epoch 71/100 ----------------\n",
      "Step 0: Loss = 3223212.0\n",
      "Step 10: Loss = 2854210.75\n",
      "Step 20: Loss = 3583982.0\n",
      "Step 30: Loss = 3050602.0\n",
      "Step 40: Loss = 2919019.75\n",
      "Step 50: Loss = 2798427.25\n",
      "Step 60: Loss = 2815712.5\n",
      "epoch:70/100 |Loss: 2543140.0\n",
      "\n",
      "Epoch 72/100 ----------------\n",
      "Step 0: Loss = 3178400.75\n",
      "Step 10: Loss = 2692432.75\n",
      "Step 20: Loss = 2347575.25\n",
      "Step 30: Loss = 2186491.0\n",
      "Step 40: Loss = 2748931.5\n",
      "Step 50: Loss = 2612812.75\n",
      "Step 60: Loss = 2528336.5\n",
      "epoch:71/100 |Loss: 1556544.375\n",
      "\n",
      "Epoch 73/100 ----------------\n",
      "Step 0: Loss = 2868566.25\n",
      "Step 10: Loss = 2386686.25\n",
      "Step 20: Loss = 2705516.75\n",
      "Step 30: Loss = 2690349.0\n",
      "Step 40: Loss = 3161046.0\n",
      "Step 50: Loss = 2498061.75\n",
      "Step 60: Loss = 2599061.0\n",
      "epoch:72/100 |Loss: 3823517.25\n",
      "\n",
      "Epoch 74/100 ----------------\n",
      "Step 0: Loss = 2655247.25\n",
      "Step 10: Loss = 2446575.25\n",
      "Step 20: Loss = 2638694.5\n",
      "Step 30: Loss = 2869584.75\n",
      "Step 40: Loss = 2628134.0\n",
      "Step 50: Loss = 2665093.25\n",
      "Step 60: Loss = 2485994.0\n",
      "epoch:73/100 |Loss: 3523574.25\n",
      "\n",
      "Epoch 75/100 ----------------\n",
      "Step 0: Loss = 2518373.5\n",
      "Step 10: Loss = 2053049.25\n",
      "Step 20: Loss = 2599042.0\n",
      "Step 30: Loss = 2343245.25\n",
      "Step 40: Loss = 1868360.875\n",
      "Step 50: Loss = 2436470.25\n",
      "Step 60: Loss = 2160189.75\n",
      "epoch:74/100 |Loss: 3155104.5\n",
      "\n",
      "Epoch 76/100 ----------------\n",
      "Step 0: Loss = 2797715.75\n",
      "Step 10: Loss = 2185944.0\n",
      "Step 20: Loss = 2204127.25\n",
      "Step 30: Loss = 1935719.125\n",
      "Step 40: Loss = 2536987.5\n",
      "Step 50: Loss = 1825850.125\n",
      "Step 60: Loss = 2095331.375\n",
      "epoch:75/100 |Loss: 2801366.25\n",
      "\n",
      "Epoch 77/100 ----------------\n",
      "Step 0: Loss = 1824345.5\n",
      "Step 10: Loss = 2428760.75\n",
      "Step 20: Loss = 1762732.875\n",
      "Step 30: Loss = 1941679.0\n",
      "Step 40: Loss = 2048144.875\n",
      "Step 50: Loss = 1956422.75\n",
      "Step 60: Loss = 1735506.875\n",
      "epoch:76/100 |Loss: 1838617.875\n",
      "\n",
      "Epoch 78/100 ----------------\n",
      "Step 0: Loss = 1636659.75\n",
      "Step 10: Loss = 1936781.25\n",
      "Step 20: Loss = 2087705.0\n",
      "Step 30: Loss = 1862709.875\n",
      "Step 40: Loss = 2016759.625\n",
      "Step 50: Loss = 1565705.0\n",
      "Step 60: Loss = 1790147.75\n",
      "epoch:77/100 |Loss: 1658711.25\n",
      "\n",
      "Epoch 79/100 ----------------\n",
      "Step 0: Loss = 1767746.125\n",
      "Step 10: Loss = 1876771.625\n",
      "Step 20: Loss = 1833580.625\n",
      "Step 30: Loss = 1909193.0\n",
      "Step 40: Loss = 1068232.375\n",
      "Step 50: Loss = 1829233.25\n",
      "Step 60: Loss = 1547833.375\n",
      "epoch:78/100 |Loss: 1383865.0\n",
      "\n",
      "Epoch 80/100 ----------------\n",
      "Step 0: Loss = 2020944.0\n",
      "Step 10: Loss = 1934255.625\n",
      "Step 20: Loss = 1301836.25\n",
      "Step 30: Loss = 1397437.25\n",
      "Step 40: Loss = 1513497.75\n",
      "Step 50: Loss = 1776311.375\n",
      "Step 60: Loss = 1700151.5\n",
      "epoch:79/100 |Loss: 2027145.875\n",
      "\n",
      "Epoch 81/100 ----------------\n",
      "Step 0: Loss = 1686316.0\n",
      "Step 10: Loss = 1559098.125\n",
      "Step 20: Loss = 1690718.375\n",
      "Step 30: Loss = 1450724.75\n",
      "Step 40: Loss = 1681843.625\n",
      "Step 50: Loss = 1653893.625\n",
      "Step 60: Loss = 1404150.375\n",
      "epoch:80/100 |Loss: 944974.0\n",
      "\n",
      "Epoch 82/100 ----------------\n",
      "Step 0: Loss = 1438258.125\n",
      "Step 10: Loss = 1576300.375\n",
      "Step 20: Loss = 1213154.375\n",
      "Step 30: Loss = 1203725.625\n",
      "Step 40: Loss = 1529708.25\n",
      "Step 50: Loss = 1184763.5\n",
      "Step 60: Loss = 1577766.0\n",
      "epoch:81/100 |Loss: 1478574.125\n",
      "\n",
      "Epoch 83/100 ----------------\n",
      "Step 0: Loss = 1578713.875\n",
      "Step 10: Loss = 1379094.875\n",
      "Step 20: Loss = 1581433.625\n",
      "Step 30: Loss = 1466992.375\n",
      "Step 40: Loss = 988371.25\n",
      "Step 50: Loss = 1406119.625\n",
      "Step 60: Loss = 1093858.5\n",
      "epoch:82/100 |Loss: 1311682.125\n",
      "\n",
      "Epoch 84/100 ----------------\n",
      "Step 0: Loss = 1022025.6875\n",
      "Step 10: Loss = 1094307.125\n",
      "Step 20: Loss = 1203520.5\n",
      "Step 30: Loss = 902274.9375\n",
      "Step 40: Loss = 1094226.875\n",
      "Step 50: Loss = 1027926.625\n",
      "Step 60: Loss = 1381498.875\n",
      "epoch:83/100 |Loss: 527368.875\n",
      "\n",
      "Epoch 85/100 ----------------\n",
      "Step 0: Loss = 670390.0\n",
      "Step 10: Loss = 1375402.125\n",
      "Step 20: Loss = 986505.875\n",
      "Step 30: Loss = 774162.4375\n",
      "Step 40: Loss = 1087971.875\n",
      "Step 50: Loss = 673328.75\n",
      "Step 60: Loss = 948181.6875\n",
      "epoch:84/100 |Loss: 874830.4375\n",
      "\n",
      "Epoch 86/100 ----------------\n",
      "Step 0: Loss = 880910.25\n",
      "Step 10: Loss = 1053294.125\n",
      "Step 20: Loss = 504004.21875\n",
      "Step 30: Loss = 958785.0625\n",
      "Step 40: Loss = 958104.4375\n",
      "Step 50: Loss = 912246.125\n",
      "Step 60: Loss = 1069029.25\n",
      "epoch:85/100 |Loss: 897079.8125\n",
      "\n",
      "Epoch 87/100 ----------------\n",
      "Step 0: Loss = 1232089.25\n",
      "Step 10: Loss = 1011864.3125\n",
      "Step 20: Loss = 729183.125\n",
      "Step 30: Loss = 851801.875\n",
      "Step 40: Loss = 862319.8125\n",
      "Step 50: Loss = 624084.8125\n",
      "Step 60: Loss = 1197780.625\n",
      "epoch:86/100 |Loss: 966954.0\n",
      "\n",
      "Epoch 88/100 ----------------\n",
      "Step 0: Loss = 558541.1875\n",
      "Step 10: Loss = 955625.5\n",
      "Step 20: Loss = 808850.5625\n",
      "Step 30: Loss = 597576.4375\n",
      "Step 40: Loss = 680636.0\n",
      "Step 50: Loss = 636257.9375\n",
      "Step 60: Loss = 623783.5625\n",
      "epoch:87/100 |Loss: 894855.75\n",
      "\n",
      "Epoch 89/100 ----------------\n",
      "Step 0: Loss = 775958.0\n",
      "Step 10: Loss = 827473.0\n",
      "Step 20: Loss = 745798.8125\n",
      "Step 30: Loss = 787310.625\n",
      "Step 40: Loss = 516296.75\n",
      "Step 50: Loss = 796984.5\n",
      "Step 60: Loss = 876506.8125\n",
      "epoch:88/100 |Loss: 203952.078125\n",
      "\n",
      "Epoch 90/100 ----------------\n",
      "Step 0: Loss = 841327.375\n",
      "Step 10: Loss = 551653.6875\n",
      "Step 20: Loss = 788709.375\n",
      "Step 30: Loss = 506351.75\n",
      "Step 40: Loss = 715162.375\n",
      "Step 50: Loss = 810751.3125\n",
      "Step 60: Loss = 892835.8125\n",
      "epoch:89/100 |Loss: 406076.0625\n",
      "\n",
      "Epoch 91/100 ----------------\n",
      "Step 0: Loss = 778958.625\n",
      "Step 10: Loss = 345660.6875\n",
      "Step 20: Loss = 664248.8125\n",
      "Step 30: Loss = 556958.5\n",
      "Step 40: Loss = 345021.21875\n",
      "Step 50: Loss = 461347.75\n",
      "Step 60: Loss = 380364.15625\n",
      "epoch:90/100 |Loss: 755559.3125\n",
      "\n",
      "Epoch 92/100 ----------------\n",
      "Step 0: Loss = 576860.4375\n",
      "Step 10: Loss = 685644.4375\n",
      "Step 20: Loss = 760330.0625\n",
      "Step 30: Loss = 540314.5\n",
      "Step 40: Loss = 729479.875\n",
      "Step 50: Loss = 433539.5\n",
      "Step 60: Loss = 490804.9375\n",
      "epoch:91/100 |Loss: 863022.875\n",
      "\n",
      "Epoch 93/100 ----------------\n",
      "Step 0: Loss = 560929.5\n",
      "Step 10: Loss = 373191.375\n",
      "Step 20: Loss = 250731.375\n",
      "Step 30: Loss = 300611.25\n",
      "Step 40: Loss = 624885.875\n",
      "Step 50: Loss = 393926.21875\n",
      "Step 60: Loss = 221009.953125\n",
      "epoch:92/100 |Loss: 1051340.625\n",
      "\n",
      "Epoch 94/100 ----------------\n",
      "Step 0: Loss = 512319.90625\n",
      "Step 10: Loss = 488129.6875\n",
      "Step 20: Loss = 420922.59375\n",
      "Step 30: Loss = 423791.875\n",
      "Step 40: Loss = 344509.90625\n",
      "Step 50: Loss = 674627.0\n",
      "Step 60: Loss = 404493.25\n",
      "epoch:93/100 |Loss: 322675.15625\n",
      "\n",
      "Epoch 95/100 ----------------\n",
      "Step 0: Loss = 452770.09375\n",
      "Step 10: Loss = 359364.8125\n",
      "Step 20: Loss = 590024.6875\n",
      "Step 30: Loss = 360389.40625\n",
      "Step 40: Loss = 407420.84375\n",
      "Step 50: Loss = 526160.25\n",
      "Step 60: Loss = 502829.0625\n",
      "epoch:94/100 |Loss: 140249.765625\n",
      "\n",
      "Epoch 96/100 ----------------\n",
      "Step 0: Loss = 456518.0\n",
      "Step 10: Loss = 256881.578125\n",
      "Step 20: Loss = 352278.3125\n",
      "Step 30: Loss = 394397.125\n",
      "Step 40: Loss = 436951.25\n",
      "Step 50: Loss = 171751.265625\n",
      "Step 60: Loss = 261552.625\n",
      "epoch:95/100 |Loss: 236043.3125\n",
      "\n",
      "Epoch 97/100 ----------------\n",
      "Step 0: Loss = 343364.09375\n",
      "Step 10: Loss = 222285.84375\n",
      "Step 20: Loss = 423069.75\n",
      "Step 30: Loss = 291877.5625\n",
      "Step 40: Loss = 294802.9375\n",
      "Step 50: Loss = 283157.09375\n",
      "Step 60: Loss = 472605.78125\n",
      "epoch:96/100 |Loss: 15704.3857421875\n",
      "\n",
      "Epoch 98/100 ----------------\n",
      "Step 0: Loss = 202940.65625\n",
      "Step 10: Loss = 379788.03125\n",
      "Step 20: Loss = 207687.296875\n",
      "Step 30: Loss = 299813.46875\n",
      "Step 40: Loss = 196551.234375\n",
      "Step 50: Loss = 150177.734375\n",
      "Step 60: Loss = 210521.46875\n",
      "epoch:97/100 |Loss: 151862.546875\n",
      "\n",
      "Epoch 99/100 ----------------\n",
      "Step 0: Loss = 267228.125\n",
      "Step 10: Loss = 186243.59375\n",
      "Step 20: Loss = 231823.15625\n",
      "Step 30: Loss = 181566.90625\n",
      "Step 40: Loss = 170667.59375\n",
      "Step 50: Loss = 156127.3125\n",
      "Step 60: Loss = 193989.953125\n",
      "epoch:98/100 |Loss: 222573.515625\n",
      "\n",
      "Epoch 100/100 ----------------\n",
      "Step 0: Loss = 123527.1015625\n",
      "Step 10: Loss = 94886.3828125\n",
      "Step 20: Loss = 147415.234375\n",
      "Step 30: Loss = 136506.046875\n",
      "Step 40: Loss = 141108.3125\n",
      "Step 50: Loss = 252765.703125\n",
      "Step 60: Loss = 257681.3125\n",
      "epoch:99/100 |Loss: 355503.0625\n",
      "max AD score 1986.5637\n",
      "thres: 862.6232\n",
      "➡️ Total true outliers in this test set: 70213\n",
      "➡️ Number of those with RMSE > threshold: 43071\n",
      "➡️ Max RMSE among outliers: 2038.74\n",
      "➡️ RMSE of first 5 outliers: [341.93338 148.36952 191.25299 190.38493 192.09023]\n",
      "Data testing:\n",
      "Precision: 0.987, Recall: 0.613, F1-score: 0.757\n",
      "Elapsed: 2.3940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_87460/617667693.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df2.drop('label', axis=1)\n",
    "X_train = X_train.astype(float)\n",
    "X_train = X_train.to_numpy()\n",
    "\n",
    "feature_size = X_train.shape[-1]\n",
    "AE(feature_size)\n",
    "\n",
    "model, thres = train(X_train, feature_size)\n",
    "\n",
    "start = time.time()\n",
    "_, recall_2, precision_2, f1_2, tp, fn, fp, tn = test_model(model, test_df2, thres)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print('Data testing:')\n",
    "print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "print(f\"Elapsed: {elapsed:.4f}\")\n",
    "\n",
    "conf_matrix_2 = pd.DataFrame(\n",
    "    [[tp, fn],\n",
    "    [fp, tn]],\n",
    "    columns=['Actual outlier', 'Actual inlier'],\n",
    "    index=['Predicted outlier', 'Predicted inlier']\n",
    ")\n",
    "\n",
    "fig, axs= plt.subplots()\n",
    "sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='gray_r', ax=axs)\n",
    "axs.set_title(\"Confusion Matrix Treatment\")\n",
    "# sns.heatmap(conf_matrix_2, annot=True, fmt='d', cmap='gray_r', ax=axs[1])\n",
    "# axs[1].set_title(\"Confusion Matrix Treatment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_heatmaps_c.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32999c",
   "metadata": {},
   "source": [
    "### c. Continuous Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8077993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data testing 1:\n",
      "Training on 5000 samples\n",
      "max AD score 21213.156\n",
      "thres: 8629.127\n",
      "Testing on 14044 samples\n",
      "{'tp: 4295'}\n",
      "{'fp: 361'}\n",
      "{'fn: 2727'}\n",
      "{'tn: 6661'}\n",
      "Precision: 0.922, Recall: 0.612, F1-score: 0.736\n",
      "Elapsed: 0.2323\n",
      "First iteration - no previous model to compare\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 2:\n",
      "Training on 5000 samples\n",
      "max AD score 19998.64\n",
      "thres: 8736.459\n",
      "Testing on 14044 samples\n",
      "{'tp: 4104'}\n",
      "{'fp: 305'}\n",
      "{'fn: 2918'}\n",
      "{'tn: 6717'}\n",
      "Precision: 0.931, Recall: 0.584, F1-score: 0.718\n",
      "Elapsed: 0.2364\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14044, 21)\n",
      "Previous results shape: (14044, 21)\n",
      "Current results index range: 0 to 14043\n",
      "Previous results index range: 0 to 14043\n",
      "Index overlap: 14044\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9635\n",
      "-1    4409\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9388\n",
      "-1    4656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1462\n",
      "Agreement - Both normal: 6441\n",
      "Disagreement - Current anomaly, Previous normal: 2947\n",
      "Disagreement - Current normal, Previous anomaly: 3194\n",
      "Total disagreements: 6141\n",
      "Agreement rate: 0.563\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1462    2947\n",
      "        Normal    3194    6441\n",
      "McNemar Statistic: 9.8544\n",
      "McNemar p-value: 0.0016942410350539706\n",
      "** Significant difference between models (p < 0.01)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 3:\n",
      "Training on 5000 samples\n",
      "max AD score 17477.793\n",
      "thres: 8608.12\n",
      "Testing on 14044 samples\n",
      "{'tp: 4325'}\n",
      "{'fp: 327'}\n",
      "{'fn: 2697'}\n",
      "{'tn: 6695'}\n",
      "Precision: 0.930, Recall: 0.616, F1-score: 0.741\n",
      "Elapsed: 0.2358\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14044, 21)\n",
      "Previous results shape: (14044, 21)\n",
      "Current results index range: 0 to 14043\n",
      "Previous results index range: 0 to 14043\n",
      "Index overlap: 14044\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9392\n",
      "-1    4652\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9635\n",
      "-1    4409\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1458\n",
      "Agreement - Both normal: 6441\n",
      "Disagreement - Current anomaly, Previous normal: 3194\n",
      "Disagreement - Current normal, Previous anomaly: 2951\n",
      "Total disagreements: 6145\n",
      "Agreement rate: 0.562\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1458    3194\n",
      "        Normal    2951    6441\n",
      "McNemar Statistic: 9.5303\n",
      "McNemar p-value: 0.002021015878233864\n",
      "** Significant difference between models (p < 0.01)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 4:\n",
      "Training on 5000 samples\n",
      "max AD score 21202.58\n",
      "thres: 8540.107\n",
      "Testing on 14042 samples\n",
      "{'tp: 4364'}\n",
      "{'fp: 364'}\n",
      "{'fn: 2657'}\n",
      "{'tn: 6657'}\n",
      "Precision: 0.923, Recall: 0.622, F1-score: 0.743\n",
      "Elapsed: 0.2318\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14044, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14043\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9314\n",
      "-1    4728\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9392\n",
      "-1    4652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1579\n",
      "Agreement - Both normal: 6242\n",
      "Disagreement - Current anomaly, Previous normal: 3149\n",
      "Disagreement - Current normal, Previous anomaly: 3072\n",
      "Total disagreements: 6221\n",
      "Agreement rate: 0.557\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1579    3149\n",
      "        Normal    3072    6242\n",
      "McNemar Statistic: 0.9285\n",
      "McNemar p-value: 0.3352613070441031\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 5:\n",
      "Training on 5000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_48937/2069904131.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  both_anomaly = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_48937/2069904131.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  curr_anom_prev_norm = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_48937/2069904131.py:57: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  curr_norm_prev_anom = len(current_results[\n",
      "/var/folders/88/psn_rp490gg_sfbw5c2mx3dh0000gn/T/ipykernel_48937/2069904131.py:62: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  both_normal = len(current_results[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max AD score 10875.386\n",
      "thres: 8550.396\n",
      "Testing on 14042 samples\n",
      "{'tp: 4289'}\n",
      "{'fp: 358'}\n",
      "{'fn: 2732'}\n",
      "{'tn: 6663'}\n",
      "Precision: 0.923, Recall: 0.611, F1-score: 0.735\n",
      "Elapsed: 0.2282\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9395\n",
      "-1    4647\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9314\n",
      "-1    4728\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1600\n",
      "Agreement - Both normal: 6267\n",
      "Disagreement - Current anomaly, Previous normal: 3047\n",
      "Disagreement - Current normal, Previous anomaly: 3128\n",
      "Total disagreements: 6175\n",
      "Agreement rate: 0.560\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1600    3047\n",
      "        Normal    3128    6267\n",
      "McNemar Statistic: 1.0364\n",
      "McNemar p-value: 0.3086515240907291\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 6:\n",
      "Training on 5000 samples\n",
      "max AD score 19385.52\n",
      "thres: 8484.129\n",
      "Testing on 14042 samples\n",
      "{'tp: 4412'}\n",
      "{'fp: 359'}\n",
      "{'fn: 2609'}\n",
      "{'tn: 6662'}\n",
      "Precision: 0.925, Recall: 0.628, F1-score: 0.748\n",
      "Elapsed: 0.2303\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9271\n",
      "-1    4771\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9395\n",
      "-1    4647\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1545\n",
      "Agreement - Both normal: 6169\n",
      "Disagreement - Current anomaly, Previous normal: 3226\n",
      "Disagreement - Current normal, Previous anomaly: 3102\n",
      "Total disagreements: 6328\n",
      "Agreement rate: 0.549\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1545    3226\n",
      "        Normal    3102    6169\n",
      "McNemar Statistic: 2.3908\n",
      "McNemar p-value: 0.12205093802095991\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 7:\n",
      "Training on 5000 samples\n",
      "max AD score 19105.58\n",
      "thres: 8608.115\n",
      "Testing on 14042 samples\n",
      "{'tp: 4303'}\n",
      "{'fp: 347'}\n",
      "{'fn: 2718'}\n",
      "{'tn: 6674'}\n",
      "Precision: 0.925, Recall: 0.613, F1-score: 0.737\n",
      "Elapsed: 0.2401\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9392\n",
      "-1    4650\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9271\n",
      "-1    4771\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1569\n",
      "Agreement - Both normal: 6190\n",
      "Disagreement - Current anomaly, Previous normal: 3081\n",
      "Disagreement - Current normal, Previous anomaly: 3202\n",
      "Total disagreements: 6283\n",
      "Agreement rate: 0.553\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1569    3081\n",
      "        Normal    3202    6190\n",
      "McNemar Statistic: 2.2919\n",
      "McNemar p-value: 0.1300507379575442\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 8:\n",
      "Training on 5000 samples\n",
      "max AD score 21203.498\n",
      "thres: 8664.829\n",
      "Testing on 14042 samples\n",
      "{'tp: 4098'}\n",
      "{'fp: 347'}\n",
      "{'fn: 2923'}\n",
      "{'tn: 6674'}\n",
      "Precision: 0.922, Recall: 0.584, F1-score: 0.715\n",
      "Elapsed: 0.2357\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9597\n",
      "-1    4445\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9392\n",
      "-1    4650\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1486\n",
      "Agreement - Both normal: 6433\n",
      "Disagreement - Current anomaly, Previous normal: 2959\n",
      "Disagreement - Current normal, Previous anomaly: 3164\n",
      "Total disagreements: 6123\n",
      "Agreement rate: 0.564\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1486    2959\n",
      "        Normal    3164    6433\n",
      "McNemar Statistic: 6.7967\n",
      "McNemar p-value: 0.009132814140722522\n",
      "** Significant difference between models (p < 0.01)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 9:\n",
      "Training on 5000 samples\n",
      "max AD score 10856.933\n",
      "thres: 8539.968\n",
      "Testing on 14042 samples\n",
      "{'tp: 4373'}\n",
      "{'fp: 358'}\n",
      "{'fn: 2648'}\n",
      "{'tn: 6663'}\n",
      "Precision: 0.924, Recall: 0.623, F1-score: 0.744\n",
      "Elapsed: 0.2323\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9311\n",
      "-1    4731\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9597\n",
      "-1    4445\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1489\n",
      "Agreement - Both normal: 6355\n",
      "Disagreement - Current anomaly, Previous normal: 3242\n",
      "Disagreement - Current normal, Previous anomaly: 2956\n",
      "Total disagreements: 6198\n",
      "Agreement rate: 0.559\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1489    3242\n",
      "        Normal    2956    6355\n",
      "McNemar Statistic: 13.1050\n",
      "McNemar p-value: 0.000294503348796233\n",
      "*** Highly significant difference between models (p < 0.001)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data testing 10:\n",
      "Training on 5000 samples\n",
      "max AD score 20012.795\n",
      "thres: 8581.39\n",
      "Testing on 14042 samples\n",
      "{'tp: 4323'}\n",
      "{'fp: 335'}\n",
      "{'fn: 2698'}\n",
      "{'tn: 6686'}\n",
      "Precision: 0.928, Recall: 0.616, F1-score: 0.740\n",
      "Elapsed: 0.2305\n",
      "\n",
      "=== DEBUG INFO ===\n",
      "Current results shape: (14042, 21)\n",
      "Previous results shape: (14042, 21)\n",
      "Current results index range: 0 to 14041\n",
      "Previous results index range: 0 to 14041\n",
      "Index overlap: 14042\n",
      "Current 'outlier' value counts:\n",
      "outlier\n",
      " 1    9384\n",
      "-1    4658\n",
      "Name: count, dtype: int64\n",
      "Previous 'outlier' value counts:\n",
      "outlier\n",
      " 1    9311\n",
      "-1    4731\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- McNemar Test Analysis ---\n",
      "Agreement - Both anomaly: 1561\n",
      "Agreement - Both normal: 6214\n",
      "Disagreement - Current anomaly, Previous normal: 3097\n",
      "Disagreement - Current normal, Previous anomaly: 3170\n",
      "Total disagreements: 6267\n",
      "Agreement rate: 0.554\n",
      "McNemar Table:\n",
      "                Previous Model\n",
      "               Anomaly  Normal\n",
      "Current Anomaly   1561    3097\n",
      "        Normal    3170    6214\n",
      "McNemar Statistic: 0.8272\n",
      "McNemar p-value: 0.3630863178308982\n",
      "No significant difference between models (p >= 0.05)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Model retraining and comparison completed!\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "import joblib\n",
    "\n",
    "previous_results = None\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Data testing {i+1}:')\n",
    "    \n",
    "    training_data = train_chunks[i][train_chunks[i]['label'] == 1].copy()\n",
    "    print(f\"Training on {len(training_data)} samples\")\n",
    "    \n",
    "    X_train = training_data.drop('label', axis=1)\n",
    "    X_train = X_train.astype(float)\n",
    "    X_train = X_train.to_numpy()\n",
    "\n",
    "    feature_size = X_train.shape[-1]\n",
    "    AE(feature_size)\n",
    "    model, thres = train(X_train, feature_size)\n",
    "    \n",
    "    test_data = test_chunks[i].copy()\n",
    "    print(f\"Testing on {len(test_data)} samples\")\n",
    "    \n",
    "    start = time.time()\n",
    "    current_results, recall_2, precision_2, f1_2 = test_model(model, test_data, thres)\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(f\"Precision: {precision_2:.3f}, Recall: {recall_2:.3f}, F1-score: {f1_2:.3f}\")\n",
    "    print(f\"Elapsed: {elapsed:.4f}\")\n",
    "    \n",
    "    if previous_results is not None:\n",
    "        print(f\"\\n=== DEBUG INFO ===\")\n",
    "        print(f\"Current results shape: {current_results.shape}\")\n",
    "        print(f\"Previous results shape: {previous_results.shape}\")\n",
    "        print(f\"Current results index range: {current_results.index.min()} to {current_results.index.max()}\")\n",
    "        print(f\"Previous results index range: {previous_results.index.min()} to {previous_results.index.max()}\")\n",
    "        print(f\"Index overlap: {len(set(current_results.index) & set(previous_results.index))}\")\n",
    "        print(f\"Current 'outlier' value counts:\\n{current_results['outlier'].value_counts()}\")\n",
    "        print(f\"Previous 'outlier' value counts:\\n{previous_results['outlier'].value_counts()}\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- McNemar Test Analysis ---\")\n",
    "        \n",
    "        if 'outlier' in current_results.columns and 'outlier' in previous_results.columns:\n",
    "            both_anomaly = len(current_results[\n",
    "                (current_results['outlier'] == -1) & \n",
    "                (previous_results['outlier'] == -1)\n",
    "            ])\n",
    "            \n",
    "            curr_anom_prev_norm = len(current_results[\n",
    "                (current_results['outlier'] == -1) & \n",
    "                (previous_results['outlier'] == 1)\n",
    "            ])\n",
    "            \n",
    "            curr_norm_prev_anom = len(current_results[\n",
    "                (current_results['outlier'] == 1) & \n",
    "                (previous_results['outlier'] == -1)\n",
    "            ])\n",
    "            \n",
    "            both_normal = len(current_results[\n",
    "                (current_results['outlier'] == 1) & \n",
    "                (previous_results['outlier'] == 1)\n",
    "            ])\n",
    "            \n",
    "            print(f\"Agreement - Both anomaly: {both_anomaly}\")\n",
    "            print(f\"Agreement - Both normal: {both_normal}\")\n",
    "            print(f\"Disagreement - Current anomaly, Previous normal: {curr_anom_prev_norm}\")\n",
    "            print(f\"Disagreement - Current normal, Previous anomaly: {curr_norm_prev_anom}\")\n",
    "            \n",
    "            total_disagreements = curr_anom_prev_norm + curr_norm_prev_anom\n",
    "            total_samples = both_anomaly + both_normal + total_disagreements\n",
    "            agreement_rate = (both_anomaly + both_normal) / total_samples\n",
    "            \n",
    "            print(f\"Total disagreements: {total_disagreements}\")\n",
    "            print(f\"Agreement rate: {agreement_rate:.3f}\")\n",
    "            \n",
    "            if total_disagreements > 0:\n",
    "                mcnemar_table = np.array([\n",
    "                    [both_anomaly, curr_anom_prev_norm], \n",
    "                    [curr_norm_prev_anom, both_normal]\n",
    "                ])\n",
    "                \n",
    "                print(f\"McNemar Table:\")\n",
    "                print(f\"                Previous Model\")\n",
    "                print(f\"               Anomaly  Normal\")\n",
    "                print(f\"Current Anomaly   {both_anomaly:4d}    {curr_anom_prev_norm:4d}\")\n",
    "                print(f\"        Normal    {curr_norm_prev_anom:4d}    {both_normal:4d}\")\n",
    "                \n",
    "                try:\n",
    "                    result = mcnemar(mcnemar_table, exact=False, correction=True)\n",
    "                    print(f\"McNemar Statistic: {result.statistic:.4f}\")\n",
    "                    print(f\"McNemar p-value: {result.pvalue}\")\n",
    "                    \n",
    "                    if result.pvalue < 0.001:\n",
    "                        print(\"*** Highly significant difference between models (p < 0.001)\")\n",
    "                    elif result.pvalue < 0.01:\n",
    "                        print(\"** Significant difference between models (p < 0.01)\")\n",
    "                    elif result.pvalue < 0.05:\n",
    "                        print(\"* Marginally significant difference between models (p < 0.05)\")\n",
    "                    else:\n",
    "                        print(\"No significant difference between models (p >= 0.05)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in McNemar test: {e}\")\n",
    "            else:\n",
    "                print(\"No disagreements between models - McNemar test not applicable\")\n",
    "        else:\n",
    "            print(\"Missing 'outlier' column in results - cannot perform McNemar test\")\n",
    "    elif previous_results is not None:\n",
    "        print(f\"Warning: Result sizes don't match - Current: {len(current_results)}, Previous: {len(previous_results)}\")\n",
    "    else:\n",
    "        print(\"First iteration - no previous model to compare\")\n",
    "    \n",
    "    previous_results = current_results.copy()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"Model retraining and comparison completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibidetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
